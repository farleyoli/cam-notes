\documentclass[a4paper]{article}

\def\npart {IB}
\def\nterm {Michaelmas}
\def\nyear {2015}
\def\nlecturer {D.\ B.\ Skinner}
\def\ncourse {Methods}
\def\nofficial {http://www.damtp.cam.ac.uk/user/dbs26/1Bmethods.html}

\input{header}

\begin{document}
\maketitle
{\small
\noindent\textbf{Self-adjoint ODEs}\\
Periodic functions. Fourier series: definition and simple properties; Parseval's theorem. Equations of second order. Self-adjoint differential operators. The Sturm-Liouville equation; eigenfunctions and eigenvalues; reality of eigenvalues and orthogonality of eigenfunctions; eigenfunction expansions (Fourier series as prototype), approximation in mean square, statement of completeness.\hspace*{\fill} [5]

\vspace{10pt}
\noindent\textbf{PDEs on bounded domains: separation of variables}\\
Physical basis of Laplace's equation, the wave equation and the diffusion equation. General method of separation of variables in Cartesian, cylindrical and spherical coordinates. Legendre's equation: derivation, solutions including explicit forms of $P_0$, $P_1$ and $P_2$, orthogonality. Bessel's equation of integer order as an example of a self-adjoint eigenvalue problem with non-trivial weight.

\vspace{5pt}
\noindent Examples including potentials on rectangular and circular domains and on a spherical domain (axisymmetric case only), waves on a finite string and heat flow down a semi-infinite rod.\hspace*{\fill} [5]

\vspace{10pt}
\noindent\textbf{Inhomogeneous ODEs: Green's functions}\\
Properties of the Dirac delta function. Initial value problems and forced problems with two fixed end points; solution using Green's functions. Eigenfunction expansions of the delta function and Green's functions.\hspace*{\fill} [4]

\vspace{10pt}
\noindent\textbf{Fourier transforms}\\
Fourier transforms: definition and simple properties; inversion and convolution theorems. The discrete Fourier transform. Examples of application to linear systems. Relationship of transfer function to Green's function for initial value problems.\hspace*{\fill} [4]

\vspace{10pt}
\noindent\textbf{PDEs on unbounded domains}\\
Classification of PDEs in two independent variables. Well posedness. Solution by the method of characteristics. Green's functions for PDEs in 1, 2 and 3 independent variables; fundamental solutions of the wave equation, Laplace's equation and the diffusion equation. The method of images. Application to the forced wave equation, Poisson's equation and forced diffusion equation. Transient solutions of diffusion problems: the error function.\hspace*{\fill} [6]}

\tableofcontents

\setcounter{section}{-1}
\section{Introduction}
In the previous courses, the (partial) differential equations we have seen are mostly linear. For example, we have Laplace's equation:
\[
  \frac{\partial^2 \phi}{\partial x^2} + \frac{\partial \phi}{\partial y^2} = 0,
\]
and the heat equation:
\[
  \frac{\partial \phi}{\partial t} = \kappa \left(\frac{\partial^2 \phi}{\partial x^2} + \frac{\partial^2 \phi }{\partial y^2}\right).
\]
The Schr\"odinger' equation in quantum mechanics is also linear:
\[
  i\hbar \frac{\partial \Phi}{\partial t}= -\frac{\hbar^2}{2m}\frac{\partial^2 \phi}{\partial x^2} + V(x) \Phi(x).
\]
By being linear, these equations have the property that if $\phi_1, \phi_2$ are solutions, then so are $\lambda_1 \phi_1 + \lambda_2 \phi_2$ (for any constants $\lambda_i$).

Why are all these linear? In general, if we just randomly write down a differential equation, most likely it is not going to be linear. So where did the linearity of equations of physics come from?

The answer is that the real world is \emph{not} linear in general. However, often we are not looking for a completely accurate and precise description of the universe. When we have low energy/speed/whatever, we can often quite accurately approximate reality by a linear equation. For example, the equation of general relativity is very complicated and nowhere near being linear, but for small masses and velocities, they reduce to Newton's law of gravitation, which is linear.

The only exception to this seems to be Schr\"odinger's equation. While there are many theories and equations that superseded the Schr\"odinger equation, these are all still linear in nature. It seems that linearity is the thing that underpins quantum mechanics.

Due to the prevalence of linear equations, it is rather important that we understand these equations well, and this is our primary objective of the course.

\section{Vector spaces}
In studying differential equations, we frequently encounter spaces of functions that possess a natural vector space structure. Given a set of basis functions $\{y_n\}$, we seek to expand an arbitrary function $f$ as
\[
  f(x) = \sum_n c_n y_n(x)
\]
for some coefficients $c_n$. A familiar example is the Taylor series expansion
\[
  f(x) = \sum_{n = 0}^\infty \frac{f^{(n)}(0)}{n!} x^n,
\]
where the monomials $\{x^n : n \in \N\}$ serve as basis functions. The choice of basis depends on the problem at hand: for periodic functions, we will use periodic basis elements (Fourier series); for other problems, different bases prove more natural.

Two fundamental questions arise in this context:
\begin{enumerate}
  \item Given a basis $\{y_n\}$, how do we compute the expansion coefficients $c_n$?
  \item When does the series $\sum_n c_n y_n(x)$ converge, and does it converge to $f$?
\end{enumerate}
Linear algebra provides the framework for addressing the first question. Although classical linear algebra deals with finite-dimensional spaces, the techniques extend to infinite-dimensional function spaces under appropriate conditions.

We begin with the basic definitions.
\begin{defi}[Vector space]
  A \emph{vector space} over $\C$ (or $\R$) is a set $V$ equipped with an addition operation $+: V \times V \to V$ and a scalar multiplication $\cdot: \C \times V \to V$ satisfying the following axioms for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and all $\lambda, \mu \in \C$:
  \begin{enumerate}
    \item $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$\hfill (commutativity)
    \item $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$\hfill (associativity of addition)
    \item There exists $\mathbf{0}\in V$ such that $\mathbf{0} + \mathbf{u} = \mathbf{u}$\hfill (additive identity)
    \item For each $\mathbf{u} \in V$, there exists $-\mathbf{u} \in V$ such that $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$\hfill (additive inverse)
    \item $\lambda(\mu \mathbf{v}) = (\lambda \mu) \mathbf{v}$ \hfill (associativity of scalar multiplication)
    \item $\lambda(\mathbf{u} + \mathbf{v}) = \lambda \mathbf{u} + \lambda \mathbf{v}$ \hfill (distributivity over vector addition)
    \item $(\lambda + \mu)\mathbf{u} = \lambda \mathbf{u} + \mu \mathbf{u}$ \hfill (distributivity over scalar addition)
    \item $1\mathbf{v} = \mathbf{v}$ \hfill (multiplicative identity)
  \end{enumerate}
\end{defi}

A vector space equipped with additional structure becomes more useful for analysis. The most important such structure is an inner product.
\begin{defi}[Inner product]
  An \emph{inner product} on a vector space $V$ over $\C$ is a map $(\ph, \ph): V\times V \to \C$ satisfying, for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $\lambda \in \C$:
  \begin{enumerate}
    \item $(\mathbf{u}, \lambda \mathbf{v}) = \lambda (\mathbf{u}, \mathbf{v})$ \hfill(linearity in second argument)
    \item $(\mathbf{u}, \mathbf{v} + \mathbf{w}) = (\mathbf{u}, \mathbf{v}) + (\mathbf{u}, \mathbf{w})$ \hfill (additivity in second argument)
    \item $(\mathbf{u}, \mathbf{v}) = (\mathbf{v}, \mathbf{u})^*$ \hfill (conjugate symmetry)
    \item $(\mathbf{u}, \mathbf{u}) \geq 0$, with equality if and only if $\mathbf{u} = \mathbf{0}$ \hfill (positive definiteness)
  \end{enumerate}
  A vector space equipped with an inner product is called an \emph{inner product space}.
\end{defi}

\begin{defi}[Norm]
  The \emph{norm} induced by an inner product is defined by $\|\mathbf{u}\| = \sqrt{(\mathbf{u}, \mathbf{u})}$. This provides the notions of length and distance in the vector space.
\end{defi}

Note that positive definiteness is well-defined: conjugate symmetry implies $(\mathbf{u}, \mathbf{u}) = (\mathbf{u}, \mathbf{u})^*$, so $(\mathbf{u}, \mathbf{u}) \in \R$.

\begin{prop}[Anti-linearity in first argument]
  For any inner product, we have
  \[
    (\lambda \mathbf{u}, \mathbf{v}) = \lambda^* (\mathbf{u}, \mathbf{v})
  \]
  for all $\mathbf{u}, \mathbf{v} \in V$ and $\lambda \in \C$.
\end{prop}
\begin{proof}
  Using conjugate symmetry and linearity in the second argument:
  \[
    (\lambda \mathbf{u}, \mathbf{v}) = (\mathbf{v}, \lambda \mathbf{u})^* = (\lambda(\mathbf{v}, \mathbf{u}))^* = \lambda^* (\mathbf{v}, \mathbf{u})^* = \lambda^* (\mathbf{u}, \mathbf{v}). \qedhere
  \]
\end{proof}

\begin{defi}[Basis]
  A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}$ forms a \emph{basis} of $V$ if every $\mathbf{u}\in V$ can be uniquely written as a linear combination
  \[
    \mathbf{u} = \sum_{i = 1}^n \lambda_i \mathbf{v}_i
  \]
  for some scalars $\lambda_i \in \C$. The \emph{dimension} of a vector space is the number of elements in any basis.
\end{defi}

\begin{defi}[Orthogonal and orthonormal bases]
  Let $V$ be an inner product space with basis $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}$.
  \begin{itemize}
    \item The basis is \emph{orthogonal} if $(\mathbf{v}_i, \mathbf{v}_j) = 0$ for all $i \neq j$.
    \item The basis is \emph{orthonormal} if it is orthogonal and $(\mathbf{v}_i, \mathbf{v}_i) = 1$ for all $i$.
  \end{itemize}
  Equivalently, an orthonormal basis satisfies $(\mathbf{v}_i, \mathbf{v}_j) = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta.
\end{defi}

Orthonormal bases are particularly convenient because they allow explicit computation of expansion coefficients.

\begin{prop}[Expansion coefficients in orthonormal basis]
  Let $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}$ be an orthonormal basis of an inner product space $V$. For any $\mathbf{u} \in V$, the expansion
  \[
    \mathbf{u} = \sum_{i=1}^n \lambda_i \mathbf{v}_i
  \]
  has coefficients given by
  \[
    \lambda_i = (\mathbf{v}_i, \mathbf{u}).
  \]
\end{prop}
\begin{proof}
  Taking the inner product of both sides with $\mathbf{v}_j$:
  \[
    (\mathbf{v}_j, \mathbf{u}) = \left(\mathbf{v}_j, \sum_{i=1}^n \lambda_i \mathbf{v}_i\right) = \sum_{i=1}^n \lambda_i (\mathbf{v}_j, \mathbf{v}_i) = \sum_{i=1}^n \lambda_i \delta_{ji} = \lambda_j,
  \]
  where we used linearity and additivity in the second argument, together with orthonormality.
\end{proof}
\subsection{Function spaces}
The preceding definitions apply to finite-dimensional spaces such as $\R^n$ or $\C^n$. However, \emph{functions} can also form vector spaces, typically of infinite dimension.

\begin{defi}[Function space]
  Let $\Omega \subseteq \R^n$ be a domain. The set of complex-valued functions $f: \Omega \to \C$ forms a vector space under pointwise operations:
  \begin{itemize}
    \item Addition: $(f + g)(x) = f(x) + g(x)$
    \item Scalar multiplication: $(\lambda f)(x) = \lambda f(x)$
  \end{itemize}
  for all $x \in \Omega$, $\lambda \in \C$.
\end{defi}

\begin{prop}[Solution spaces of linear equations]
  The set of solutions to a linear differential equation forms a vector space.
\end{prop}
\begin{proof}
  Let $\mathcal{L}$ be a linear differential operator. If $f$ and $g$ satisfy $\mathcal{L}f = 0$ and $\mathcal{L}g = 0$, then for any scalars $\alpha, \beta$:
  \[
    \mathcal{L}(\alpha f + \beta g) = \alpha \mathcal{L}f + \beta \mathcal{L}g = 0.
  \]
  Hence $\alpha f + \beta g$ is also a solution, and the solution set is closed under linear combinations.
\end{proof}

To apply the techniques of inner product spaces to functions, we need an appropriate inner product.

\begin{defi}[Inner product on function spaces]
  Let $\Omega \subseteq \R^n$ be a domain with measure $\d\mu$. For functions $f, g: \Omega \to \C$, define
  \[
    (f, g) = \int_{\Omega} f(x)^* g(x) \;\d \mu.
  \]
  This defines an inner product on the space of functions for which the integral converges.
\end{defi}

This definition is the natural analogue of the finite-dimensional inner product $(\mathbf{u}, \mathbf{v}) = \sum_i u_i^* v_i$: viewing a function as a ``continuous list'' of its values, the integral replaces the sum over components.

\begin{eg}[Inner product on interval and disk]
  \leavevmode
  \begin{enumerate}
    \item Let $\Omega = [a, b] \subseteq \R$. With the standard Lebesgue measure $\d\mu = \d x$:
    \[
      (f, g) = \int_a^b f(x)^* g(x)\;\d x.
    \]
    \item Let $\Omega = D^2 = \{(x,y) \in \R^2 : x^2 + y^2 \leq 1\}$ be the unit disk. In polar coordinates $(r, \theta)$, the measure is $\d\mu = r\,\d r\,\d\theta$, giving:
    \[
      (f, g) = \int_0^1 \int_0^{2\pi}f(r, \theta)^* g(r, \theta)\; r\,\d r\,\d \theta.
    \]
  \end{enumerate}
\end{eg}

Note that the measure $\d\mu$ depends on the domain and coordinate system. In polar coordinates, the factor of $r$ arises from the Jacobian of the coordinate transformation. More general weighted measures will appear in Section~\ref{sec:sturm-liouville}.

\subsection{Boundary conditions}
When $\Omega$ has a boundary $\partial\Omega$, we often restrict attention to functions satisfying prescribed conditions on $\partial\Omega$. For the solution space to remain a vector space, these boundary conditions must be compatible with linear combinations.

\begin{defi}[Homogeneous boundary conditions]
  A boundary condition is \emph{homogeneous} if whenever $f$ and $g$ satisfy the boundary conditions, then so does $\lambda f + \mu g$ for any $\lambda, \mu \in \C$ (or $\R$).
\end{defi}
\begin{eg}[Homogeneous vs non-homogeneous boundary conditions]
  Let $\Sigma = [a, b]$. We could require that $f(a) + 7 f'(b) = 0$, or maybe $f(a) + 3 f''(a) = 0$. These are examples of homogeneous boundary conditions. On the other hand, the requirement $f(a) = 1$ is \emph{not} homogeneous.
\end{eg}

\section{Fourier series}
In this section, we develop the theory of Fourier series for periodic functions.

\begin{defi}[Periodic function]
  A function $f: \R \to \C$ is \emph{periodic} with \emph{period} $R > 0$ if $f(x + R) = f(x)$ for all $x \in \R$. We call $R$ the \emph{fundamental period} if it is the smallest such positive number.
\end{defi}

\begin{remark}
  It is often convenient to view a periodic function with period $2\pi$ as a function $f: S^1 \to \C$ from the unit circle to $\C$, parametrized by an angle $\theta \in [0, 2\pi)$.
\end{remark}

Periodic functions arise naturally in many applications. Moreover, they can be used to model functions on a bounded interval: given any $f: [0, 1] \to \C$, we can extend it periodically to all of $\R$ by defining $f(x + n) = f(x)$ for all $n \in \Z$.
\begin{center}
  \begin{tikzpicture}
    \draw [->] (-0.3, 0) -- (2.3, 0) node [right] {$x$};
    \draw [mred, thick] plot [smooth] coordinates {(0, 0.5) (0.7, -0.3) (1.6, 1) (2, 0.5)};

    \draw [->] (2.8, 0.5) -- (3.8, 0.5);
    \draw [->] (4.1, 0) -- (10.9, 0) node [right] {$x$};
    \foreach \x in {4.5, 6.5, 8.5} {
      \draw [mred, thick] plot [smooth] coordinates {(\x, 0.5) (\x + 0.7, -0.3) (\x + 1.6, 1) (\x + 2, 0.5)};
    }
  \end{tikzpicture}
\end{center}
\subsection{Fourier series}
As mentioned in the previous chapter, we want to find a set of ``basis functions'' for periodic functions. A natural choice is the exponential function $e^{in\theta}$, where $n \in \Z$. These functions have period $2\pi$ and are straightforward to integrate and differentiate.

\begin{prop}[Orthogonality of complex exponentials]
  The set of functions $\{e^{in\theta} : n \in \Z\}$ forms an orthogonal set with respect to the inner product on $[-\pi, \pi]$. Specifically, for any $m, n \in \Z$,
  \[
    (e^{im \theta}, e^{in\theta}) = \int_{-\pi}^{\pi} e^{-im\theta} e^{in\theta}\;\d \theta = \int_{-\pi}^\pi e^{i(n - m)\theta}\;\d \theta =
    \begin{cases}
      2\pi & n = m\\
      0 & n\not= m
    \end{cases} = 2\pi \delta_{nm}.
  \]
  Consequently, the set $\left\{\frac{1}{\sqrt{2\pi}} e^{in\theta}: n\in \Z\right\}$ is orthonormal.
\end{prop}

\begin{proof}
  For $n = m$, the integrand is $e^{i \cdot 0 \cdot \theta} = 1$, so the integral evaluates to $2\pi$. For $n \neq m$, we have
  \[
    \int_{-\pi}^\pi e^{i(n-m)\theta}\;\d\theta = \left[\frac{e^{i(n-m)\theta}}{i(n-m)}\right]_{-\pi}^{\pi} = \frac{e^{i(n-m)\pi} - e^{-i(n-m)\pi}}{i(n-m)} = 0,
  \]
  since $e^{i(n-m)\pi} = e^{-i(n-m)\pi}$ when $n - m$ is an integer.
\end{proof}

Fourier's idea was to use this orthogonal set as a basis for \emph{any} periodic function.

\begin{defi}[Fourier series and Fourier coefficients]
  Let $f: S^1 \to \C$ be a periodic function. The \emph{Fourier series} of $f$ is the formal series
  \[
    f(\theta) = \sum_{n \in \Z}\hat{f}_n e^{in\theta},
  \]
  where the \emph{Fourier coefficients} $\hat{f}_n$ are defined by
  \[
    \hat{f}_n = \frac{1}{2\pi} (e^{in\theta}, f) = \frac{1}{2\pi}\int_{-\pi}^\pi e^{-in\theta} f(\theta)\;\d \theta.
  \]
\end{defi}

\begin{remark}
  Strictly speaking, one should write $f(\theta) = \sum \hat{f}_n \frac{e^{in\theta}}{\sqrt{2\pi}}$ with $\hat{f}_n = \left(\frac{e^{in\theta}}{\sqrt{2\pi}}, f\right)$ to use the orthonormal basis. However, for convenience, we absorb all constant factors into the $\hat{f}_n$ coefficients.
\end{remark}

For real-valued functions, there is an alternative formulation in terms of sines and cosines.

\begin{prop}[Real Fourier series]
  \label{prop:real-fourier}
  Let $f: S^1 \to \R$ be a real-valued periodic function. Then the Fourier coefficients satisfy $\hat{f}_{-n} = \hat{f}_n^*$ (complex conjugate), and the Fourier series can be written as
  \[
    f(\theta) = \frac{a_0}{2} + \sum_{n = 1}^\infty (a_n \cos n\theta + b_n \sin n\theta),
  \]
  where the real coefficients $a_n$ and $b_n$ are given by
  \[
    a_n = \frac{1}{\pi}\int_{-\pi}^\pi \cos n\theta \cdot f(\theta) \;\d \theta,\quad b_n = \frac{1}{\pi}\int_{-\pi}^\pi \sin n\theta \cdot f(\theta) \;\d \theta.
  \]
\end{prop}

\begin{proof}
  Since $f$ is real-valued, we have
  \[
    (\hat{f}_n)^* = \left(\frac{1}{2\pi}\int_{-\pi}^\pi e^{-in\theta}f(\theta)\;\d \theta\right)^* = \frac{1}{2\pi}\int_{-\pi}^{\pi}e^{in\theta}f(\theta)\;\d \theta = \hat{f}_{-n}.
  \]
  Thus we can rewrite the Fourier series as
  \[
    f(\theta) = \hat{f}_0 + \sum_{n = 1}^\infty\left(\hat{f}_n e^{in\theta} + \hat{f}_{-n}e^{-in\theta}\right) = \hat{f}_0 + \sum_{n = 1}^\infty \left(\hat{f}_n e^{in\theta} + \hat{f}_n^* e^{-in\theta}\right).
  \]
  Setting $\hat{f}_n = \frac{a_n - ib_n}{2}$ for $n \geq 1$ and using $e^{in\theta} + e^{-in\theta} = 2\cos n\theta$ and $e^{in\theta} - e^{-in\theta} = 2i\sin n\theta$, we obtain the stated form.
\end{proof}

\begin{remark}
  When working with a real function, the choice between the complex exponential form and the sine/cosine form depends on the application. If $f$ is odd (or even), the sine/cosine expansion is useful since the cosine (or sine) terms vanish. On the other hand, for differential equations, the exponential form is often more convenient.
\end{remark}

\subsection{Convergence of Fourier series}
When Fourier first proposed the idea of a Fourier series, mathematicians were skeptical. How can we be sure that the infinite series actually converges? It turns out that in many cases, it does not. To make this question precise, we first introduce some definitions.

\begin{defi}[Partial Fourier sum]
  Let $f$ be a periodic function with Fourier coefficients $\hat{f}_m$. The \emph{$n$-th partial Fourier sum} of $f$ is
  \[
    S_n f(\theta) = \sum_{m = -n}^n \hat{f}_m e^{im\theta}.
  \]
\end{defi}

The central question is whether $S_n f$ ``converges'' to $f$ as $n \to \infty$. There are several notions of convergence for functions, and we now define the most important ones.

\begin{defi}[Convergence in mean square]
  A sequence of functions $(g_n)$ \emph{converges in mean square} (or \emph{in norm}) to a function $g$ on $[-\pi, \pi]$ if
  \[
    \lim_{n\to \infty}\|g_n - g\|^2 = \lim_{n \to \infty} \frac{1}{2\pi}\int_{-\pi}^\pi |g_n(\theta) - g(\theta)|^2 \;\d \theta = 0.
  \]
\end{defi}

\begin{remark}
  Mean square convergence does not distinguish between functions that differ only on a set of measure zero. For instance, if $\lim_{n \to \infty}S_n f$ differs from $f$ at countably many points (such as all rational points), mean square convergence still holds.
\end{remark}

\begin{defi}[Pointwise convergence]
  A sequence of functions $(g_n)$ \emph{converges pointwise} to a function $g$ if
  \[
    \lim_{n \to \infty} g_n(\theta) = g(\theta)
  \]
  for every $\theta$ in the domain.
\end{defi}

\begin{defi}[Uniform convergence]
  A sequence of functions $(g_n)$ \emph{converges uniformly} to a function $g$ if
  \[
    \lim_{n \to \infty}\sup_{\theta} |g_n(\theta) - g(\theta)| = 0.
  \]
  This requires that the rate of convergence is independent of $\theta$.
\end{defi}

Different notions of convergence can yield different answers. A general criterion for convergence in norm requires more analysis (see the Part II Analysis of Functions course). Instead, we illustrate the behaviour with an example.

\begin{eg}[Sawtooth function]
  \label{eg:sawtooth}
  Consider the \emph{sawtooth function} $f(\theta) = \theta$ on $[-\pi, \pi]$, extended periodically to all of $\R$.
  \begin{center}
    \begin{tikzpicture}
      \draw (-3.2, 0) -- (3.2, 0);

      \draw [thick, mblue] (-3, -1) -- (-1, 1);
      \draw [dashed, mblue] (-1, 1) -- (-1, -1);
      \draw [thick, mblue] (-1, -1) -- (1, 1);
      \draw [dashed, mblue] (1, 1) -- (1, -1);
      \draw [thick, mblue] (1, -1) -- (3, 1);

      \node at (-3, 0) [below] {$-3\pi$};
      \node at (-1, 0) [below] {$-\pi$};
      \node at (1, 0) [below] {$\pi$};
      \node at (3, 0) [below] {$3\pi$};
    \end{tikzpicture}
  \end{center}
  This function has jump discontinuities at odd multiples of $\pi$.

  We compute the Fourier coefficients. For $n = 0$:
  \[
    \hat{f}_0 = \frac{1}{2\pi}\int_{-\pi}^\pi \theta\;\d\theta = 0.
  \]
  For $n \neq 0$, integrating by parts:
  \[
    \hat{f}_n = \frac{1}{2\pi}\int_{-\pi}^\pi e^{-in\theta}\theta\;\d \theta = \left[-\frac{1}{2\pi i n}e^{-in\theta}\theta\right]_{-\pi}^\pi + \frac{1}{2\pi i n}\int_{-\pi}^\pi e^{-in\theta}\;\d \theta = \frac{(-1)^{n + 1}}{in}.
  \]
  Thus the Fourier series is
  \[
    f(\theta) = \sum_{n \not= 0}\frac{(-1)^{n + 1}}{in}e^{in\theta}.
  \]
  This series converges to $f(\theta)$ for all $\theta \neq (2m+1)\pi$, i.e., at every point where $f$ is continuous.
\end{eg}

\begin{remark}[Behaviour at discontinuities]
  At a discontinuity $\theta = \pi$, each term of the Fourier series vanishes (since $e^{-in\pi} = (-1)^n$), so the series converges to $0$. This equals the average of the left and right limits:
  \[
    \frac{1}{2}\left(\lim_{\varepsilon \to 0^+} f(\pi - \varepsilon) + \lim_{\varepsilon \to 0^+} f(-\pi + \varepsilon)\right) = \frac{1}{2}(\pi + (-\pi)) = 0.
  \]
  This is a general phenomenon: at an isolated jump discontinuity, the Fourier series converges to the average of the one-sided limits.
\end{remark}
\subsection{Differentiability and Fourier series}
The smoothness of a function is closely related to the decay of its Fourier coefficients. We begin with a motivating observation.

\begin{eg}[Smoothing effect of integration]
  Consider the Heaviside step function
  \[
    \Theta(x) =
    \begin{cases}
      1 & x > 0\\
      0 & x < 0
    \end{cases}.
  \]
  Its integral is
  \[
    \int \Theta(x)\;\d x =
    \begin{cases}
      x & x > 0\\
      0 & x < 0
    \end{cases},
  \]
  which is now continuous. A further integration produces a function that is also differentiable.
  \begin{center}
    \begin{tikzpicture}
      \draw [->] (-2, 0) -- (2, 0) node [right] {$x$};
      \draw [->] (0, -0.5) -- (0, 2.5) node [above] {$y$};
      \draw [mblue, thick] (-2, 0) -- (0, 0);
      \draw [mblue, thick] (0, 2) -- (2, 2);

      \draw [->] (2.5, 1) -- (3.5, 1) node [above, pos=0.5] {$\int\d x$};

      \draw [->] (4, 0) -- (8, 0) node [right] {$x$};
      \draw [->] (6, -0.5) -- (6, 2.5) node [above] {$y$};
      \draw [mred, thick] (4, 0) -- (6, 0);
      \draw [mred, thick] (6, 0) -- (8, 2);
    \end{tikzpicture}
  \end{center}
  Conversely, differentiation makes functions ``rougher'': differentiating $\int \Theta(x)\;\d x$ yields the discontinuous $\Theta(x)$, and differentiating again produces the Dirac $\delta$-function.
\end{eg}

This observation motivates characterizing the ``smoothness'' of a function by how many times it can be differentiated. The following theorem makes precise how smoothness affects Fourier coefficients.

\begin{thm}[Decay of Fourier coefficients]
  \label{thm:fourier-decay}
  Let $f: S^1 \to \C$ be a periodic function such that $f, f', \ldots, f^{(m-1)}$ are all continuous, but $f^{(m)}$ has isolated discontinuities at points $\theta_1, \theta_2, \ldots, \theta_r \in (-\pi, \pi)$. Then the Fourier coefficients satisfy
  \[
    \hat{f}_k = O\left(\frac{1}{|k|^{m+1}}\right) \quad \text{as } |k| \to \infty.
  \]
  In particular, smoother functions have faster-decaying Fourier coefficients.
\end{thm}

\begin{proof}
  For $k \neq 0$, we integrate by parts:
  \begin{align*}
    \hat{f}_k &= \frac{1}{2\pi}\int_{-\pi}^\pi e^{-ik\theta}f(\theta) \;\d \theta\\
    &= \left[-\frac{1}{2\pi i k}e^{-ik\theta}f(\theta)\right]^{\pi}_{-\pi} + \frac{1}{2\pi i k}\int_{-\pi}^\pi e^{-ik\theta}f'(\theta)\;\d \theta.
  \end{align*}
  The boundary term vanishes since $f$ is continuous and periodic ($f(\pi) = f(-\pi)$). Repeating this process $m$ times yields
  \[
    \hat{f}_k = \frac{1}{(ik)^m}\frac{1}{2\pi} \int_{-\pi}^\pi e^{-ik\theta} f^{(m)}(\theta)\;\d \theta.
  \]
  Since $f^{(m)}$ has discontinuities, we split the integral around them. Writing $-\pi = \theta_0$ and $\pi = \theta_{r+1}$, and integrating by parts once more on each subinterval, we obtain boundary contributions from the jumps in $f^{(m)}$. The result is
  \[
    \hat{f}_k = \frac{1}{(ik)^{m + 1}} \frac{1}{2\pi}\left[\sum_{s = 1}^r \left(f^{(m)}(\theta_s^+) - f^{(m)}(\theta_s^-)\right)e^{-ik\theta_s} + \int_{(-\pi, \pi)\setminus \{\theta_s\}} e^{-ik\theta} f^{(m + 1)}(\theta) \;\d \theta\right].
  \]
  The sum is $O(1)$ and the integral is bounded, so $\hat{f}_k = O(k^{-(m+1)})$.
\end{proof}

\begin{remark}
  The intuition is clear: a smooth function varies slowly, so it is well-approximated by low-frequency Fourier modes, and higher-frequency coefficients decay rapidly. A rough function with rapid oscillations requires higher-frequency terms, so the coefficients decay more slowly.
\end{remark}

An important result relating the inner product of a function to its Fourier coefficients is Parseval's theorem.

\begin{thm}[Parseval's theorem]
  \label{thm:parseval}
  Let $f: S^1 \to \C$ be a periodic function with Fourier coefficients $\hat{f}_n$. Then
  \[
    (f, f) = \int_{-\pi}^\pi |f(\theta)|^2 \;\d \theta = 2\pi \sum_{n\in \Z}|\hat{f}_n|^2.
  \]
\end{thm}

\begin{proof}
  Substituting the Fourier expansion and using orthogonality:
  \begin{align*}
    (f, f) &= \int_{-\pi}^\pi |f(\theta)|^2 \;\d \theta\\
    &= \int_{-\pi}^{\pi} \left(\sum_{m \in \Z} \hat{f}_m^* e^{-im\theta}\right)\left(\sum_{n \in \Z} \hat{f}_n e^{in\theta}\right)\;\d \theta\\
    &= \sum_{m, n\in \Z}\hat{f}_m^* \hat{f}_n \int_{-\pi}^\pi e^{i(n - m)\theta}\;\d \theta\\
    &= 2\pi \sum_{m, n\in \Z}\hat{f}_m^* \hat{f}_n \delta_{mn}\\
    &= 2\pi \sum_{n\in \Z}|\hat{f}_n|^2.\qedhere
  \end{align*}
\end{proof}

\begin{remark}
  This proof assumes that the Fourier series converges to $f$ and that the sums can be interchanged with the integral. A rigorous treatment requires analysis beyond the scope of this course.
\end{remark}

Parseval's theorem has surprising applications in number theory.

\begin{defi}[Riemann zeta function]
  The \emph{Riemann zeta function} is defined for $s > 1$ by
  \[
    \zeta(s) = \sum_{n = 1}^\infty \frac{1}{n^s}.
  \]
\end{defi}

\begin{eg}[Computing $\zeta(2)$ via Parseval]
  \label{eg:zeta-two}
  We show that $\zeta(2) = \frac{\pi^2}{6}$.

  From the sawtooth example, the function $f(\theta) = \theta$ on $[-\pi, \pi]$ has Fourier coefficients
  \[
    \hat{f}_0 = 0,\quad \hat{f}_n = \frac{i(-1)^{n}}{n}\text{ for }n\not= 0.
  \]
  By direct computation,
  \[
    (f, f) = \int_{-\pi}^\pi \theta^2 \;\d \theta = \frac{2\pi^3}{3}.
  \]
  On the other hand, Parseval's theorem gives
  \[
    (f, f) = 2\pi \sum_{n \in \Z}|\hat{f}_n|^2 = 2\pi \cdot 2 \sum_{n = 1}^\infty \frac{1}{n^2} = 4\pi \sum_{n = 1}^\infty \frac{1}{n^2}.
  \]
  Equating these expressions yields
  \[
    \sum_{n = 1}^\infty \frac{1}{n^2} = \zeta(2) = \frac{\pi^2}{6}.
  \]
  By integrating the sawtooth function repeatedly and applying the same argument, one can show that $\zeta(2m) = \pi^{2m} q$ for some $q \in \Q$ for all positive integers $m$.
\end{eg}

\begin{remark}
  One may ask: why use $\{e^{in\theta}\}$ as a basis? In finite dimensions, any linearly independent set forms a basis, but in practice we choose bases that respect the symmetries of the problem. For periodic functions, the complex exponentials $e^{in\theta}$ are natural because they are eigenfunctions of the differentiation operator and capture the periodicity of the domain.
\end{remark}

\section{Sturm-Liouville Theory}
\label{sec:sturm-liouville}
\subsection{Sturm-Liouville operators}
In finite dimensions, a linear map $M: V \to W$ between inner product spaces with bases $\{\mathbf{v}_i\}$ and $\{\mathbf{w}_a\}$ can be represented by a matrix with entries $M_{ai} = (\mathbf{w}_a, M\mathbf{v}_i)$. A map $M: V \to V$ is called \emph{self-adjoint} if $M^\dagger = M$ as matrices. To extend this notion to infinite-dimensional spaces where matrix representations may not exist, we use the following definition.

\begin{defi}[Adjoint and self-adjoint operator]
  Let $V$ be an inner product space and $A: V \to V$ a linear map. The \emph{adjoint} of $A$ is a map $A^\dagger: V \to V$ such that
  \[
    (A^\dagger\mathbf{u}, \mathbf{v}) = (\mathbf{u}, A\mathbf{v})
  \]
  for all $\mathbf{u}, \mathbf{v} \in V$. The map $A$ is \emph{self-adjoint} (or \emph{Hermitian}) if $A = A^\dagger$, i.e.,
  \[
    (A\mathbf{u}, \mathbf{v}) = (\mathbf{u}, A\mathbf{v})
  \]
  for all $\mathbf{u}, \mathbf{v} \in V$.
\end{defi}

Self-adjoint operators have special spectral properties. Recall that an \emph{eigenvalue} $\lambda$ of $M$ is a root of $\det(M - \lambda I) = 0$, and a corresponding \emph{eigenvector} $\mathbf{v}$ satisfies $M\mathbf{v} = \lambda \mathbf{v}$.

\begin{prop}[Self-adjoint operators have real eigenvalues]
  \label{prop:self-adjoint-real-eigenvalues}
  Let $M: V \to V$ be a self-adjoint operator. Then all eigenvalues of $M$ are real.
\end{prop}

\begin{proof}
  Suppose $M\mathbf{v} = \lambda \mathbf{v}$ for some nonzero eigenvector $\mathbf{v}$. Then
  \[
    \lambda (\mathbf{v}, \mathbf{v}) = (\mathbf{v}, M\mathbf{v}) = (M\mathbf{v}, \mathbf{v}) = \lambda^* (\mathbf{v}, \mathbf{v}).
  \]
  Since $(\mathbf{v}, \mathbf{v}) \neq 0$, we have $\lambda = \lambda^*$, so $\lambda \in \R$.
\end{proof}

\begin{prop}[Eigenvectors of self-adjoint operators are orthogonal]
  \label{prop:self-adjoint-orthogonal}
  Let $M: V \to V$ be a self-adjoint operator. Eigenvectors corresponding to distinct eigenvalues are orthogonal.
\end{prop}

\begin{proof}
  Suppose $M\mathbf{v}_i = \lambda_i \mathbf{v}_i$ and $M\mathbf{v}_j = \lambda_j \mathbf{v}_j$ with $\lambda_i \neq \lambda_j$. Then
  \[
    \lambda_i (\mathbf{v}_j, \mathbf{v}_i) = (\mathbf{v}_j, M\mathbf{v}_i) = (M\mathbf{v}_j, \mathbf{v}_i) = \lambda_j (\mathbf{v}_j, \mathbf{v}_i).
  \]
  Since $\lambda_i \neq \lambda_j$, we must have $(\mathbf{v}_j, \mathbf{v}_i) = 0$.
\end{proof}

Eigenvector expansions provide a systematic method for solving linear equations.

\begin{eg}[Solving linear equations via eigenvector expansion]
  \label{eg:eigenvector-solve}
  Consider the equation $M\mathbf{u} = \mathbf{f}$, where $M$ is self-adjoint with eigenvectors $\{\mathbf{v}_i\}$ forming a basis. We can expand $\mathbf{u} = \sum_i u_i \mathbf{v}_i$ and $\mathbf{f} = \sum_i f_i \mathbf{v}_i$. Then
  \[
    M\mathbf{u} = \sum_i u_i M\mathbf{v}_i = \sum_i u_i \lambda_i \mathbf{v}_i = \sum_i f_i \mathbf{v}_i.
  \]
  By orthogonality, $u_i \lambda_i = f_i$, so
  \[
    u_i = \frac{f_i}{\lambda_i},
  \]
  provided all $\lambda_i \neq 0$.
\end{eg}

Sturm--Liouville theory extends these ideas to infinite-dimensional function spaces, where ``matrices'' become differential operators.

\begin{defi}[Linear differential operator]
  A \emph{linear differential operator} of order $p$ on an interval $[a, b]$ is an operator of the form
  \[
    \mathcal{L} = A_p(x) \frac{\d^p}{\d x^p} + A_{p - 1}(x) \frac{\d^{p - 1}}{\d x^{p - 1}} + \cdots + A_1(x)\frac{\d}{\d x} + A_0(x),
  \]
  where $A_0, A_1, \ldots, A_p: [a,b] \to \C$ are coefficient functions and $A_p(x) \neq 0$. Linearity means $\mathcal{L}(\alpha f + \beta g) = \alpha \mathcal{L}f + \beta \mathcal{L}g$ for all scalars $\alpha, \beta$ and functions $f, g$.
\end{defi}

In most applications, we consider second-order operators ($p = 2$). The key question is: when is such an operator self-adjoint?

\begin{prop}[Reduction to Sturm--Liouville form]
  \label{prop:sturm-liouville-reduction}
  Any second-order linear differential operator
  \[
    \mathcal{L} y = P(x) \frac{\d^2 y}{\d x^2} + R(x) \frac{\d y}{\d x} + Q(x) y
  \]
  with $P(x) \neq 0$ can be written (up to multiplication by a nonzero function) in \emph{Sturm--Liouville form}.
\end{prop}

\begin{proof}
  Factor out $P$ and complete the derivative:
  \begin{align*}
    \mathcal{L} y &= P \frac{\d^2 y}{\d x^2} + R \frac{\d y}{\d x} + Q y \\
    &= P\left[\frac{\d^2 y}{\d x^2} + \frac{R}{P}\frac{\d y}{\d x} + \frac{Q}{P}y\right] \\
    &= P\left[e^{-\int \frac{R}{P}\;\d x}\frac{\d}{\d x}\left(e^{\int \frac{R}{P}\;\d x}\frac{\d y}{\d x}\right) + \frac{Q}{P}y\right].
  \end{align*}
  Setting $p(x) = \exp\left(\int \frac{R}{P}\;\d x\right)$ and $q(x) = -\frac{Q}{P}p(x)$, and absorbing the factor $Pp^{-1}$, we obtain
  \[
    \mathcal{L} = \frac{\d}{\d x}\left(p(x)\frac{\d}{\d x}\right) - q(x).\qedhere
  \]
\end{proof}

\begin{defi}[Sturm--Liouville operator]
  \label{def:sturm-liouville-operator}
  A \emph{Sturm--Liouville operator} on $[a, b]$ is a second-order differential operator of the form
  \[
    \mathcal{L} = \frac{\d}{\d x}\left(p(x)\frac{\d}{\d x}\right) - q(x),
  \]
  where $p, q: [a, b] \to \R$ are real-valued functions with $p(x) > 0$ on $(a, b)$.
\end{defi}

\begin{prop}[Self-adjointness of Sturm--Liouville operators]
  \label{prop:sturm-liouville-self-adjoint}
  A Sturm--Liouville operator $\mathcal{L}$ satisfies
  \[
    (f, \mathcal{L}g) = [(f^*g' - f'^*g)p]_a^b + (\mathcal{L}f, g)
  \]
  for all sufficiently differentiable functions $f, g$. In particular, $\mathcal{L}$ is self-adjoint if the boundary term vanishes.
\end{prop}

\begin{proof}
  Integrating by parts twice:
  \begin{align*}
    (f, \mathcal{L}g) &= \int_a^b f^*\left(\frac{\d}{\d x}\left(p \frac{\d g}{\d x}\right) - qg\right)\;\d x \\
    &= [f^* pg']_a^b - \int_a^b \left(\frac{\d f^*}{\d x}p\frac{\d g}{\d x} + f^* qg\right)\;\d x\\
    &= [f^*pg' - f'^*pg]_a^b + \int_a^b \left(\frac{\d}{\d x}\left(p\frac{\d f^*}{\d x}\right) - qf^*\right)g\;\d x\\
    &= [(f^*g' - f'^*g)p]_a^b + (\mathcal{L}f, g).\qedhere
  \end{align*}
\end{proof}

\begin{remark}
  The boundary term $[(f^*g' - f'^*g)p]_a^b$ vanishes in several important cases:
  \begin{enumerate}
    \item periodic boundary conditions: $f(a) = f(b)$ and $f'(a) = f'(b)$;
    \item Dirichlet conditions: $f(a) = f(b) = 0$;
    \item singular endpoints: $p(a) = 0$ or $p(b) = 0$.
  \end{enumerate}
\end{remark}

\begin{eg}[Second derivative operator]
  \label{eg:second-derivative-self-adjoint}
  The operator $\mathcal{L} = \frac{\d^2}{\d x^2}$ corresponds to $p = 1$ and $q = 0$. For periodic functions on $[a, b]$, the boundary terms cancel, giving
  \[
    \int_a^b f^*\frac{\d^2 g}{\d x^2}\;\d x = \int_a^b \frac{\d^2 f^*}{\d x^2}g\;\d x.
  \]
  Self-adjointness requires a \emph{second-order} operator; a first-order operator would acquire a sign change from a single integration by parts.
\end{eg}

Just as in finite dimensions, self-adjoint operators have eigenfunctions and eigenvalues with special properties. To state these results, we introduce a weighted inner product.

\begin{defi}[Weighted inner product]
  \label{def:weighted-inner-product}
  Let $w: [a, b] \to \R$ be a \emph{weight function}: a real, non-negative function with at most finitely many zeros. The \emph{weighted inner product} is
  \[
    (f, g)_w = \int_a^b f^*(x) g(x) w(x)\;\d x.
  \]
\end{defi}

\begin{remark}
  Weight functions arise naturally in curvilinear coordinates. For instance, in polar coordinates on a disk, the area element is $r\,\d r\,\d\theta$, giving weight $w(r) = r$ that vanishes at the origin. We allow finitely many zeros so that $(f, f)_w = 0$ still implies $f = 0$ for continuous $f$.
\end{remark}

\begin{defi}[Eigenfunction of a Sturm--Liouville operator]
  \label{def:sturm-liouville-eigenfunction}
  Let $\mathcal{L}$ be a Sturm--Liouville operator on $[a, b]$ and $w$ a weight function. An \emph{eigenfunction} of $\mathcal{L}$ with weight $w$ is a nonzero function $y: [a, b] \to \C$ satisfying
  \[
    \mathcal{L} y = \lambda w y
  \]
  for some $\lambda \in \C$, called the \emph{eigenvalue}. The eigenfunction must also satisfy appropriate boundary conditions.
\end{defi}

\begin{remark}
  The eigenvalue equation $\mathcal{L} y = \lambda w y$ is not satisfied by arbitrary functions. The combination of the differential equation, boundary conditions, and constraints on $w$ (positive, real, finitely many zeros) severely restricts which $y$ are eigenfunctions. The weight function $w$ is particularly convenient for problems in curvilinear coordinates.
\end{remark}

\begin{prop}[Sturm-Liouville eigenvalues, reality]
  The eigenvalues of a Sturm-Liouville operator are real.
\end{prop}

\begin{proof}
  Suppose $\mathcal{L} y_i = \lambda_i w y_i$. Then
  \[
    \lambda_i (y_i, y_i)_w = \lambda_i (y_i, w y_i) = (y_i, \mathcal{L} y_i) = (\mathcal{L} y_i, y_i) = (\lambda_i w y_i, y_i) = \lambda_i^* (y_i, y_i)_w.
  \]
  Since $(y_i, y_i)_w \not= 0$, we have $\lambda_i = \lambda_i^*$.
\end{proof}
Note that the first and last terms use the weighted inner product, but the middle terms use the unweighted inner product.

\begin{prop}[Sturm-Liouville eigenfunctions, orthogonality]
  Eigenfunctions with different eigenvalues (but same weight) are orthogonal.
\end{prop}

\begin{proof}
  Let $\mathcal{L} y_i = \lambda_i w y_i$ and $\mathcal{L} y_j = \lambda_j w y_j$. Then
  \[
    \lambda_i (y_j, y_i)_w = (y_j, \mathcal{L} y_i) = (\mathcal{L} y_j, y_i) = \lambda_j (y_j, y_i)_w.
  \]
  Since $\lambda_i \not= \lambda_j$, we must have $(y_j, y_i)_w = 0$.
\end{proof}

Those were pretty straightforward manipulations. However, the main results of Sturm--Liouville theory are significantly harder, and we will not prove them. We shall just state them and explore some examples.
\begin{thm}[Sturm-Liouville spectrum, compact domain]
  On a \emph{compact} domain, the eigenvalues $\lambda_1, \lambda_2, \cdots$ form a countably infinite sequence and are discrete.
\end{thm}

This will be a rather helpful result in quantum mechanics, since in quantum mechanics, the possible values of, say, the energy are the eigenvalues of the Hamiltonian operator. Then this result says that the possible values of the energy are discrete and form an infinite sequence.

Note here the word \emph{compact}. In quantum mechanics, if we restrict a particle in a well $[0, 1]$, then it will have quantized energy level since the domain is compact. However, if the particle is free, then it can have any energy at all since we no longer have a compact domain. Similarly, angular momentum is quantized, since it describe rotations, which takes values in $S^1$, which is compact.

\begin{thm}[Sturm-Liouville completeness]
  The eigenfunctions are complete: any function $f: [a, b] \to \C$ (obeying appropriate boundary conditions) can be expanded as
  \[
    f(x) = \sum_n \hat{f}_n y_n(x),
  \]
  where
  \[
    \hat{f}_n = \int y^*_n(x) f(x) w(x)\;\d x.
  \]
\end{thm}

\begin{eg}[Fourier series as eigenfunction expansion]
  \label{eg:fourier-sturm-liouville}
  Consider $[a, b] = [-L, L]$ with $\mathcal{L} = \frac{\d^2}{\d x^2}$, $w = 1$, and periodic boundary conditions. The eigenvalue equation
  \[
    \frac{\d^2 y_n}{\d x^2} = \lambda_n y_n(x)
  \]
  has eigenfunctions
  \[
    y_n(x) = \exp\left(\frac{in\pi x}{L}\right), \quad n \in \Z,
  \]
  with eigenvalues
  \[
    \lambda_n = -\frac{n^2 \pi^2}{L^2}.
  \]
  This recovers the Fourier series as a special case of Sturm--Liouville theory.
\end{eg}

\begin{eg}[Hermite polynomials]
  \label{eg:hermite-polynomials}
  Consider the differential equation on $\R$:
  \[
    \frac{1}{2}H'' - xH' = \lambda H.
  \]
  To put this in Sturm--Liouville form, we compute $p(x) = \exp\left(-\int_0^x 2t \;\d t\right) = e^{-x^2}$ and $q(x) = 0$, giving
  \[
    \frac{\d}{\d x}\left(e^{-x^2}\frac{\d H}{\d x}\right) = -2\lambda e^{-x^2} H(x).
  \]
  The weight function is $w(x) = e^{-x^2}$.

  For self-adjointness on the unbounded domain, we require $e^{-x^2}H(x)^2 \to 0$ as $|x| \to \infty$, which ensures the boundary terms vanish.

  The eigenfunctions are the \emph{Hermite polynomials}:
  \[
    H_n(x) = (-1)^n e^{x^2}\frac{\d^n}{\d x^n}\left(e^{-x^2}\right), \quad n = 0, 1, 2, \ldots
  \]
  These are indeed polynomials: each differentiation of $e^{-x^2}$ produces terms containing $e^{-x^2}$, which cancels with the prefactor $e^{x^2}$.
\end{eg}

Just as for matrices, eigenfunction expansions provide a systematic method for solving forced differential equations.

\begin{prop}[Solving forced equations via eigenfunction expansion]
  \label{prop:forced-equation-eigenfunctions}
  Consider the equation $\mathcal{L} g = w(x) F(x)$, where $\mathcal{L}$ is a Sturm--Liouville operator with eigenfunctions $\{y_n\}$ and eigenvalues $\{\lambda_n\}$. If all $\lambda_n \neq 0$, then
  \[
    g(x) = \sum_{n}\frac{\hat{F}_n}{\lambda_n}y_n(x),
  \]
  where $\hat{F}_n = (y_n, F)_w$ are the expansion coefficients of $F$.
\end{prop}

\begin{proof}
  Expand $g(x) = \sum_n \hat{g}_n y_n(x)$ and $F(x) = \sum_n \hat{F}_n y_n(x)$. By linearity of $\mathcal{L}$:
  \[
    \mathcal{L}g = \sum_n \hat{g}_n \mathcal{L} y_n = \sum_n \hat{g}_n \lambda_n w(x) y_n(x).
  \]
  Comparing with $w(x) F(x) = w(x) \sum_n \hat{F}_n y_n(x)$ and using orthogonality, we get $\hat{g}_n \lambda_n = \hat{F}_n$, so $\hat{g}_n = \hat{F}_n / \lambda_n$.
\end{proof}

This can be reformulated using a Green's function.

\begin{defi}[Green's function]
  \label{def:greens-function}
  Let $\mathcal{L}$ be a Sturm--Liouville operator with eigenfunctions $\{y_n\}$, eigenvalues $\{\lambda_n\}$, and weight $w$. The \emph{Green's function} for $\mathcal{L}$ is
  \[
    G(x, t) = \sum_{n}\frac{1}{\lambda_n}y_n^*(t) y_n(x),
  \]
  provided all $\lambda_n \neq 0$.
\end{defi}

\begin{prop}[Solution via Green's function]
  \label{prop:greens-function-solution}
  The solution to $\mathcal{L} g = w(x) F(x)$ can be written as
  \[
    g(x) = \int_a^b G(x, t) F(t) w(t)\;\d t.
  \]
\end{prop}

\begin{proof}
  Using $\hat{F}_n = (y_n, F)_w = \int_a^b y_n^*(t) F(t) w(t)\,\d t$:
  \[
    g(x) = \sum_{n}\frac{\hat{F}_n}{\lambda_n} y_n(x) = \int_a^b \left(\sum_{n}\frac{1}{\lambda_n} y_n^*(t) y_n(x)\right) F(t) w(t)\;\d t = \int_a^b G(x, t) F(t) w(t)\;\d t,
  \]
  where we have interchanged sum and integral (justified under appropriate convergence conditions).
\end{proof}

\begin{remark}
  The Green's function $G(x, t)$ depends only on the operator $\mathcal{L}$ (through its eigenfunctions and eigenvalues), not on the forcing term $F$. It acts as the ``inverse'' of $\mathcal{L}$, analogous to the inverse matrix. Just as a matrix inverse exists when all eigenvalues are nonzero, the Green's function requires $\lambda_n \neq 0$ for all $n$.
\end{remark}

The weighted generalization of Parseval's theorem holds for Sturm--Liouville eigenfunctions.

\begin{thm}[Parseval's theorem for weighted inner products]
  \label{thm:parseval-weighted}
  Let $\{y_n\}$ be orthonormal eigenfunctions of a Sturm--Liouville operator with weight $w$. For any function $f$ with expansion $f = \sum_n \hat{f}_n y_n$, we have
  \[
    (f, f)_w = \sum_{n}|\hat{f}_n|^2.
  \]
\end{thm}

\begin{proof}
  Substituting the eigenfunction expansion:
  \begin{align*}
    (f, f)_w &= \int_a^b f^*(x) f(x) w(x) \;\d x\\
    &= \sum_{n, m}\hat{f}^*_n \hat{f}_m \int_a^b y_n^*(x) y_m(x)w(x)\;\d x\\
    &= \sum_{n, m}\hat{f}^*_n \hat{f}_m (y_n, y_m)_w \\
    &= \sum_{n}|\hat{f}_n|^2,
  \end{align*}
  using orthonormality $(y_n, y_m)_w = \delta_{nm}$.
\end{proof}

\subsection{Least squares approximation}
In practice, we must truncate infinite eigenfunction expansions. The following theorem shows that the standard expansion coefficients give the optimal truncation.

\begin{thm}[Least squares approximation]
  \label{thm:least-squares}
  Let $f: [a, b] \to \C$ and let $\{y_1, y_2, \ldots, y_N\}$ be orthonormal eigenfunctions with respect to weight $w$. Among all approximations of the form
  \[
    g(x) = \sum_{k = 1}^N c_k y_k(x),
  \]
  the weighted mean-square error $\|f - g\|_w^2 = (f - g, f - g)_w$ is minimized when
  \[
    c_k = \hat{f}_k = (y_k, f)_w,
  \]
  i.e., when the coefficients are the usual expansion coefficients.
\end{thm}

\begin{proof}
  Expanding the error:
  \[
    (f - g, f - g)_w = (f, f)_w + (g, g)_w - (f, g)_w - (g, f)_w.
  \]
  Using orthonormality and $g = \sum_k c_k y_k$, we have $(g, g)_w = \sum_k |c_k|^2$, $(f, g)_w = \sum_k \hat{f}_k^* c_k$, and $(g, f)_w = \sum_k c_k^* \hat{f}_k$.

  To minimize, we differentiate with respect to $c_j$ (treating $c_j$ and $c_j^*$ as independent, which can be justified by varying real and imaginary parts separately):
  \[
    \frac{\partial}{\partial c_j}(f - g, f - g)_w = c^*_j - \hat{f}_j^* = 0.
  \]
  Thus $c_j = \hat{f}_j$ at the critical point. The second derivative $\frac{\partial^2}{\partial c_i^* \partial c_j}(f-g,f-g)_w = \delta_{ij} > 0$ confirms this is a minimum.
\end{proof}

\begin{remark}
  This result shows that truncating the eigenfunction expansion at any point gives the best possible approximation using that many terms. There is no better choice of coefficients.
\end{remark}

\section{Partial differential equations}
In this section, we apply the theory developed so far to solve partial differential equations (PDEs). The primary technique is \emph{separation of variables}, which reduces a PDE to a system of ordinary differential equations.

\subsection{Laplace's equation}
\begin{defi}[Laplacian and Laplace's equation]
  \label{def:laplacian}
  The \emph{Laplacian} (or \emph{Laplace operator}) on $\R^n$ is the differential operator
  \[
    \nabla^2 = \sum_{i = 1}^n \frac{\partial^2}{\partial x_i^2}.
  \]
  \emph{Laplace's equation} for a function $\phi: \Omega \subseteq \R^n \to \C$ is
  \[
    \nabla^2 \phi = 0.
  \]
\end{defi}

\begin{defi}[Harmonic function]
  \label{def:harmonic}
  A function satisfying Laplace's equation is called \emph{harmonic}.
\end{defi}

\begin{remark}
  Laplace's equation arises in many physical contexts:
  \begin{itemize}
    \item For a conservative force $\mathbf{F} = -\nabla \phi$ in a source-free region where $\nabla \cdot \mathbf{F} = 0$ (e.g., electrostatics without charges).
    \item As the steady-state limit of the heat equation $\frac{\partial \phi}{\partial t} = \kappa \nabla^2 \phi$ when $\frac{\partial \phi}{\partial t} = 0$.
    \item As the time-independent case of the wave equation $\frac{\partial^2 \phi}{\partial t^2} = c^2 \nabla^2 \phi$.
  \end{itemize}
\end{remark}

\begin{defi}[Dirichlet and Neumann boundary conditions]
  \label{def:dirichlet-neumann}
  Let $\Omega$ be a domain with boundary $\partial\Omega$. A \emph{Dirichlet boundary condition} specifies the value of $\phi$ on $\partial\Omega$:
  \[
    \phi|_{\partial\Omega} = f.
  \]
  A \emph{Neumann boundary condition} specifies the normal derivative:
  \[
    \mathbf{n} \cdot \nabla\phi|_{\partial\Omega} = g.
  \]
\end{defi}

\begin{thm}[Uniqueness for the Dirichlet problem]
  \label{thm:laplace-uniqueness}
  Let $\Omega \subseteq \R^n$ be a compact domain with boundary $\partial \Omega$, and let $f: \partial \Omega\to \R$. Then there is at most one solution to Laplace's equation $\nabla^2 \phi = 0$ on $\Omega$ with $\phi|_{\partial \Omega} = f$.
\end{thm}

\begin{proof}
  Suppose $\phi_1$ and $\phi_2$ are both solutions with $\phi_1|_{\partial \Omega} = \phi_2|_{\partial \Omega} = f$. Let $\Phi = \phi_1 - \phi_2$, so $\Phi = 0$ on $\partial\Omega$ and $\nabla^2\Phi = 0$ in $\Omega$. Then
  \[
    0 = \int_\Omega \Phi \nabla^2 \Phi \;\d^n x = -\int_\Omega |\nabla \Phi|^2\;\d^n x + \int_{\partial \Omega} \Phi (\nabla \Phi \cdot \mathbf{n}) \;\d^{n -1 }x.
  \]
  The boundary term vanishes since $\Phi = 0$ on $\partial\Omega$, leaving
  \[
    \int_\Omega |\nabla \Phi|^2\;\d^n x = 0.
  \]
  Since $|\nabla \Phi|^2 \geq 0$ with equality if and only if $\nabla \Phi = 0$, we conclude $\Phi$ is constant. Since $\Phi = 0$ on $\partial\Omega$, we have $\Phi = 0$ throughout, i.e., $\phi_1 = \phi_2$.
\end{proof}

\begin{remark}
  For Neumann boundary conditions, uniqueness holds up to an additive constant.
\end{remark}

\subsection{Laplace's equation in the unit disk in \texorpdfstring{$\R^2$}{R2}}
\begin{eg}[Laplace's equation on the unit disk]
  \label{eg:laplace-unit-disk}
  Let $\Omega = \{(x, y) \in \R^2: x^2 + y^2 \leq 1\}$ be the closed unit disk. In two dimensions, Laplace's equation can be written using complex coordinates $z = x + iy$ and $\bar{z} = x - iy$ as
  \[
    \nabla^2 \phi = \frac{\partial^2 \phi}{\partial x^2} + \frac{\partial^2 \phi}{\partial y^2} = 4\frac{\partial^2 \phi}{\partial z \partial \bar{z}} = 0.
  \]
  The general solution is therefore $\phi(z, \bar z) = \psi(z) + \chi(\bar z)$ for holomorphic functions $\psi$ and $\chi$.

  Suppose we seek a solution satisfying the Dirichlet condition $\phi|_{\partial \Omega} = f(\theta)$, where $\theta$ parametrizes the boundary circle. We expand $f$ as a Fourier series:
  \[
    f(\theta) = \sum_{n \in \Z}\hat{f}_n e^{in\theta} = \hat{f}_0 + \sum_{n = 1}^\infty \hat{f}_n e^{in\theta} + \sum_{n = 1}^\infty \hat{f}_{-n} e^{-in\theta}.
  \]
  On the boundary, $z = e^{i\theta}$ and $\bar{z} = e^{-i\theta}$, so we can write
  \[
    f(\theta) = \hat{f}_0 + \sum_{n = 1}^\infty \hat{f}_n z^n + \sum_{n = 1}^\infty \hat{f}_{-n}\bar{z}^n.
  \]
  This suggests defining the solution on the entire disk by
  \[
    \phi(z, \bar z) = \hat{f}_0 + \sum_{n = 1}^\infty \hat{f}_n z^n + \sum_{n = 1}^\infty \hat{f}_{-n}\bar{z}^n.
  \]
  This function:
  \begin{enumerate}
    \item satisfies the boundary condition by construction;
    \item converges for $|z| < 1$ whenever the Fourier series for $f$ converges;
    \item has the form $\psi(z) + \chi(\bar{z})$, hence solves Laplace's equation.
  \end{enumerate}
  By uniqueness, this is the solution.
\end{eg}

\subsection{Separation of variables}
The complex variable method is specific to $\R^2$. In higher dimensions, we use \emph{separation of variables}: we seek solutions of the form $\phi = X_1(x_1) X_2(x_2) \cdots X_n(x_n)$, where each factor depends on only one variable.

\begin{eg}[Laplace's equation on a half-infinite rectangular prism]
  \label{eg:laplace-rectangular}
  Let $\Omega = \{(x, y, z) \in \R^3: 0 \leq x \leq a, 0 \leq y \leq b, z \geq 0\}$. We seek a solution $\psi$ to $\nabla^2\psi = 0$ with boundary conditions:
  \begin{align*}
    \psi(0, y, z) &= \psi(a, y, z) = 0, & &\text{(homogeneous in $x$)}\\
    \psi(x, 0, z) &= \psi(x, b, z) = 0, & &\text{(homogeneous in $y$)}\\
    \lim_{z\to \infty} \psi(x, y, z) &= 0, & &\text{(decay at infinity)}\\
    \psi(x, y, 0) &= f(x, y), & &\text{(inhomogeneous at $z=0$)}
  \end{align*}
  where $f$ is a given function.

  \textbf{Step 1: Separation ansatz.} We try $\psi(x, y, z) = X(x)Y(y)Z(z)$. Substituting into $\nabla^2 \psi = 0$:
  \[
    YZX'' + XZY'' + XYZ'' = 0 \implies \frac{X''}{X} + \frac{Y''}{Y} + \frac{Z''}{Z} = 0.
  \]
  Since each term depends on only one variable, and the sum is constant (zero), each term must be separately constant:
  \[
    X'' = -\lambda X,\quad Y'' = -\mu Y,\quad Z'' = (\lambda + \mu) Z.
  \]
  The signs are chosen for convenience.

  \textbf{Step 2: Solve the ODEs.} The general solutions are
  \begin{align*}
    X &= a\sin \sqrt{\lambda} x + b \cos \sqrt{\lambda} x,\\
    Y &= c\sin \sqrt{\mu} y + d \cos \sqrt{\mu} y,\\
    Z &= g\exp(\sqrt{\lambda + \mu} z) + h\exp(-\sqrt{\lambda + \mu} z).
  \end{align*}

  \textbf{Step 3: Apply homogeneous boundary conditions.} We impose the conditions that $\psi$ vanishes at the walls and at infinity:
  \begin{itemize}
    \item At $x = 0$: $\psi(0, y, z) = 0$ requires $b = 0$.
    \item At $x = a$: $\psi(a, y, z) = 0$ requires $\lambda = \left(\frac{n\pi}{a}\right)^2$ for $n \in \N$.
    \item At $y = 0$: $\psi(x, 0, z) = 0$ requires $d = 0$.
    \item At $y = b$: $\psi(x, b, z) = 0$ requires $\mu = \left(\frac{m\pi}{b}\right)^2$ for $m \in \N$.
    \item As $z \to \infty$: $\psi(x, y, z) \to 0$ requires $g = 0$.
  \end{itemize}
  Thus, for each $n, m \in \N$, we have a separated solution
  \[
    \psi_{n,m}(x, y, z) = A_{n, m}\sin \left(\frac{n\pi x}{a}\right) \sin \left(\frac{m\pi y}{b}\right) \exp(-s_{n, m} z),
  \]
  where $A_{n, m}$ is an arbitrary constant and
  \[
    s_{n, m}^2 = \left(\frac{n^2}{a^2} + \frac{m^2}{b^2}\right)\pi^2.
  \]
  This satisfies the homogeneous boundary conditions but not yet the inhomogeneous condition at $z = 0$.

  \textbf{Step 4: Form general solution and determine coefficients.} By linearity, the general solution satisfying all homogeneous boundary conditions is
  \[
    \psi(x, y, z) = \sum_{n, m = 1}^{\infty} A_{n, m} \sin\left(\frac{n\pi x}{a}\right) \sin \left(\frac{m\pi y}{b}\right) \exp(-s_{n, m} z).
  \]
  To satisfy the inhomogeneous condition $\psi(x, y, 0) = f(x, y)$, we need
  \[
    \sum_{n, m = 1}^{\infty} A_{n, m} \sin\left(\frac{n\pi x}{a}\right) \sin \left(\frac{m\pi y}{b}\right) = f(x, y). \tag{$\dagger$}
  \]
  We use the orthogonality relation
  \[
    \int_0^a \sin\left(\frac{k\pi x}{a}\right) \sin \left(\frac{n\pi x}{a}\right)\;\d x = \delta_{k, n} \frac{a}{2}.
  \]
  Multiplying $(\dagger)$ by $\sin \left(\frac{k\pi x}{a}\right)$ and integrating over $[0,a]$:
  \[
    \sum_{n, m = 1}^\infty A_{n, m}\int_0^a \sin \left(\frac{k\pi x}{a}\right) \sin \left(\frac{n\pi x}{a}\right)\;\d x \sin\left(\frac{m\pi y}{b}\right) = \int_0^a \sin\left(\frac{k \pi x}{a}\right)f(x, y) \;\d x.
  \]
  Applying orthogonality:
  \[
    \frac{a}{2}\sum_{m = 1}^\infty A_{k, m} \sin\left(\frac{m\pi y}{b}\right) = \int_0^a \sin\left(\frac{k\pi x}{a}\right) f(x, y)\;\d x.
  \]
  Repeating with the $y$ variable:
  \[
    \frac{ab}{4}A_{k, j} = \int_{[0, a]\times [0, b]}\sin \left(\frac{k\pi x}{a}\right) \sin\left(\frac{j\pi y}{b}\right) f(x, y)\;\d x\;\d y.\tag{$*$}
  \]
  Thus the solution is
  \[
    \psi(x, y, z) = \sum_{n, m = 1}^{\infty} A_{n, m} \sin\left(\frac{n\pi x}{a}\right) \sin \left(\frac{m\pi y}{b}\right) \exp(-s_{n, m} z)
  \]
  where $A_{m, n}$ is given by $(*)$. Uniqueness of the Dirichlet problem guarantees this is the solution.
\end{eg}

\begin{remark}
  If we had a boundary condition at finite $z$ (say $0 \leq z \leq c$), both exponentials $\exp (\pm \sqrt{\lambda + \mu} z)$ would contribute. Similarly, if $\psi$ does not vanish at the other boundaries, the $\cos$ terms would also contribute.
\end{remark}

In general, computing the coefficients $A_{m, n}$ requires evaluating the integral in $(*)$, which may be difficult.
\begin{eg}[Rectangular prism with constant boundary]
  \label{eg:rectangular-constant}
  Suppose $f(x, y) = 1$. Then
  \[
    A_{m, n} = \frac{4}{ab} \int_0^a \sin \left(\frac{n\pi x}{a}\right)\;\d x \int_0^ b \sin \left(\frac{m\pi y}{b}\right)\;\d y =
    \begin{cases}
      \frac{16}{\pi^2 mn} & n, m\text{ both odd}\\
      0 & \text{otherwise}
    \end{cases}
  \]
  Hence we have
  \[
    \psi(x, y, z) = \frac{16}{\pi^2}\sum_{n, m\text{ odd}} \frac{1}{nm}\sin\left(\frac{n\pi x}{a}\right) \sin \left(\frac{m\pi y}{b}\right) \exp(-s_{m, n}z).
  \]
\end{eg}

\subsection{Laplace's equation in spherical polar coordinates}
For spherically symmetric domains, we use spherical coordinates.

\begin{defi}[Spherical coordinates]
  \label{def:spherical-coordinates}
  \emph{Spherical coordinates} $(r, \theta, \phi)$ on $\R^3$ are defined by
  \[
    x = r\sin \theta \cos \phi,\quad y = r\sin \theta \sin \phi,\quad z = r\cos \theta,
  \]
  where $r \geq 0$, $0 \leq \theta \leq \pi$ (polar angle), and $0 \leq \phi < 2\pi$ (azimuthal angle). The Laplacian in these coordinates is
  \[
    \nabla^2 = \frac{1}{r^2}\frac{\partial}{\partial r}\left(r^2 \frac{\partial}{\partial r}\right) + \frac{1}{r^2 \sin \theta} \frac{\partial}{\partial \theta}\left(\sin \theta \frac{\partial}{\partial \theta}\right) + \frac{1}{r^2 \sin^2 \theta}\frac{\partial^2}{\partial\phi^2},
  \]
  and the volume element is $\d V = r^2 \sin \theta \;\d r\;\d \theta \;\d \phi$.
\end{defi}

Consider the domain $\Omega = \{(x, y, z)\in \R^3: \sqrt{x^2 + y^2 + z^2} \leq a\}$. We restrict to \emph{axisymmetric solutions} where $\psi(r, \theta, \phi) = \psi(r, \theta)$ does not depend on $\phi$.

Applying separation of variables with $\psi(r, \theta) = R(r) \Theta(\theta)$, Laplace's equation becomes
\[
  \frac{\psi}{r^2}\left[\frac{1}{R}\frac{\d}{\d r}(r^2 R') + \frac{1}{\Theta \sin \theta}\frac{\d}{\d \theta}\left(\sin \theta \frac{\d \Theta}{\d \theta}\right)\right] = 0.
\]
Since each term depends on only one variable and the sum is constant (zero), each must be separately constant:
\[
  \frac{\d}{\d r}\left(r^2 \frac{\d R}{\d r}\right) = \lambda R,\quad \frac{\d}{\d \theta}\left(\sin \theta \frac{\d \Theta}{\d \theta}\right) = -\lambda \sin \theta \Theta.
\]
Both equations are eigenfunction equations for Sturm-Liouville operators. The angular equation has weight function $w(\theta) = \sin \theta$.
\subsubsection{Legendre polynomials}

\begin{defi}[Legendre's equation]
  \label{def:legendre-equation}
  The angular equation from separation of variables is \emph{Legendre's equation}. Using the substitution $x = \cos \theta$ (where $x \in [-1, 1]$), it takes the form
  \[
    \frac{\d}{\d x}\left[(1 - x^2) \frac{\d \Theta}{\d x}\right] = -\lambda \Theta.
  \]
  This is a Sturm-Liouville equation with $p(x) = 1 - x^2$ and $q(x) = 0$.
\end{defi}

\begin{remark}
  Since $p(x) = 1 - x^2$ vanishes at the boundary points $x = \pm 1$, the boundary terms in the self-adjointness condition vanish automatically. The operator is self-adjoint provided solutions remain regular (finite) at $x = \pm 1$.
\end{remark}

We seek power series solutions $\Theta(x) = \sum_{n = 0}^\infty a_n x^n$. Substituting into Legendre's equation:
\[
  (1 - x^2) \sum_{n = 0}^\infty a_n n(n - 1)x^{n - 2} - 2\sum_{n = 0}^\infty a_n nx^n + \lambda \sum_{n = 0}^\infty a_n x^n = 0.
\]
Matching coefficients yields the recurrence relation
\[
  a_{n + 2} = \frac{n(n + 1) - \lambda}{(n + 2)(n + 1)} a_n.
\]
This relates even coefficients to even and odd to odd, giving two linearly independent solutions:
\begin{align*}
  \Theta_0(x) &= a_0 \left[1 - \frac{\lambda}{2} x^2 - \frac{(6 - \lambda)\lambda}{4!}x^4 + \cdots\right] \quad \text{(even)}\\
  \Theta_1(x) &= a_1 \left[x + \frac{(2 - \lambda)}{3!}x^3 + \frac{(12 - \lambda)(2 - \lambda)}{5!}x^5 + \cdots\right] \quad \text{(odd)}.
\end{align*}

\begin{prop}[Eigenvalue quantization]
  \label{prop:legendre-eigenvalues}
  For solutions to remain regular at $x = \pm 1$, we must have $\lambda = \ell(\ell + 1)$ for some $\ell \in \N_0$.
\end{prop}
\begin{proof}
  Since $\lim_{n \to \infty}\frac{a_{n + 2}}{a_n} = 1$, the ratio test is inconclusive at $x = \pm 1$, and more refined tests show the infinite series diverges there. For regularity, the series must truncate. Setting $\lambda = \ell(\ell + 1)$ makes $a_{\ell+2} = 0$, truncating to a polynomial.
\end{proof}

\begin{remark}
  This eigenvalue quantization is the mathematical origin of angular momentum quantization in quantum mechanics.
\end{remark}

\begin{defi}[Legendre polynomials]
  \label{def:legendre-polynomials}
  The \emph{Legendre polynomials} $P_\ell(x)$ for $\ell \in \N_0$ are the polynomial solutions to Legendre's equation with $\lambda = \ell(\ell+1)$, normalized so that $P_\ell(1) = 1$. The first few are:
  \begin{align*}
    P_0(x) &= 1, & P_1(x) &= x, & P_2(x) &= \frac{1}{2}(3x^2 - 1), & P_3(x) &= \frac{1}{2}(5x^3 - 3x).
  \end{align*}
\end{defi}

\begin{prop}[Rodrigues formula]
  \label{prop:rodrigues}
  The Legendre polynomials satisfy
  \[
    P_\ell(x) = \frac{1}{2^\ell \ell!} \frac{\d^\ell}{\d x^\ell} (x^2 - 1)^\ell.
  \]
\end{prop}
\begin{proof}
  One can verify this formula satisfies Legendre's equation. To check normalization:
  \begin{align*}
    P_\ell(1) &= \left.\frac{1}{2^\ell \ell!} \frac{\d^\ell}{\d x^\ell}[(x - 1)^\ell (x + 1)^\ell]\right|_{x = 1}
    = \frac{1}{2^\ell \ell!} \left[\ell!(x + 1)^\ell + (x - 1)(\cdots)\right]_{x = 1}
    = 1. \qedhere
  \end{align*}
\end{proof}

\begin{lemma}
  \label{lem:derivative-factor}
  For $0 \leq k \leq \ell$, we have $\frac{\d^k}{\d x^k}(x^2 - 1)^\ell = Q_{\ell, k}(x)(x^2 - 1)^{\ell - k}$ for some polynomial $Q_{\ell, k}$ of degree $k$.
\end{lemma}
\begin{proof}
  By induction on $k$. The base case $k=0$ is trivial. For the inductive step:
  \begin{align*}
    \frac{\d^{k + 1}}{\d x^{k + 1}} (x^2 - 1)^\ell &= \frac{\d}{\d x}\left[Q_{\ell, k}(x) (x^2 - 1)^{\ell - k}\right] \\
    &= (x^2 - 1)^{\ell - k - 1}\Big[Q_{\ell, k}'(x^2 - 1) + 2(\ell - k)Q_{\ell, k} x\Big].
  \end{align*}
  Since $\deg Q_{\ell, k} = k$, the bracketed expression has degree $k + 1$.
\end{proof}

\begin{thm}[Orthogonality of Legendre polynomials]
  \label{thm:legendre-orthogonality}
  The Legendre polynomials satisfy
  \[
    (P_\ell, P_{\ell'}) = \int_{-1}^1 P_\ell(x) P_{\ell'}(x) \;\d x = \frac{2\delta_{\ell\ell'}}{2\ell + 1}.
  \]
\end{thm}
\begin{proof}
  Using the Rodrigues formula:
  \[
    (P_\ell, P_{\ell'}) = \frac{1}{2^\ell \ell!}\int_{-1}^1 \frac{\d^\ell}{\d x^\ell} (x^2 - 1)^\ell P_{\ell'}(x)\;\d x.
  \]
  Integrating by parts, the boundary terms vanish by Lemma~\ref{lem:derivative-factor} since each derivative of $(x^2-1)^\ell$ retains a factor of $(x^2-1)$ until the $\ell$-th derivative. Thus we can transfer derivatives to $P_{\ell'}$.

  If $\ell' < \ell$, integrating by parts $\ell'$ times gives the $\ell'$-th derivative of $P_{\ell'}$, which is a constant times $\ell'!$. But we need $\ell$ derivatives total, and the remaining $(\ell - \ell')$ derivatives annihilate the constant. Hence $(P_\ell, P_{\ell'}) = 0$ for $\ell \neq \ell'$.

  The normalization $2/(2\ell+1)$ follows from direct calculation.
\end{proof}

\begin{prop}[Roots of Legendre polynomials]
  \label{prop:legendre-roots}
  All $\ell$ roots of $P_\ell(x)$ are real and lie in $(-1, 1)$.
\end{prop}
\begin{proof}
  Suppose only $m < \ell$ roots lie in $(-1, 1)$. Let $Q_m(x) = \prod_{r = 1}^m (x - x_r)$ where $\{x_1, \ldots, x_m\}$ are these roots. Then $P_{\ell}(x)Q_m(x)$ does not change sign on $(-1, 1)$: the roots outside $(-1,1)$ contribute no sign changes there, and the roots inside appear squared. Hence
  \[
    \pm\int_{-1}^1 P_\ell(x) Q_m(x) \;\d x > 0.
  \]
  But expanding $Q_m(x) = \sum_{r = 0}^m q_r P_r(x)$ and using orthogonality, $(P_\ell, Q_m) = 0$ since $m < \ell$. Contradiction.
\end{proof}

\subsubsection{Solution to radial part}
With $\Theta(\theta) = P_\ell(\cos \theta)$, the radial equation becomes
\[
  (r^2 R')' = \ell(\ell + 1)R.
\]

\begin{prop}[Radial solutions]
  \label{prop:radial-solutions}
  Trying $R(r) = r^\alpha$ gives $\alpha(\alpha + 1) = \ell(\ell + 1)$, so $\alpha = \ell$ or $\alpha = -(\ell + 1)$.
\end{prop}

\begin{thm}[General solution in spherical coordinates]
  \label{thm:laplace-spherical}
  The general axisymmetric solution to Laplace's equation is
  \[
    \phi(r, \theta) = \sum_{\ell=0}^\infty \left(a_\ell r^\ell + \frac{b_\ell}{r^{\ell + 1}}\right) P_\ell(\cos \theta).
  \]
  For regularity at $r = 0$, we require $b_\ell = 0$ for all $\ell$.
\end{thm}

\begin{eg}[Dirichlet problem on a ball]
  \label{eg:dirichlet-ball}
  To solve $\nabla^2\phi = 0$ on $\{r \leq a\}$ with $\phi(a, \theta) = f(\theta)$: expand $f$ in Legendre polynomials,
  \[
    f(\theta) = \sum_{\ell = 0}^\infty F_\ell P_\ell(\cos \theta), \quad F_\ell = \frac{2\ell+1}{2}\int_{-1}^1 P_{\ell}(x) f(\arccos x) \;\d x.
  \]
  Then the solution is
  \[
    \phi(r, \theta) = \sum_{\ell = 0}^\infty F_\ell \left(\frac{r}{a}\right)^\ell P_\ell(\cos \theta).
  \]
\end{eg}
\subsection{Multipole expansions for Laplace's equation}

\begin{prop}[Fundamental solution]
  \label{prop:fundamental-solution}
  The function $\phi(\mathbf{r}) = \frac{1}{|\mathbf{r} - \mathbf{r}'|}$ solves Laplace's equation $\nabla^2 \phi = 0$ for all $\mathbf{r}\in \R^3 \setminus \{\mathbf{r}'\}$.
\end{prop}

\begin{thm}[Multipole expansion]
  \label{thm:multipole}
  For $|\mathbf{r}| < |\mathbf{r}'|$,
  \[
    \frac{1}{|\mathbf{r} - \mathbf{r}'|}= \frac{1}{r'} \sum_{\ell = 0}^\infty \left(\frac{r}{r'}\right)^\ell P_\ell(\hat{\mathbf{r}}\cdot \hat{\mathbf{r}}'),
  \]
  where $r = |\mathbf{r}|$, $r' = |\mathbf{r}'|$, and $\hat{\mathbf{r}} = \mathbf{r}/r$.
\end{thm}
\begin{proof}
  For $\mathbf{r}' = \hat{\mathbf{k}}$ (unit vector in $z$ direction), we have
  \[
    \frac{1}{|\mathbf{r} - \hat{\mathbf{k}}|} = \frac{1}{\sqrt{r^2 + 1 - 2r \cos \theta}} = \sum_{\ell = 0}^\infty c_\ell r^\ell P_\ell(\cos \theta).
  \]
  At $\theta = 0$, using $P_\ell(1) = 1$:
  \[
    \sum_{\ell = 0}^\infty c_\ell r^\ell = \frac{1}{1 - r} = \sum_{\ell = 0}^\infty r^\ell,
  \]
  so $c_\ell = 1$ for all $\ell$. The general case follows by rotational symmetry.
\end{proof}

\begin{defi}[Multipole terminology]
  \label{def:multipoles}
  Expanding the multipole series:
  \[
    \frac{1}{|\mathbf{r} - \mathbf{r}'|} = \underbrace{\frac{1}{r'}}_{\text{monopole}} + \underbrace{\frac{r}{r'^2}\hat{\mathbf{r}}\cdot \hat{\mathbf{r}}'}_{\text{dipole}} + \cdots
  \]
  In electromagnetism, the monopole term corresponds to a point charge, and the dipole term to an electric dipole.
\end{defi}

\subsection{Laplace's equation in cylindrical coordinates}

\begin{defi}[Cylindrical coordinates]
  \label{def:cylindrical-coordinates}
  \emph{Cylindrical coordinates} $(r, \theta, z)$ on $\R^3$ have Laplacian
  \[
    \nabla^2 \phi = \frac{1}{r}\frac{\partial}{\partial r}\left(r \frac{\partial \phi}{\partial r}\right) + \frac{1}{r^2}\frac{\partial^2 \phi}{\partial \theta^2} + \frac{\partial^2 \phi}{\partial z^2}.
  \]
\end{defi}

\begin{eg}[Laplace's equation on a half-cylinder]
  \label{eg:laplace-cylinder}
  Let $\Omega = \{(r, \theta, z) \in \R^3: r \leq a, z \geq 0\}$. We seek a regular solution to $\nabla^2\phi = 0$ with boundary conditions:
  \begin{align*}
    \phi(a, \theta, z) &= 0, & \phi(r, \theta, 0) &= f(r, \theta), & \lim_{z\to \infty} \phi(r, \theta, z) &= 0.
  \end{align*}

  \textbf{Step 1: Separation of variables.} With $\phi(r, \theta, z) = R(r) \Theta(\theta)Z(z)$:
  \[
    \frac{1}{rR} (rR')' + \frac{1}{r^2}\frac{\Theta''}{\Theta} + \frac{Z''}{Z} = 0.
  \]
  The separated equations are $Z'' = \mu Z$, $\Theta'' = -\lambda \Theta$, and
  \[
    r^2 R'' + rR' + (\mu r^2 - \lambda)R = 0.
  \]

  \textbf{Step 2: Angular and axial parts.} Periodicity in $\theta$ requires
  \[
    \Theta(\theta) = a_n \sin n\theta + b_n \cos n\theta, \quad \lambda = n^2,\quad n \in \N_0.
  \]
  Decay as $z \to \infty$ requires $Z(z) = c_\mu e^{-\sqrt{\mu}z}$ (with $\mu > 0$).

  \textbf{Step 3: Radial equation.} The radial equation is Sturm-Liouville type:
  \[
    \frac{\d}{\d r}\left(r \frac{\d R}{\d r}\right) - \frac{n^2}{r}R = -\mu rR,
  \]
  with $p(r) = r$, $q(r) = -n^2/r$, $w(r) = r$. Substituting $x = r\sqrt{\mu}$ gives Bessel's equation.
\end{eg}

\begin{defi}[Bessel's equation and Bessel functions]
  \label{def:bessel}
  \emph{Bessel's equation of order $n$} is
  \[
    x^2 \frac{\d^2 R}{\d x^2} + x\frac{\d R}{\d x} + (x^2 - n^2)R = 0.
  \]
  Its two linearly independent solutions are \emph{Bessel functions}:
  \begin{itemize}
    \item $J_n(x)$: Bessel function of the first kind, regular at the origin with $J_n(x) \sim x^n$ as $x \to 0$.
    \item $Y_n(x)$: Bessel function of the second kind, singular at the origin with $Y_0(x) \sim \ln x$ and $Y_n(x) \sim x^{-n}$ for $n > 0$.
  \end{itemize}
\end{defi}

\begin{remark}
  Bessel functions resemble decaying sine waves, but their zeros are not regularly spaced.
\end{remark}

Continuing Example~\ref{eg:laplace-cylinder}:

\textbf{Step 4: Apply regularity and boundary conditions.} For regularity at $r = 0$, we discard $Y_n$ terms. The separated solution is
\[
  \phi(r, \theta, z) = (a_n \sin n \theta + b_n \cos n\theta) e^{-\sqrt{\mu}z} J_n (r\sqrt{\mu}).
\]
The boundary condition $\phi(a, \theta, z) = 0$ requires $J_n(a \sqrt{\mu}) = 0$, so
\[
  \sqrt{\mu} = \frac{k_{ni}}{a},
\]
where $k_{ni}$ denotes the $i$-th positive zero of $J_n$.

\textbf{Step 5: General solution.} The general solution satisfying homogeneous conditions is
\[
  \phi(r, \theta, z) = \sum_{n = 0}^\infty \sum_{i=1}^\infty (A_{ni} \sin n \theta + B_{ni}\cos n\theta) \exp\left(-\frac{k_{ni}}{a}z\right) J_n \left(\frac{k_{ni}r}{a}\right).
\]

\begin{prop}[Orthogonality of Bessel functions]
  \label{prop:bessel-orthogonality}
  With weight function $w(r) = r$:
  \[
    \int_0^a J_n\left(\frac{k_{nj}r}{a}\right)J_n\left(\frac{k_{ni}r}{a}\right) r\;\d r = \frac{a^2}{2}\delta_{ij}[J_n'(k_{ni})]^2.
  \]
  This relates zeros of Bessel functions of the \emph{same order} $n$.
\end{prop}

\textbf{Step 6: Determine coefficients.} Imposing $\phi(r, \theta, 0) = f(r, \theta)$ and using orthogonality in both $\theta$ and $r$:
\[
  B_{mj} = \frac{2}{\pi a^2 [J'_m(k_{mj})]^2}\int_0^a \int_{-\pi}^\pi \cos m\theta \, J_m\left(\frac{k_{mj}r}{a}\right) f(r, \theta)\,r \;\d r \;\d \theta.
\]

\begin{remark}
  In practice, these integrals are often computed numerically, since the zeros $k_{mj}$ of Bessel functions have no simple closed form.
\end{remark}

\subsection{The heat equation}

\begin{defi}[Heat equation]
  \label{def:heat-equation}
  The \emph{heat equation} on $\Omega \subseteq \R^n$ for a function $\phi: \Omega \times [0,\infty) \to \R$ is
  \[
    \frac{\partial \phi}{\partial t} = \kappa \nabla^2 \phi,
  \]
  where $\kappa > 0$ is the \emph{diffusion constant}. Physically, $\phi$ represents temperature and the equation states that the rate of change of temperature is proportional to the Laplacian.
\end{defi}

\begin{remark}
  In equilibrium ($\partial\phi/\partial t = 0$), the heat equation reduces to Laplace's equation.
\end{remark}

\begin{prop}[Conservation of total heat]
  \label{prop:heat-conservation}
  If $\phi$ solves the heat equation and $\nabla \phi \to 0$ as $|\mathbf{x}|\to \infty$, then
  \[
    \frac{\d}{\d t}\int_{\R^n} \phi(\mathbf{x}, t)\;\d^n x = 0.
  \]
\end{prop}
\begin{proof}
  \[
    \frac{\d}{\d t}\int_{\R^n} \phi(\mathbf{x}, t)\;\d^n x = \int_{\R^n} \frac{\partial \phi}{\partial t}\; \d^n x = \kappa \int_{\R^n} \nabla^2 \phi \;\d^n x = 0,
  \]
  since the last integral vanishes by the divergence theorem.
\end{proof}

\begin{prop}[Symmetries of the heat equation]
  \label{prop:heat-symmetries}
  If $\phi(\mathbf{x}, t)$ solves the heat equation, so do:
  \begin{enumerate}
    \item Translation: $\phi_1(\mathbf{x}, t) = \phi(\mathbf{x} - \mathbf{x}_0, t - t_0)$
    \item Scaling: $\phi_2(\mathbf{x}, t) = A \phi(\lambda \mathbf{x}, \lambda^2 t)$ for any $A, \lambda \in \R$.
  \end{enumerate}
\end{prop}

The scaling symmetry suggests seeking self-similar solutions. If $\phi_2 = \lambda^n \phi(\lambda \mathbf{x}, \lambda^2 t)$ preserves total heat, setting $\lambda = 1/\sqrt{\kappa t}$ suggests the ansatz
\[
  \phi(\mathbf{x}, t) = \frac{1}{(\kappa t)^{n/2}} F\left(\frac{\mathbf{x}}{\sqrt{\kappa t}}\right) = \frac{1}{(\kappa t)^{n/2}}F(\boldsymbol\eta), \quad \boldsymbol\eta = \frac{\mathbf{x}}{\sqrt{\kappa t}}.
\]

\begin{eg}[Self-similar solution in one dimension]
  \label{eg:heat-1d}
  In $1+1$ dimensions, we seek $\phi(x,t) = \frac{1}{\sqrt{\kappa t}} F(\eta)$ where $\eta = x/\sqrt{\kappa t}$.

  Computing derivatives:
  \[
    \frac{\partial\phi}{\partial t} = \frac{-1}{2\sqrt{\kappa t^3}}[F + \eta F'], \quad \kappa\frac{\partial^2 \phi}{\partial x^2} = \frac{1}{\sqrt{\kappa t^3}}F''.
  \]
  The heat equation becomes $(2F' + \eta F)' = 0$, so $2F' + \eta F = C$. With decay at infinity, $C = 0$, giving $F' = -\frac{\eta}{2}F$, hence
  \[
    F(\eta) = a \exp\left(-\frac{\eta^2}{4}\right).
  \]
  Normalizing to $\int_{-\infty}^\infty \phi \,dx = 1$ gives $a = 1/\sqrt{4\pi}$.
\end{eg}

\begin{defi}[Heat kernel]
  \label{def:heat-kernel}
  The \emph{heat kernel} (or \emph{fundamental solution}) on $\R^n$ is
  \[
    K(\mathbf{x}, t; \mathbf{x}_0, t_0) = \frac{1}{(4\pi\kappa (t - t_0))^{n/2}} \exp\left(-\frac{|\mathbf{x} - \mathbf{x}_0|^2}{4\kappa(t - t_0)}\right), \quad t > t_0.
  \]
  This is a Gaussian centered at $\mathbf{x}_0$ with standard deviation $\sqrt{2\kappa(t-t_0)}$. As $t$ increases, the solution spreads out and flattens while preserving total integral.
\end{defi}

\begin{prop}[Positivity of Laplacian eigenvalues]
  \label{prop:laplacian-eigenvalues}
  On a closed compact domain $\Omega$ (or with Dirichlet boundary conditions), all eigenvalues of $-\nabla^2$ are non-negative.
\end{prop}
\begin{proof}
  Let $\nabla^2 y = -\lambda y$. Then
  \[
    - \lambda \int_\Omega |y|^2 \d^n x = \int_\Omega y^* \nabla^2 y\;\d^n x = \int_{\partial\Omega} y^* \mathbf{n}\cdot \nabla y \;\d^{n - 1}x - \int_\Omega |\nabla y|^2 \;\d^n x.
  \]
  If the boundary term vanishes, then $\lambda = \frac{\int |\nabla y|^2\;\d^n x}{\int_\Omega |y|^2 \;\d^n x} \geq 0$.
\end{proof}

\begin{thm}[Solution by eigenfunction expansion]
  \label{thm:heat-eigenfunction}
  Let $\{y_I\}$ be eigenfunctions of $-\nabla^2$ with eigenvalues $\lambda_I \geq 0$. If $\phi(\mathbf{x}, 0) = \sum_I c_I y_I(\mathbf{x})$, then the solution to the heat equation (with $\kappa = 1$) is
  \[
    \phi(\mathbf{x}, t) = \sum_I c_I e^{-\lambda_I t} y_I(\mathbf{x}).
  \]
\end{thm}

\begin{remark}
  Since all $\lambda_I > 0$, the coefficients $c_I(t) = c_I e^{-\lambda_I t}$ decay exponentially. Eigenfunctions with larger eigenvalues (corresponding to more oscillatory, ``spiky'' modes) decay fastest, explaining why heat flow smooths out solutions.
\end{remark}

\subsubsection{Brownian motion and the heat equation}
The heat equation is not time-reversible, yet arises from microscopic dynamics that are. Einstein showed how this emerges from Brownian motion.

\begin{eg}[Derivation from random walk]
  \label{eg:brownian-motion}
  Let $p(y, \Delta t)$ be the probability of a particle moving by $y$ in time $\Delta t$, with:
  \begin{itemize}
    \item Normalization: $\int_{-\infty}^\infty p(y, \Delta t)\;\d y = 1$
    \item Symmetry: $p(y, \Delta t) = p(-y, \Delta t)$
    \item Localization: $p$ is strongly peaked around $y = 0$
  \end{itemize}
  Let $P(x, t)$ be the probability density at position $x$, time $t$. Then
  \[
    P(x, t + \Delta t) = \int_{-\infty}^\infty P(x - y, t) p(y, \Delta t)\;\d y.
  \]
  Taylor expanding $P(x - y, t)$ and using $\langle y \rangle = 0$ (by symmetry):
  \[
    P(x, t + \Delta t) - P(x, t) \approx \frac{1}{2}\langle y^2\rangle \frac{\partial^2 P}{\partial x^2},
  \]
  where $\langle y^r\rangle = \int y^r p(y, \Delta t)\,dy$. If $\langle y^2\rangle/(2\Delta t) \to \kappa$ as $\Delta t \to 0$, we obtain
  \[
    \frac{\partial P}{\partial t} = \kappa \frac{\partial^2 P}{\partial x^2}.
  \]
\end{eg}
\begin{thm}[Uniqueness for the heat equation]
  \label{thm:heat-uniqueness}
  Suppose $\phi: \Omega\times [0, \infty) \to \R$ satisfies the heat equation $\frac{\partial \phi}{\partial t} = \kappa \nabla^2 \phi$, with:
  \begin{itemize}
    \item Initial condition: $\phi(\mathbf{x}, 0) = f(\mathbf{x})$ for all $\mathbf{x} \in \Omega$
    \item Boundary condition: $\phi(\mathbf{x}, t)|_{\partial \Omega} = g(\mathbf{x}, t)$ for all $t \in [0, \infty)$.
  \end{itemize}
  Then $\phi$ is unique.
\end{thm}

\begin{proof}
  Suppose $\phi_1$ and $\phi_2$ are both solutions. Then define $\Phi = \phi_1 - \phi_2$ and
  \[
    E(t) = \frac{1}{2}\int_\Omega \Phi^2 \;\d V.
  \]
  Then we know that $E(t) \geq 0$. Since $\phi_1, \phi_2$ both obey the heat equation, so does $\Phi$. Also, on the boundary and at $t = 0$, we know that $\Phi = 0$. We have
  \begin{align*}
    \frac{\d E}{\d t} &= \int_\Omega \Phi \frac{\d \Phi}{\d t}\;\d V\\
    &= \kappa \int_\Omega \Phi \nabla^2 \Phi\;\d V\\
    &= \kappa \int_{\partial \Omega} \Phi \nabla \Phi \cdot \d \mathbf{S} - \kappa \int_\Omega (\nabla \Phi)^2\;\d V\\
    &= - \kappa \int_\Omega (\nabla \Phi)^2\;\d V\\
    &\leq 0.
  \end{align*}
  So we know that $E$ decreases with time but is always non-negative. We also know that at time $t = 0$, $E = \Phi = 0$. So $E = 0$ always. So $\Phi = 0$ always. So $\phi_1 = \phi_2$.
\end{proof}

\begin{eg}[Heat conduction in soil]
  \label{eg:heat-soil}
  Consider temperature $\phi(x, t)$ in soil at depth $x \geq 0$, where the surface temperature varies with daily and seasonal cycles.

  We let $\phi(x, t)$ be the temperature of the soil as a function of the depth $x$, defined on $\R^{\geq 0} \times [0, \infty)$. Then it obeys the heat equation with conditions
  \begin{enumerate}
    \item $\phi(0, t) = \phi_0 + A \cos \left(\frac{2\pi t}{t_D}\right) + B\cos \left(\frac{2\pi t}{t_Y}\right)$.
    \item $\phi(x, t) \to $ const as $x \to \infty$.
  \end{enumerate}
  We know that
  \[
    \frac{\partial \phi}{\partial t} = K \frac{\partial^2 \phi}{\partial x^2}.
  \]
  We try the separation of variables
  \[
    \phi(x, t) = T(t) X(x).
  \]
  Then we get the equations
  \[
    T' = \lambda T,\quad X'' = \frac{\lambda}{K}X.
  \]
  From the boundary solution, we know that our things will be oscillatory. So we let $\lambda$ be imaginary, and set $\lambda = i \omega$. So we have
  \[
    \phi(x, t) = e^{i\omega t}\left(a_\omega e^{-\sqrt{i\omega/K}x} + b_\omega e^{\sqrt{i\omega/K}x}\right).
  \]
  Note that we have
  \[
    \sqrt{\frac{i\omega}{K}} =
    \begin{cases}
      (1 + i) \sqrt{\frac{|\omega|}{2K}} & \omega > 0\\
      (i - 1) \sqrt{\frac{|\omega|}{2K}} & \omega < 0
    \end{cases}
  \]
  Since $\phi(x, t) \to $ constant as $x \to \infty$, we don't want our $\phi$ to blow up. So if $\omega < 0$, we need $a_\omega = 0$. Otherwise, we need $b_\omega = 0$.

  To match up at $x = 0$, we just want terms with
  \[
    |\omega| = \omega_D = \frac{2\pi}{t_D},\quad |\omega| = \omega_Y = \frac{2\pi}{t_Y}.
  \]
  So we can write out solution as
  \begin{align*}
    \phi(x, t) = \phi_0 &+ A \exp\left(-\sqrt{\frac{\omega_D}{2K}}\right)\cos\left(\omega_Dt - \sqrt{\frac{\omega_D}{2K}}x\right) \\
    &+ B \exp\left(-\sqrt{\frac{\omega_Y}{2K}}\right)\cos\left(\omega_Yt - \sqrt{\frac{\omega_Y}{2K}}x\right)
  \end{align*}
  We can notice a few things. Firstly, as we go further down, the effect of the sun decays, and the temperature is more stable. Also, the effect of the day-night cycle decays more quickly than the annual cycle, which makes sense. We also see that while the temperature does fluctuate with the same frequency as the day-night/annual cycle, as we go down, there is a phase shift. This is helpful since we can store things underground and make them cool in summer, warm in winter.
\end{eg}

\begin{eg}[Heat conduction in a finite rod]
  \label{eg:heat-rod}
  Consider a rod of length $2L$ from $x = -L$ to $x = L$.
  \begin{center}
    \begin{tikzpicture}
      \draw [thick] (-2, 0) -- (2, 0);
      \node [circ] at (0, 0) {};
      \node [below] at (0, 0) {$x = 0$};

      \node [circ] at (-2, 0) {};
      \node [below] at (-2, 0) {$x = -L$};

      \node [circ] at (2, 0) {};
      \node [below] at (2, 0) {$x = L$};
    \end{tikzpicture}
  \end{center}
  We have the initial conditions
  \[
    \phi(x, 0) = \Theta(x) =
    \begin{cases}
      1 & 0 < x < L\\
      0 & -L < x < 0
    \end{cases}
  \]
  and the boundary conditions
  \[
    \phi(-L, t) = 0,\quad \phi(L, t) = 1
  \]
  So we start with a step temperature, and then maintain the two ends at fixed temperatures $0$ and $1$.

  We are going to do separation of variables, but we note that all our boundary and initial conditions are inhomogeneous. This is not helpful. So we use a little trick. We first look for \emph{any} solution satisfying the boundary conditions $\phi_S(-L, t) = 0$, $\phi_S(L, t) = 1$. For example, we can look for time-independent solutions $\phi_S(x, t) = \phi_S(x)$. Then we need $\frac{\d^2 \phi_S}{\d x^2} = 0$. So we get
  \[
    \phi_S(x) = \frac{x + L}{2L}.
  \]
  By linearity, $\psi(x, t) = \phi(x, t) - \phi_s(x)$ obeys the heat equation with the conditions
  \[
    \psi(-L, t) = \psi(L, t) = 0,
  \]
  which is homogeneous! Our initial condition now becomes
  \[
    \psi(x, 0) = \Theta(x) - \frac{x + L}{2L}.
  \]
  We now perform separation of variables for
  \[
    \psi(x, t) = X(x) T(t).
  \]
  Then we obtain the equations
  \[
    T' = -\kappa \lambda T,\quad X' = - \lambda X.
  \]
  Then we have
  \[
    \psi(x, t) = \left[a\sin (\sqrt{\lambda} x) + b\cos (\sqrt{\lambda}x)\right] e^{-\kappa \lambda t}.
  \]
  Since initial condition is odd, we can eliminate all $\cos$ terms. Our boundary conditions also requires
  \[
    \lambda = \frac{n^2 \pi^2}{L^2}, \quad n = 1, 2, \cdots.
  \]
  So we have
  \[
    \phi(x, t) = \phi_s(x) + \sum_{n = 1}^\infty a_n \sin\left(\frac{n\pi x}{L}\right) \exp\left(-\frac{\kappa n^2 \pi^2}{L^2}t\right),
  \]
  where $a_n$ are the Fourier coefficients
  \[
    a_n = \frac{1}{L}\int_{-L}^L \left[\Theta(x) - \frac{x + L}{2L}\right] \sin\left(\frac{n\pi x}{L}\right)\;\d x = \frac{1}{n\pi}.
  \]
\end{eg}

\begin{eg}[Cooling of a uniform sphere]
  Once upon a time, Charles Darwin went around the Earth, looked at species, and decided that evolution happened. When he came up with his theory, it worked quite well, except that there was one worry. Was there enough time on Earth for evolution to occur?

  This calculation was done by Kelvin. He knew well that the Earth started as a ball of molten rock, and obviously life couldn't have evolved when the world was still molten rock. So he would want to know how long it took for the Earth to cool down to its current temperature, and if that was sufficient for evolution to occur.

  We can, unsurprisingly, use the heat equation. We assume that at time $t = 0$, the temperature is $\phi_0$, the melting temperature of rock. We also assume that the space is cold, and we let the temperature on the boundary of Earth as $0$. We then solve the heat equation on a sphere (or ball), and see how much time it takes for Earth to cool down to its present temperature.

  We let $\Omega = \{(x, y, z) \in \R^3, r \leq R\}$, and we want a solution $\phi(r, t)$ of the heat equation that is spherically symmetric and obeys the conditions
  \begin{itemize}
    \item $\phi(R, t) = 0$
    \item $\phi(r, 0) = \phi_0$,
  \end{itemize}
  We can write the heat equation as
  \[
    \frac{\partial \phi}{\partial t} = \kappa \nabla^2 \phi = \frac{\kappa}{r^2}\frac{\partial}{\partial r}\left(r^2 \frac{\partial \phi}{\partial r}\right).
  \]
  Again, we do the separation of variables.
  \[
    \phi(r, t) = R(r) T(t).
  \]
  So we get
  \[
    T' = -\lambda^2 \kappa T,\quad \frac{\d}{\d r}\left(r^2 \frac{\d R}{\d r}\right) = -\lambda^2 r^2 R.
  \]
  We can simplify this a bit by letting $R(r) = \frac{S(r)}{r}$, then our radial equation just becomes
  \[
    S'' = -\lambda^2 S.
  \]
  We can solve this to get
  \[
    R(r) = A_\lambda \frac{\sin \lambda r}{r} + B_\lambda \frac{\cos \lambda r}{r}.
  \]
  We want a regular solution at $r = 0$. So we need $B_\lambda = 0$. Also, the boundary condition $\phi(R, t) = 0$ gives
  \[
    \lambda = \frac{n\pi}{R},\quad n = 1, 2, \cdots
  \]
  So we get
  \[
    \phi(r, t) = \sum_{n \in \Z} \frac{A_n}{r} \sin\left(\frac{n\pi r}{R}\right) \exp\left(\frac{-\kappa n^2 \pi^2 t}{R^2}\right),
  \]
  where our coefficients are
  \[
    A_n = (-1)^{n + 1}\frac{\phi_0 R}{n\pi}.
  \]
  We know that the Earth isn't just a cold piece of rock. There are still volcanoes. So we know many terms still contribute to the sum nowadays. This is rather difficult to work with. So we instead look at the temperature gradient
  \[
    \frac{\partial \phi}{\partial r} = \frac{\phi_0}{r} \sum_{n \in \Z} (-1)^{n + 1} \cos \left(\frac{n\pi r}{R}\right) \exp\left(\frac{-\kappa n^2 \pi^2 t}{R^2}\right) + \sin\text{stuff}.
  \]
  We evaluate this at the surface of the Earth, $R = r$. So we get the gradient
  \[
    \left.\frac{\partial\phi}{\partial r}\right|_R = -\frac{\phi_0}{R} \sum_{n\in \Z} \exp\left(-\frac{\kappa n^2 \pi^2 t}{R^2}\right) \approx \frac{\phi_0}{R} \int_{-\infty}^\infty \exp\left(-\frac{\kappa \pi y^2 t}{R^2}\right)\;\d y = \phi_0 \sqrt{\frac{1}{\pi \kappa t}}.
  \]
  So the age of the Earth is approximately
  \[
    t \approx \left(\frac{\phi_0}{V}\right)^2 \frac{1}{4\pi K},
  \]
  where
  \[
    V = \left.\frac{\partial \phi}{\partial r}\right|_R.
  \]
  We can plug the numbers in, and get that the Earth is 100 million years. This is not enough time for evolution.

  Later on, people discovered that fission of metals inside the core of Earth produce heat, and provides an alternative source of heat. So problem solved! The current estimate of the age of the universe is around 4 billion years, and evolution did have enough time to occur.
\end{eg}

\subsection{The wave equation}
We derive the wave equation from physical principles and study its solutions.

\subsubsection{Derivation from string dynamics}
Consider a string $x\in [0, L]$ undergoing small transverse oscillations described by displacement $\phi(x, t)$.
\begin{center}
  \begin{tikzpicture}[xscale=0.75]
    \draw (0, 0) sin (2, 1) cos (4, 0) sin (6, -1) cos (8, 0) sin (10, 1) cos (12, 0);
    \draw [dashed] (0, 0) -- (12, 0);

    \draw [->] (2, 0) -- (2, 1) node [pos=0.5, right] {$\phi(x, t)$};
    \node [circ] at (8.666, 0.5) {};
    \node [left] at (8.666, 0.5) {$A$};
    \node [circ] at (9.333, 0.866) {};
    \node [above] at (9.333, 0.866) {$B$};
    \draw [dashed] (8.666, 0.5) -- +(0.7, 0);
    \draw (8.666, 0.5) +(0.3, 0) arc(0:30:0.3);
    \node [right] at (8.85, 0.6) {\tiny$\theta\!_A$};

    \draw [dashed] (9.333, 0.866) -- +(0.7, 0);
    \draw (9.333, 0.866) +(0.3, 0) arc (0:15:0.3);
    \node [right] at (9.5, 0.7) {\tiny$\theta\!_B$};
  \end{tikzpicture}
\end{center} % improve
Consider two points $A$, $B$ separated by a small distance $\delta x$. Let $T_A$ ($T_B$) be the outward pointing tension tangent to the string at $A$ ($B$). Since there is no sideways ($x$) motion, there is no net horizontal force. So
\[
  T_A\cos \theta_A = T_B \cos \theta_B = T.\tag{$*$}
\]
If the string has mass per unit length $\mu$, then in the vertical direction, Newton's second law gives
\[
  \mu \delta x \frac{\partial^2 \phi}{\partial t^2} = T_B \sin \theta_B - T_A \sin \theta_A.
\]
We now divide everything by $T$, noting the relation $(*)$, and get
\begin{align*}
  \mu \frac{\delta x}{T}\frac{\partial^2 \phi}{\partial t^2} &= \frac{T_B \sin \theta_B}{T_B\cos \theta_B} - \frac{T_A \sin \theta_A}{T_A \cos \theta_A}\\
  &= \tan \theta_B - \tan \theta_A\\
  &= \left.\frac{\partial\phi}{\partial x}\right|_B - \left.\frac{\partial\phi}{\partial x}\right|_A\\
  &\approx \delta x\frac{\partial^2 \phi}{\partial x^2}.
\end{align*}
Taking the limit $\delta x \to 0$ and setting $c^2 = T/\mu$, we obtain:

\begin{defi}[Wave equation]
  \label{def:wave-equation}
  The \emph{wave equation} in one spatial dimension is
  \[
    \frac{\partial^2 \phi}{\partial t^2} = c^2 \frac{\partial^2 \phi}{\partial x^2},
  \]
  where $c > 0$ is the \emph{wave speed}. For a string with tension $T$ and mass per unit length $\mu$, we have $c = \sqrt{T/\mu}$.
\end{defi}

\begin{remark}
  The general solution in one dimension is $\phi(x,t) = f(x - ct) + g(x + ct)$, representing left- and right-traveling waves. However, separation of variables generalizes to higher dimensions.
\end{remark}

\begin{eg}[String with fixed ends]
  \label{eg:wave-string}
  For a string fixed at both ends ($\phi(0, t) = \phi(L, t) = 0$), separation of variables gives
  \[
    \phi(x, t) = \sum_{n = 1}^\infty \sin\left(\frac{n \pi x}{L}\right)\left[A_n \cos\left(\frac{n \pi c t}{L}\right) + B_n \sin\left(\frac{n\pi ct}{L}\right)\right].
  \]
  The coefficients $A_n$ are determined by the initial displacement $\phi(x, 0)$, and $B_n$ by the initial velocity $\frac{\partial \phi}{\partial t}(x, 0)$. Since the wave equation is second-order in time, \emph{two} initial conditions are required.
\end{eg}

\subsubsection{Energetics and uniqueness}

\begin{defi}[Energy of a vibrating string]
  \label{def:string-energy}
  For a vibrating string with displacement $\phi(x,t)$, mass density $\mu$, and wave speed $c$:
  \begin{itemize}
    \item \emph{Kinetic energy}: $K(t) = \frac{\mu}{2}\int_0^L \left(\frac{\partial \phi}{\partial t}\right)^2 \;\d x$
    \item \emph{Potential energy}: $V(t) = \frac{\mu c^2}{2}\int_0^L \left(\frac{\partial \phi}{\partial x}\right)^2 \;\d x$
    \item \emph{Total energy}: $E(t) = K(t) + V(t)$
  \end{itemize}
\end{defi}

The potential energy arises from the work done against tension due to string extension:
\begin{align*}
  T\times \text{extension} &= T(\delta s - \delta x) = T(\sqrt{\delta x^2 + \delta \phi^2} - \delta x)\\
  &\approx \frac{T}{2}\delta x \left(\frac{\partial \phi}{\partial x}\right)^2 = \frac{\mu c^2}{2}\delta x \left(\frac{\partial \phi}{\partial x}\right)^2.
\end{align*}

Using our series expansion for $\phi$, we get
\begin{align*}
  K(t) &= \frac{\mu \pi^2 c^2}{4L}\sum_{n = 1}^\infty n^2 \left[A_n \sin \left(\frac{n\pi c t}{L}\right) - B_n \cos \left(\frac{n\pi ct}{L}\right)\right]^2\\
  V(t) &= \frac{\mu\pi^2 c^2}{4L} \sum_{n = 1}^\infty n^2 \left[A_n \cos\left(\frac{n\pi ct}{L}\right) + B_n \sin\left(\frac{n\pi ct}{L}\right)\right]^2
\end{align*}
The \emph{total energy} is
\[
  E(t) = \frac{n\pi^2 c^2}{4L}\sum_{n = 1}^\infty n^2 (A_n^2 + B_n^2).
\]
What can we see here? Our solution is essentially an (infinite) sum of independent harmonic oscillators, one for each $n$. The period of the fundamental mode ($n = 1$) is $\frac{2\pi}{\omega} = 2\pi \cdot \frac{L}{\pi c} = \frac{2L}{c}$. Thus, averaging over a period, the average kinetic energy is
\[
  \bar K = \frac{c}{2L} \int_0^{2L/c} K(t)\;\d t = \bar V = \frac{c}{2L}\int_0^{2L/c} V(t)\;\d t = \frac{E}{2}.
\]
Hence we have an equipartition of the energy between the kinetic and potential energy.

The energy also allows us to prove a uniqueness theorem. First we show that energy is conserved in general.

\begin{thm}[Energy conservation for wave equation]
  \label{thm:wave-energy}
  If $\phi$ satisfies the wave equation $\frac{\partial^2 \phi}{\partial t^2} = c^2 \nabla^2 \phi$ on $\Omega \times (0, \infty)$ with $\phi|_{\partial\Omega}$ fixed, then the total energy $E = K + V$ is constant.
\end{thm}

\begin{proof}
  By definition, we have
  \[
    \frac{\d E}{\d t} = \int_\Omega \frac{\partial^2 \psi}{\partial t^2}\frac{\partial \psi}{\partial t} + c^2 \nabla \left(\frac{\partial \phi}{\partial t}\right)\cdot \nabla \phi \;\d V.
  \]
  We integrate by parts in the second term to obtain
  \[
    \frac{\d E}{\d t} = \int_\Omega \frac{\d \phi}{\d t}\left(\frac{\partial^2 \phi}{\partial t^2} - c^2 \nabla^2 \phi\right)\;\d V + c^2 \int_{\partial \Omega} \frac{\partial \phi}{\partial t}\nabla \phi \cdot \d S.
  \]
  Since $\frac{\partial^2 \phi}{\partial t^2} = c^2 \nabla^2 \phi$ by the wave equation, and $\phi$ is constant on $\partial \Omega$, we know that
  \[
    \frac{\d E_\phi}{\d t} = 0.\qedhere
  \]
\end{proof}

\begin{thm}[Uniqueness for wave equation]
  \label{thm:wave-uniqueness}
  Suppose $\phi: \Omega \times [0, \infty) \to \R$ satisfies the wave equation $\frac{\partial^2 \phi}{\partial t^2} = c^2 \nabla^2 \phi$ with:
  \begin{enumerate}
    \item Initial displacement: $\phi(\mathbf{x}, 0) = f(\mathbf{x})$
    \item Initial velocity: $\frac{\partial \phi}{\partial t}(\mathbf{x}, 0) = g(\mathbf{x})$
    \item Boundary condition: $\phi|_{\partial \Omega\times [0, \infty)} = h(\mathbf{x}, t)$
  \end{enumerate}
  Then $\phi$ is unique.
\end{thm}
\begin{proof}
  Suppose $\phi_1$ and $\phi_2$ are two such solutions. Then $\psi = \phi_1 - \phi_2$ obeys the wave equation
  \[
    \frac{\partial^2 \psi}{\partial t^2} = c^2 \nabla^2 \psi,
  \]
  and
  \[
    \psi|_{\partial \Omega \times [0, \infty)} = \psi|_{\Omega \times \{0\}} = \left.\frac{\partial \psi}{\partial t}\right|_{\Omega \times \{0\}} = 0.
  \]
  We let
  \[
    E_\psi(t) = \frac{1}{2}\int_\Omega \left[\left(\frac{\partial \psi}{\partial t}\right)^2 + c^2 \nabla \psi \cdot \nabla \psi\right] \;\d V.
  \]
  Then since $\psi$ obeys the wave equation with fixed boundary conditions, we know $E_\psi$ is constant.

  Initially, at $t = 0$, we know that $\psi = \frac{\partial \psi}{\partial t} = 0$. So $E_\psi(0) = 0$. At time $t$, we have
  \[
    E_\psi = \frac{1}{2}\int_\Omega \left(\frac{\partial \psi}{\partial t}\right)^2 + c^2 (\nabla \psi)\cdot (\nabla \psi) \;\d V = 0.
  \]
  Hence we must have $\frac{\partial \psi}{\partial t} = 0$. So $\psi$ is constant. Since it is $0$ at the beginning, it is always $0$.
\end{proof}

\begin{eg}[Circular drum]
  \label{eg:circular-drum}
  Consider a circular membrane $\Omega = \{(x, y) \in \R^2 : x^2 + y^2 \leq 1\}$. Let $\phi(r, \theta, t)$ solve
  \[
    \frac{1}{c^2}\frac{\partial^2 \phi}{\partial t^2} = \nabla^2 \phi = \frac{1}{r} \left(r \frac{\partial \phi}{\partial r}\right) + \frac{1}{r^2}\frac{\partial^2\phi}{\partial \theta^2},
  \]
  with the boundary condition $\phi|_{\partial \Omega} = 0$. We can imagine this as a drum, where the membrane can freely oscillate with the boundary fixed.

  Separating variables with $\phi(r, \theta, t) = T(t) R(r) \Theta(\theta)$, we get
  \[
    T'' = - c^2 \lambda T,\quad \Theta'' = -\mu \Theta,\quad r(R')' + (r^2\lambda - \mu)R = 0.
  \]
  Then as before, $T$ and $\Theta$ are both sine and cosine waves. Since we are in polars coordinates, we need $\phi(t, r, \theta + 2\pi) = \phi(t, r, \theta)$. So we must have $\mu = m^2$ for some $m \in \N$. Then the radial equation becomes
  \[
    r(rR')' + (r^2 \lambda - m^2) R = 0,
  \]
  which is Bessel's equation of order $m$. So we have
  \[
    R(r) = a_m J_m(\sqrt{\lambda} r) + b_m Y_m(\sqrt{\lambda} r).
  \]
  Since we want regularity at $r = 0$, we need $b_m = 0$ for all $m$. To satisfy the boundary condition $\phi|_{\partial \Omega} = 0$, we must choose $\sqrt{\lambda} = k_{mi}$, where $k_{mi}$is the $i$th root of $J_m$.

  Hence the general solution is
  \begin{align*}
    \phi(t, r, \theta) &= \sum_{i = 0}^\infty [A_{0i}\sin (k_{0i} ct) + B_{0i}\cos(k_{0i}ct)] J_0 (k_{0i}r)\\\
    &\quad\quad +\sum_{m = 1}^\infty\sum_{i = 0}^\infty [A_{mi}\cos (m \theta) + B_{mi}\sin (m\theta)]\sin k_{mi}ct J_m (k_{mi} r)\\
    &\quad\quad +\sum_{m = 1}^\infty\sum_{i = 0}^\infty [C_{mi}\cos (m \theta) + D_{mi}\sin (m\theta)]\cos k_{mi}ct J_m (k_{mi} r)
  \end{align*}
  For example, suppose we have the initial conditions $\phi(0, r, \theta) = 0$, $\partial_t \phi(0, r, \theta) = g(r)$. So we start with a flat surface and suddenly hit it with some force. By symmetry, we must have $A_{mi}, B_{mi}, C_{mi}, D_{mi} = 0$ for $m \not= 0$. If this does not seem obvious, we can perform some integrals with the orthogonality relations to prove this.

  At $t = 0$, we need $\phi = 0$. So we must have $B_{0i} = 0$. So we are left with
  \[
    \phi = \sum_{i = 0}^\infty A_{0i} \sin (k_{0i} ct) J_0(k_{0j}r).
  \]
  We can differentiate this, multiply with $J_0(k_{0j} r) r$ to obtain
  \[
    \int_0^1 \sum_{i = 0}^\infty k_{0i}c A_{0i}J_0(k_{0i}r) J_0(k_{0j} r)r\;\d r = \int_0^1 g(r) J_0(k_{0j}r)r \;\d r.
  \]
  Using the orthogonality relations for $J_0$ from the example sheet, we get
  \[
    A_{0i} = \frac{2}{ck_{0i}} \frac{1}{[J_0'(k_{0i})]^2} \int_0^1 g(r)J_0(k_{0j} r) r\;\d r.
  \]
  Note that the frequencies come from the roots of the Bessel's function, and are not evenly spaced. This is different from, say, string instruments, where the frequencies are evenly spaced. So drums sound differently from strings.
\end{eg}

So far, we have used separation of variables to solve our differential equations. This is quite good, as it worked in our examples, but there are a few issues with it. Of course, we have the problem of whether it converges or not, but there is a more fundamental problem.

To perform separation of variables, we need to pick a good coordinate system, such that the boundary conditions come in a nice form. We were able to perform separation of variables because we can find some coordinates that fit our boundary conditions well. However, in real life, most things are not nice. Our domain might have a weird shape, and we cannot easily find good coordinates for it.

Hence, instead of solving things directly, we want to ask some more general questions about them. In particular, Kac asked the question ``can you hear the shape of a drum?'' --- suppose we know all the frequencies of the modes of oscillation on some domain $\Omega$. Can we know what $\Omega$ is like?

The answer is no, and we can explicitly construct two distinct drums that sound the same. Fortunately, we get an affirmative answer if we restrict ourselves a bit. If we require $\Omega$ to be convex, and has a real analytic boundary, then yes! In fact, we have the following result: let $N(\lambda_0)$ be the number of eigenvalues less than $\lambda_0$. Then we can show that
\[
  4\pi^2 \lim_{\lambda_0 \to \infty} \frac{N(\lambda_0)}{\lambda_0} = \mathrm{Area}(\Omega).
\]

\section{Distributions}
\subsection{Distributions}
When using separation of variables, we encounter convergence questions: does the infinite sum of separated solutions converge, and does it satisfy the differential equation? Since infinite sums of continuous functions can be discontinuous (as seen in Fourier series), we need a broader framework. This leads to \emph{distributions}, or generalized functions.

\begin{defi}[Test functions]
  \label{def:test-functions}
  The space of \emph{test functions} on $\Omega \subseteq \R^n$, denoted $D(\Omega)$ or $C^\infty_{\mathrm{cpt}}(\Omega)$, consists of all infinitely differentiable functions with compact support in $\Omega$.
\end{defi}

\begin{eg}[Bump function]
  \label{eg:bump-function}
  A standard example is the bump function
  \[
    \phi(x) =
    \begin{cases}
      e^{-1/(1- x^2)} & |x| < 1\\
      0 & \text{otherwise}.
    \end{cases}
  \]
  \begin{center}
    \begin{tikzpicture}[scale=1.5]
      \draw [->] (-2, 0) -- (2, 0) node [right] {$x$};
      \draw [->] (0, -0.5) -- (0, 1);

      \draw [semithick, domain=-0.999:0.999, mblue] plot (\x, {exp (-1 / (1 - \x * \x))});
      \draw [semithick, mblue] (-1.5, 0) -- (-1, 0);
      \draw [semithick, mblue] (1, 0) -- (1.5, 0);
    \end{tikzpicture}
  \end{center}
\end{eg}

\begin{defi}[Distribution]
  \label{def:distribution}
  A \emph{distribution} on $\Omega$ is a continuous linear functional $T: D(\Omega) \to \R$. For $\phi \in D(\Omega)$, we write the value of $T$ at $\phi$ as $T[\phi]$.
\end{defi}

\begin{eg}[Regular distribution]
  \label{eg:regular-distribution}
  Any locally integrable function $f$ defines a distribution $T_f$ by
  \[
    T_f[\phi] = \int_\Omega f(x) \phi(x) \;\d x.
  \]
  Linearity follows from linearity of integration. Thus every function can be viewed as a distribution.
\end{eg}

The power of distributions lies in allowing ``generalized functions'' that are not ordinary functions.

\begin{defi}[Dirac delta distribution]
  \label{def:dirac-delta}
  The \emph{Dirac delta} is the distribution $\delta: D(\Omega) \to \R$ defined by
  \[
    \delta[\phi] = \phi(0).
  \]
  More generally, for $a \in \Omega$, the \emph{shifted Dirac delta} is $\delta_a[\phi] = \phi(a)$.
\end{defi}

\begin{remark}
  By analogy with regular distributions, we write symbolically
  \[
    \delta[\phi] = \int_\Omega \delta(x) \phi(x) \;\d x,
  \]
  treating $\delta(x)$ as if it were a function. However, $\delta$ is \emph{not} a function: if $\delta(x) = 0$ for $x \neq 0$ (as required since $\delta[\phi]$ depends only on $\phi(0)$), then the integral would vanish for any finite $\delta(0)$. Informally, one thinks of $\delta$ as ``zero everywhere except at the origin, where it is infinite.''
\end{remark}

\begin{defi}[Derivative of a distribution]
  \label{def:distributional-derivative}
  The \emph{derivative} of a distribution $T$ is the distribution $T'$ defined by
  \[
    T'[\phi] = -T[\phi'].
  \]
  More generally, the $k$-th derivative is $T^{(k)}[\phi] = (-1)^k T[\phi^{(k)}]$.
\end{defi}

\begin{remark}
  This definition is motivated by integration by parts: for a regular distribution $T_f$,
  \[
    \int_\Omega f'(x) \phi(x)\;\d x = -\int_\Omega f(x)\phi'(x) \;\d x,
  \]
  with no boundary terms since $\phi$ has compact support. Since test functions are infinitely differentiable, \emph{every distribution has derivatives of all orders}.
\end{remark}

\begin{eg}[Delta as a limit of smooth functions]
  \label{eg:delta-limit}
  The Gaussians
  \[
    G_n(x) = \frac{n}{\sqrt{\pi}} \exp(-n^2 x^2)
  \]
  are smooth and satisfy $\int G_n = 1$. As $n \to \infty$, they become increasingly peaked at the origin:
  \begin{center}
    \begin{tikzpicture}
      \draw [->] (-3, 0) -- (3.25, 0) node [right] {$x$};
      \draw [->, use as bounding box] (0, 0) -- (0, 4) node [above] {$G_n$};

      \draw [semithick, mblue, domain=-3:3, samples = 100] plot (\x, { 1.6 * exp( - \x * \x)});
      \draw [semithick, morange, domain=-3:3, samples = 100] plot (\x, { 3.2 * exp( - 4 * \x * \x)});
    \end{tikzpicture}
  \end{center}
  As distributions, $G_n \to \delta$: for any test function $\phi$, we have $\int G_n(x)\phi(x)\,dx \to \phi(0)$.
\end{eg}

\begin{eg}[Derivative of delta]
  \label{eg:delta-derivative}
  By definition,
  \[
    \delta'[\phi] = -\delta[\phi'] = -\phi'(0).
  \]
  This can also be viewed as $\lim_{n \to \infty} G'_n$ in the distributional sense.
\end{eg}

\begin{prop}[Properties of the Dirac delta]
  \label{prop:delta-properties}
  The Dirac delta satisfies:
  \begin{enumerate}
    \item \emph{Translation}: $\displaystyle\int_{-\infty}^\infty \delta(x - a) \phi(x)\;\d x = \phi(a)$.
    \item \emph{Scaling}: For $c \neq 0$, $\displaystyle\int_{-\infty}^\infty \delta(cx) \phi(x)\;\d x = \frac{1}{|c|}\phi(0)$, i.e., $\delta(cx) = \frac{1}{|c|}\delta(x)$.
    \item \emph{Composition}: If $f$ is continuously differentiable with simple zeros at $\{x_i\}$, then
      \[
        \delta(f(x)) = \sum_i \frac{\delta(x - x_i)}{|f'(x_i)|}.
      \]
  \end{enumerate}
\end{prop}
\begin{proof}
  For (1), substitute $y = x - a$. For (2), substitute $y = cx$ to get
  \[
    \int \delta(cx)\phi(x)\,dx = \int \delta(y)\phi(y/c)\frac{dy}{|c|} = \frac{\phi(0)}{|c|}.
  \]
  For (3), near each zero $x_i$, we have $f(x) \approx (x - x_i)f'(x_i)$, so by (2),
  \[
    \int \delta(f(x))\phi(x)\,dx = \sum_i \int \delta((x-x_i)f'(x_i))\phi(x)\,dx = \sum_i \frac{\phi(x_i)}{|f'(x_i)|}. \qedhere
  \]
\end{proof}
\subsubsection{Eigenfunction expansions of delta}

\begin{prop}[Fourier expansion of delta]
  \label{prop:delta-fourier}
  On the interval $[-L, L]$,
  \[
    \delta(x) = \frac{1}{2L} \sum_{n \in \Z} e^{in\pi x/L}
  \]
  in the sense of distributions.
\end{prop}
\begin{proof}
  Computing the Fourier coefficients:
  \[
    \hat{\delta}_n = \frac{1}{2L} \int_{-L}^L e^{-in\pi x/L}\delta(x)\;\d x = \frac{1}{2L}.
  \]
  To verify this makes sense as a distribution, let $S_N\delta(x) = \frac{1}{2L} \sum_{n = -N}^N e^{in \pi x/L}$. Then
  \begin{align*}
    \lim_{N \to \infty} \int_{-L}^L S_N \delta(x) \phi(x)\;\d x &= \lim_{N \to \infty} \sum_{n = -N}^N \frac{1}{2L}\int_{-L}^L e^{in\pi x/L}\phi(x) \;\d x\\
    &= \lim_{N\to \infty} \sum_{n = -N}^N \hat{\phi}_{-n} = \phi(0),
  \end{align*}
  since the Fourier series of the smooth function $\phi$ converges pointwise.
\end{proof}

\begin{prop}[General eigenfunction expansion of delta]
  \label{prop:delta-eigenfunction}
  Let $\{y_n(x)\}$ be a complete orthonormal set of eigenfunctions on $[a, b]$ with respect to weight $w(x)$, i.e.,
  \[
    \int_a^b y_m^*(x) y_n(x) w(x)\,dx = \delta_{mn}.
  \]
  Then
  \[
    \delta(x - \xi) = w(\xi) \sum_n y_n^*(\xi) y_n(x) = w(x) \sum_n y_n^*(\xi) y_n(x).
  \]
\end{prop}
\begin{proof}
  Expanding $\delta(x - \xi) = \sum_n c_n y_n(x)$, the coefficients are
  \[
    c_n = \int_a^b y_n^*(x) \delta(x - \xi) w(x)\;\d x = y_n^*(\xi) w(\xi).
  \]
  The two forms are equivalent since $\delta(x - \xi) = \frac{w(\xi)}{w(x)} \delta(x - \xi)$ (the factor only matters at $x = \xi$).
\end{proof}
\subsection{Green's functions}
One of the main applications of the Dirac delta is in solving differential equations via Green's functions.

\begin{defi}[Green's function]
  \label{def:greens-function}
  Consider a linear differential operator $\mathcal{L}$ on $[a,b]$, such as
  \[
    \mathcal{L} = \alpha(x) \frac{\d^2}{\d x^2} + \beta(x) \frac{\d}{\d x} + \gamma(x).
  \]
  The \emph{Green's function} $G(x, \xi)$ for $\mathcal{L}$ with given boundary conditions is the solution to
  \[
    \mathcal{L}_x G(x, \xi) = \delta(x - \xi),
  \]
  where $\mathcal{L}_x$ acts on the $x$ variable, satisfying the same homogeneous boundary conditions.
\end{defi}

\begin{prop}[Solution via Green's function]
  \label{prop:greens-solution}
  If $G(x, \xi)$ is the Green's function for $\mathcal{L}$ with homogeneous boundary conditions, then the solution to $\mathcal{L}y = f$ with those boundary conditions is
  \[
    y(x) = \int_{a}^b G(x, \xi) f(\xi) \;\d \xi.
  \]
\end{prop}
\begin{proof}
  Apply $\mathcal{L}$ to the integral:
  \[
    \mathcal{L}y = \int_a^b \mathcal{L}_x G(x, \xi) f(\xi)\;\d \xi = \int_a^b \delta(x - \xi) f(\xi) \;\d\xi = f(x). \qedhere
  \]
\end{proof}

\begin{remark}
  The Green's function represents the ``inverse'' of $\mathcal{L}$: symbolically, $y = \mathcal{L}^{-1}f$, where $\mathcal{L}^{-1}$ is an integral operator with kernel $G$.
\end{remark}

\subsubsection{Construction of Green's function for boundary value problems}

We construct the Green's function for Dirichlet boundary conditions $G(a, \xi) = G(b, \xi) = 0$.

\begin{enumerate}
  \item \textbf{Homogeneous solutions away from $\xi$}: Since $\mathcal{L}G = 0$ for $x \neq \xi$, we expand $G$ in homogeneous solutions. Let $y_1, y_2$ be a basis for $\mathcal{L}y = 0$ with $y_1(a) = 0$ and $y_2(b) = 0$. Then
    \[
      G(x, \xi) =
      \begin{cases}
        A(\xi) y_1(x) & a \leq x < \xi\\
        B(\xi) y_2(x) & \xi < x \leq b.
      \end{cases}
    \]

  \item \textbf{Continuity condition}: If $G$ were discontinuous at $\xi$, then $G''$ would contain $\delta'$, which cannot be balanced by $\mathcal{L}G = \delta$. Hence $G$ must be continuous:
    \[
      A(\xi) y_1(\xi) = B(\xi) y_2(\xi). \tag{$*$}
    \]

  \item \textbf{Jump condition on derivative}: Integrating $\mathcal{L}G = \delta(x-\xi)$ over $(\xi - \varepsilon, \xi + \varepsilon)$:
    \[
      \int_{\xi - \varepsilon}^{\xi + \varepsilon} \left[\alpha G'' + \beta G' + \gamma G\right]\d x = 1.
    \]
    The $\gamma G$ and $\beta G'$ terms vanish as $\varepsilon \to 0$ (since $G$ is continuous and $G'$ is bounded). Integration by parts gives
    \[
      \alpha(\xi)\left(G'(\xi^+) - G'(\xi^-)\right) = 1,
    \]
    yielding $B(\xi) y'_2(\xi) - A(\xi) y_1'(\xi) = 1/\alpha(\xi)$.
\end{enumerate}

\begin{defi}[Wronskian]
  \label{def:wronskian}
  The \emph{Wronskian} of two functions $y_1, y_2$ is $W = y_1 y_2' - y_2 y_1'$.
\end{defi}

\begin{thm}[Green's function for second-order BVP]
  \label{thm:greens-bvp}
  For the operator $\mathcal{L} = \alpha(x)\frac{d^2}{dx^2} + \beta(x)\frac{d}{dx} + \gamma(x)$ with Dirichlet conditions on $[a,b]$, the Green's function is
  \[
    G(x, \xi) = \frac{1}{\alpha(\xi)W(\xi)}
    \begin{cases}
      y_2(\xi) y_1(x) & a \leq x \leq \xi\\
      y_1(\xi) y_2(x) & \xi < x \leq b,
    \end{cases}
  \]
  where $y_1, y_2$ satisfy $\mathcal{L}y = 0$ with $y_1(a) = 0$, $y_2(b) = 0$, and $W$ is their Wronskian.

  Equivalently, using the Heaviside step function $\Theta$:
  \[
    G(x, \xi) = \frac{\Theta(\xi - x) y_2(\xi)y_1(x) + \Theta(x - \xi)y_1(\xi)y_2(x)}{\alpha(\xi)W(\xi)}.
  \]
\end{thm}

The general solution to $\mathcal{L}y = f$ is then
\[
  y(x) = y_1(x)\int_x^b \frac{y_2(\xi)f(\xi)}{\alpha(\xi)W(\xi)}\d\xi + y_2(x)\int_a^x \frac{y_1(\xi)f(\xi)}{\alpha(\xi)W(\xi)}\d\xi.
\]

\begin{eg}[Green's function for $-y'' - y = f$]
  \label{eg:greens-sinusoidal}
  Consider $\mathcal{L}y = -y'' - y = f$ on $(0, 1)$ with $y(0) = y(1) = 0$.

  The homogeneous equation $y'' + y = 0$ has solutions $\sin x$ and $\sin(1-x)$ satisfying the boundary conditions at $x=0$ and $x=1$ respectively. The Wronskian is
  \[
    W = -\sin x \cos(1 - x) - \sin(1 - x) \cos x = -\sin 1.
  \]
  With $\alpha = -1$, the Green's function is
  \[
    G(x, \xi) = \frac{\Theta(\xi - x) \sin(1 - \xi) \sin x + \Theta(x - \xi) \sin\xi \sin(1 - x)}{\sin 1}.
  \]
  The solution is
  \[
    y(x) = \frac{\sin x}{\sin 1} \int_{x}^1 \sin(1 - \xi)f(\xi)\d\xi + \frac{\sin(1 - x)}{\sin 1}\int_0^x \sin\xi\, f(\xi)\d\xi.
  \]
\end{eg}

\begin{eg}[String under gravity]
  \label{eg:greens-string}
  A string with mass density $\mu(x)$ under tension $T$ satisfies (from Newton's second law)
  \[
    \mu\frac{\partial^2 y}{\partial t^2} = T \frac{\partial^2 y}{\partial x^2} + \mu g.
  \]
  For the steady state ($\partial_t y = 0$) with $y(0) = y(L) = 0$:
  \[
    \frac{d^2 y}{d x^2} = -\frac{\mu(x) g}{T}.
  \]
  The Green's function satisfies $G'' = -\delta(x - \xi)$, representing a point mass at $x = \xi$:
  \begin{center}
    \begin{tikzpicture}
      \draw [gray] (-0.5, 0) -- (4.5, 0);
      \node [circ] at (0, 0) {};
      \node [circ] at (4, 0) {};
      \draw (0, 0) -- (1, -1) -- (4, 0);
      \node [circ] at (1, -1) {};
      \node [above] at (0, 0) {$0$};
      \node [above] at (4, 0) {$L$};
      \node [above] at (1, 0) {$\xi$};
      \draw [dashed] (1, 0) -- (1, -1);
      \draw [->] (1, -1) -- +(0, -0.5) node [below] {$y$};
    \end{tikzpicture}
  \end{center}
  The homogeneous solution $y'' = 0$ gives $y = Ax$ or $y = B(x - L)$. Thus
  \[
    G(x, \xi) =
    \begin{cases}
      A(\xi) x & 0 \leq x < \xi\\
      B(\xi)(x - L) & \xi < x \leq L.
    \end{cases}
  \]
  Continuity ($A\xi = B(\xi - L)$) and the jump condition ($B - A = 1$, since $\alpha = -1$) give
  \[
    A(\xi) = \frac{\xi - L}{L},\quad B(\xi) = -\frac{\xi}{L}.
  \]
  The Green's function is
  \[
    G(x, \xi) = \frac{(\xi - L)x}{L}\Theta(\xi - x) - \frac{\xi(x - L)}{L}\Theta(x - \xi).
  \]
  The full solution is
  \[
    y(x) = \frac{g}{T} \int_0^L G(x, \xi) \mu(\xi)\;\d \xi,
  \]
  representing the superposition of contributions from infinitesimal mass elements.
\end{eg}
\subsection{Green's functions for initial value problems}
For initial value problems (IVPs), we specify both the function value and its derivative at a single point, rather than values at two boundary points.

\begin{defi}[Green's function for IVP]
  \label{def:greens-ivp}
  Consider $\mathcal{L}y = f(t)$ with initial conditions $y(t_0) = y'(t_0) = 0$. The \emph{causal Green's function} satisfies $\mathcal{L}_t G(t, \tau) = \delta(t - \tau)$ with $G(t_0, \tau) = \partial_t G(t_0, \tau) = 0$.
\end{defi}

\begin{prop}[Structure of causal Green's function]
  \label{prop:causal-greens}
  Let $y_1, y_2$ be any basis of solutions to $\mathcal{L}y = 0$. The causal Green's function has the form
  \[
    G(t, \tau) =
    \begin{cases}
      0 & t_0 \leq t < \tau\\
      C(\tau) y_1(t) + D(\tau) y_2(t) & t > \tau.
    \end{cases}
  \]
\end{prop}
\begin{proof}
  For $t < \tau$, write $G = A(\tau)y_1(t) + B(\tau)y_2(t)$. The initial conditions give
  \[
    \begin{pmatrix}
      y_1(t_0) & y_2(t_0)\\
      y_1'(t_0) & y_2'(t_0)
    \end{pmatrix}
    \begin{pmatrix}
      A \\ B
    \end{pmatrix}
    =
    \begin{pmatrix}
      0 \\ 0
    \end{pmatrix}.
  \]
  Since $y_1, y_2$ are linearly independent, the matrix (the Wronskian at $t_0$) is non-singular, so $A = B = 0$.

  For $t > \tau$, the coefficients $C, D$ are determined by continuity at $\tau$ and the jump condition on $G'$.
\end{proof}

\begin{thm}[Causality]
  \label{thm:causality}
  The solution to the IVP $\mathcal{L}y = f$ with $y(t_0) = y'(t_0) = 0$ is
  \[
    y(t) = \int_{t_0}^t G(t, \tau) f(\tau)\;\d \tau.
  \]
  The upper limit is $t$ (not $\infty$) because $G(t, \tau) = 0$ for $\tau > t$. This expresses \emph{causality}: the solution at time $t$ depends only on the forcing at earlier times.
\end{thm}

\begin{eg}[Simple harmonic oscillator]
  \label{eg:greens-sho}
  Consider $\ddot{y} + y = f(t)$ with $y(0) = \dot{y}(0) = 0$.

  The homogeneous solutions are $\cos t$ and $\sin t$. For $t > \tau$, we write
  \[
    G(t, \tau) = C(\tau) \cos(t - \tau) + D(\tau) \sin(t - \tau),
  \]
  using shifted solutions for convenience. Continuity at $t = \tau$ gives $C(\tau) = 0$. The jump condition (with $\alpha = 1$) gives $D(\tau) = 1$. Thus
  \[
    G(t, \tau) = \Theta(t - \tau) \sin(t - \tau),
  \]
  and the solution is
  \[
    y(t) = \int_0^t \sin(t - \tau) f(\tau) \;\d \tau.
  \]
  This is a \emph{convolution} of $\sin t$ with the forcing $f(t)$.
\end{eg}

\section{Fourier transforms}
\subsection{The Fourier transform}
So far, we have considered writing a function $f: S^1 \to \C$ (or a periodic function) as a Fourier sum
\[
  f(\theta) = \sum_{n \in \Z} \hat{f}_n e^{in\theta},\quad \hat{f}_n = \frac{1}{2\pi}\int_{-\pi}^\pi e^{-in\theta}f(\theta)\;\d \theta.
\]
What if we don't have a periodic function, but just an arbitrary function $f:\R \to \C$? Can we still do Fourier analysis? Yes!

To start with, instead of $\hat{f}_n = \frac{1}{2\pi}\int_{-\pi}^\pi e^{-in\theta }f(\theta)\;\d \theta$, we define the Fourier transform as follows:
\begin{defi}[Fourier transform]
  \label{def:fourier-transform}
  The \emph{Fourier transform} of an (absolutely integrable) function $f: \R \to \C$ is defined as
  \[
    \tilde{f}(k) = \int_{-\infty}^\infty e^{-ikx}f(x)\;\d x
  \]
  for \emph{all} $k \in \R$. We will also write $\tilde{f}(k) = \mathcal{F}[f(x)]$.
\end{defi}
Note that for any $k$, we have
\[
  |\tilde{f}(k)| = \left|\int_{-\infty}^\infty e^{-ikx}f(x)\;\d x\right| \leq \int_{-\infty}^\infty |e^{-ikx} f(x)|\;\d x = \int_{-\infty}^\infty |f(x)|\;\d x.
\]
Since we have assumed that our function is absolutely integrable, this is finite, and the definition makes sense.

\begin{prop}[Properties of the Fourier transform]
  \label{prop:fourier-properties}
  Let $f, g: \R \to \C$ be absolutely integrable functions.
  \begin{enumerate}
  \item \emph{Linearity}: If $c_1, c_2$ are constants, then
    \[
      \mathcal{F}[c_1 f(x) + c_2 g(x)] = c_1 \mathcal{F}[f(x)] + c_2 \mathcal{F}[g(x)].
    \]
  \item \emph{Translation}: For $a \in \R$,
    \[
      \mathcal{F}[f(x - a)] = e^{-ika}\mathcal{F}[f(x)].
    \]
    A translation in $x$-space becomes a re-phasing in $k$-space.
  \item \emph{Modulation}: For $\ell \in \R$,
    \[
      \mathcal{F}[e^{i\ell x}f(x)] = \tilde{f}(k - \ell).
    \]
    A modulation in $x$-space becomes a translation in $k$-space.
  \item \emph{Scaling}: For $c \neq 0$,
    \[
      \mathcal{F}[f(cx)] = \frac{1}{|c|} \tilde{f}\left(\frac{k}{c}\right).
    \]
  \item \emph{Convolution}: The \emph{convolution} of $f$ and $g$ is
    \[
      (f * g)(x) = \int_{-\infty}^\infty f(x - y) g(y)\;\d y.
    \]
    The Fourier transform of a convolution is the product of Fourier transforms:
    \[
      \mathcal{F}[f * g] = \mathcal{F}[f] \cdot \mathcal{F}[g].
    \]
  \item \emph{Differentiation}: The Fourier transform turns differentiation into multiplication:
    \[
      \mathcal{F}[f'(x)] = ik \mathcal{F}[f(x)].
    \]
    Conversely, multiplication becomes differentiation:
    \[
      \mathcal{F}[xf(x)] = i\tilde{f}'(k).
    \]
  \end{enumerate}
\end{prop}

These properties allow us to break complicated Fourier transforms into simpler pieces.

\begin{prop}[Solving ODEs via Fourier transform]
  \label{prop:fourier-ode}
  Consider the differential equation $\mathcal{L}(\partial) y = f$, where
  \[
    \mathcal{L}(\partial) = \sum_{r = 0}^p c_r \frac{\d^r}{\d x^r}
  \]
  is a differential operator of order $p$ with constant coefficients. Taking Fourier transforms converts this to the algebraic equation
  \[
    \mathcal{L}(ik) \tilde{y}(k) = \tilde{f}(k),
  \]
  where $\mathcal{L}(ik) = \sum_{r=0}^p c_r (ik)^r$. The solution in Fourier space is
  \[
    \tilde{y}(k) = \frac{\tilde{f}(k)}{\mathcal{L}(ik)}.
  \]
\end{prop}
\begin{eg}[Screened Poisson equation]
  \label{eg:screened-poisson}
  Consider $\nabla^2 \phi - m^2 \phi = \rho(\mathbf{x})$ for $\phi: \R^n \to \C$. In higher dimensions, the Fourier transform is
  \[
    \tilde{\phi}(\mathbf{k}) = \int_{\R^n} e^{-i\mathbf{k}\cdot \mathbf{x}} \phi(\mathbf{x})\;\d^n \mathbf{x}.
  \]
  Using integration by parts twice, $\mathcal{F}[\nabla^2 \phi] = -|\mathbf{k}|^2 \tilde{\phi}(\mathbf{k})$. Thus the transformed equation is
  \[
    (-|\mathbf{k}|^2 - m^2) \tilde{\phi}(\mathbf{k}) = \tilde{\rho}(\mathbf{k}),
  \]
  giving
  \[
    \tilde{\phi}(\mathbf{k}) = -\frac{\tilde{\rho}(\mathbf{k})}{|\mathbf{k}|^2 + m^2}.
  \]
\end{eg}

\begin{rmk}
  Differential equations become algebraic in Fourier space. The challenge is recovering the solution in position space, which requires inverting the Fourier transform.
\end{rmk}

\subsection{The Fourier inversion theorem}
We need to recover a function $f(x)$ from its Fourier transform $\tilde{f}(k)$. A heuristic derivation proceeds by taking the $L \to \infty$ limit of Fourier series, treating the discrete sum as a Riemann sum that becomes an integral.

\begin{thm}[Fourier inversion]
  \label{thm:fourier-inversion}
  For sufficiently well-behaved functions, the \emph{inverse Fourier transform} recovers $f$ from $\tilde{f}$:
  \[
    f(x) = \mathcal{F}^{-1}[\tilde{f}(k)] = \frac{1}{2\pi} \int_{-\infty}^\infty e^{ikx} \tilde{f}(k) \;\d k.
  \]
\end{thm}

\begin{rmk}
  The heuristic derivation involves taking $L \to \infty$ and $\Delta k = 2\pi/L \to 0$ simultaneously, which is not rigorous. A proper proof requires careful analysis of convergence.
\end{rmk}

The inverse Fourier transform differs from the forward transform only by a factor of $\frac{1}{2\pi}$ and a sign change in the exponent. Some conventions distribute the $2\pi$ symmetrically by defining $\mathcal{F}[f] = \frac{1}{\sqrt{2\pi}} \int e^{-ikx} f(x)\,\d x$. We use the asymmetric convention, which gives:

\begin{prop}[Duality]
  \label{prop:fourier-duality}
  The Fourier transform and inverse are related by
  \[
    \mathcal{F}[f(x)](k) = 2\pi \cdot \mathcal{F}^{-1}[f(-x)](k).
  \]
  This allows us to compute inverse Fourier transforms using forward transforms.
\end{prop}

\begin{rmk}
  This duality does not occur for Fourier series, where the forward operation (integration) and inverse operation (discrete summation) are fundamentally different.
\end{rmk}

\begin{prop}[Convolution and multiplication]
  \label{prop:convolution-multiplication}
  The Fourier transform exchanges convolution and multiplication:
  \begin{enumerate}
    \item $\mathcal{F}[f * g] = \tilde{f} \cdot \tilde{g}$ (convolution becomes multiplication).
    \item $\mathcal{F}[f \cdot g] = \frac{1}{2\pi} \tilde{f} * \tilde{g}$ (multiplication becomes convolution).
  \end{enumerate}
\end{prop}

\begin{eg}[Screened Poisson: inversion]
  \label{eg:screened-poisson-inversion}
  From Example~\ref{eg:screened-poisson}, the screened Poisson equation $\nabla^2 \phi - m^2 \phi = \rho$ has Fourier solution
  \[
    \tilde{\phi}(\mathbf{k}) = -\frac{\tilde{\rho}(\mathbf{k})}{|\mathbf{k}|^2 + m^2}.
  \]
  Inverting gives
  \[
    \phi(\mathbf{x}) = -\frac{1}{(2\pi)^n} \int_{\R^n} e^{i\mathbf{k}\cdot \mathbf{x}} \frac{\tilde{\rho}(\mathbf{k})}{|\mathbf{k}|^2 + m^2}\;\d^n \mathbf{k}.
  \]
  Since this is the inverse transform of a product, $\phi$ is a convolution of $\rho$ with $\mathcal{F}^{-1}[(|\mathbf{k}|^2 + m^2)^{-1}]$. This is analogous to how Green's function solutions are convolutions of the Green's function with the forcing term.
\end{eg}

\subsection{Parseval's theorem for Fourier transforms}
\begin{thm}[Parseval's theorem for Fourier transforms]
  \label{thm:parseval-fourier}
  Let $f, g: \R \to \C$ be functions such that $\tilde{f}, \tilde{g}$ exist and $\mathcal{F}^{-1}[\tilde{f}] = f$, $\mathcal{F}^{-1}[\tilde{g}] = g$. Then
  \[
    (f, g) = \int_\R f^*(x) g(x)\;\d x = \frac{1}{2\pi}(\tilde{f}, \tilde{g}).
  \]
  In particular,
  \[
    \|f\|^2 = \frac{1}{2\pi} \|\tilde{f}\|^2.
  \]
\end{thm}

\begin{rmk}
  The Fourier transform preserves the $L^2$ norm up to a factor of $\sqrt{2\pi}$. With the symmetric convention $\mathcal{F}[f] = \frac{1}{\sqrt{2\pi}} \int e^{-ikx} f(x)\,\d x$, it would be an isometry.
\end{rmk}
\begin{proof}
  \begin{align*}
    (f, g) &= \int_\R f^*(x) g(x)\;\d x\\
    &= \int_{-\infty}^\infty f^*(x) \left[\frac{1}{2\pi}\int_{-\infty}^\infty e^{ikx} \tilde{g}(x) \;\d k\right]\;\d x\\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty \left[\int_{-\infty}^\infty f^*(x) e^{ikx} \;\d x\right] \tilde{g}(k)\;\d k\\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty \left[\int_{-\infty}^\infty f(x) e^{-ikx} \;\d x\right]^* \tilde{g}(k)\;\d k\\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty \tilde{f}^*(k) \tilde{g}(k) \;\d k\\
    &= \frac{1}{2\pi}(\tilde{f}, \tilde{g}).\qedhere
  \end{align*}
\end{proof}
\subsection{A word of warning}
The Fourier inversion theorem requires care about convergence.

\begin{eg}[Box function]
  \label{eg:box-function}
  Consider the box function
  \[
    f(x) =
    \begin{cases}
      1 & |x| < 1\\
      0 & |x| \geq 1.
    \end{cases}
  \]
  \begin{center}
    \begin{tikzpicture}
      \draw [->] (-3, 0) -- (3, 0) node [right] {$x$};
      \draw [->] (0, -0.5) -- (0, 2) node [above] {$y$};

      \draw [mred, semithick] (-2.5, 0) -- (-1, 0) -- (-1, 1) -- (1, 1) -- (1, 0) -- (2.5, 0);
    \end{tikzpicture}
  \end{center}
  This function is absolutely integrable, and its Fourier transform is
  \[
    \tilde{f}(k) = \int_{-1}^1 e^{-ikx} \;\d x = \frac{2\sin k}{k}.
  \]
\end{eg}

\begin{prop}[Sinc is not absolutely integrable]
  \label{prop:sinc-not-integrable}
  The function $\operatorname{sinc}(k) = \frac{\sin k}{k}$ is not absolutely integrable on $\R$.
\end{prop}

\begin{proof}
  On the intervals $[(n + \frac{1}{4})\pi, (n + \frac{3}{4})\pi]$, we have $|\sin k| \geq \frac{1}{\sqrt{2}}$. Thus
  \[
    \int_{-\infty}^\infty \left|\frac{\sin k}{k}\right|\;\d k \geq \sqrt{2} \sum_{n = 0}^N \int_{(n + 1/4) \pi}^{(n + 3/4)\pi} \frac{\d k}{(n + 1)\pi} = \frac{\sqrt{2}}{2} \sum_{n = 0}^N \frac{1}{n + 1},
  \]
  which diverges as $N \to \infty$.
\end{proof}

\begin{rmk}
  This means the inverse Fourier transform $\mathcal{F}^{-1}[\tilde{f}]$ does not converge absolutely for the box function. The inversion still works, but requires more careful analysis. In practice, we often ignore such subtleties.
\end{rmk}

\subsection{Fourier transformation of distributions}
We have
\[
  \mathcal{F}[\delta(x)] = \int_{-\infty}^\infty e^{ikx} \delta (x)\;\d x = 1.
\]
Hence we have
\[
  \mathcal{F}^{-1}[1] = \frac{1}{2\pi} \int_{-\infty}^\infty e^{ikx}\;\d k = \delta (x).
\]
Of course, it is extremely hard to make sense of this integral. It quite obviously diverges as a normal integral, but we can just have faith and believe this makes sense as long as we are talking about distributions.

Similarly, from our rules of translations, we get
\[
  \mathcal{F}[\delta(x - a)] = e^{-ika}
\]
and
\[
  \mathcal{F}[e^{-i\ell x}] = 2\pi \delta(k - \ell),
\]
Hence we get
\[
  \mathcal{F}[\cos (\ell x)] = \mathcal{F}\left[\frac{1}{2}( e^{i\ell x} + e^{-i\ell x})\right] = \frac{1}{2} \mathcal{F}[e^{i\ell x}] + \frac{1}{2} \mathcal{F}[e^{-i\ell x}] = \pi [\delta (k - \ell) + \delta(k + \ell)].
\]
We see that highly localized functions in $x$-space have very spread-out behaviour in $k$-space, and vice versa.

\subsection{Linear systems and response functions}
Suppose we have an amplifier that modifies an input signal $I(t)$ to produce an output $O(t)$. Typically, amplifiers work by modifying the amplitudes and phases of specific frequencies in the output. By Fourier's inversion theorem, we know
\[
  I(t) = \frac{1}{2\pi} \int e^{i\omega t} \tilde{I}(\omega) \;\d \omega.
\]
This $\tilde{I}(\omega)$ is the \emph{resolution} of $I(t)$.

We specify what the amplifier does by the \emph{transfer function} $\tilde{R}(\omega)$ such that the output is given by
\[
  O(t) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{i\omega t} \tilde{R}(\omega) \tilde{I}(\omega)\;\d \omega.
\]
Since this $\tilde{R}(\omega) \tilde{I}(\omega)$ is a product, on computing $O(t) = \mathcal{F}^{-1}[\tilde{R}(\omega) \tilde{I}(\omega)]$, we obtain a \emph{convolution}
\[
  O(t) = \int_{-\infty}^\infty R(t - u) I(u)\;\d u,
\]
where
\[
  R(t) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{i\omega t} \tilde{R}(\omega) \;\d \omega
\]
is the \emph{response function}. By plugging it directly into the equation above, we see that $R(t)$ is the response to an input signal $I(t) = \delta(t)$.

Now note that causality implies that the amplifier cannot ``respond'' before any input has been given. So we must have $R(t) = 0$ for all $t < 0$.

Assume that we only start providing input at $t = 0$. Then
\[
  O(t) = \int_{-\infty}^\infty R(t - u) I(u)\;\d u = \int_0^t R(t - u)I(u)\;\d u.
\]
This is exactly the same form of solution as we found for initial value problems with the response function $R(t)$ playing the role of the Green's function.

\subsection{General form of transfer function}
To model the situation, suppose the amplifier's operation is described by the ordinary differential equation
\[
  I(t) = \mathcal{L}_m O(t),
\]
where
\[
  \mathcal{L}_m = \sum_{i = 0}^m a_i \frac{\d^i}{\d t^i}
\]
with $a_i \in \C$. In other words, we have an $m$th order ordinary differential equation with constant coefficients, and the input is the forcing function. Notice that we are not saying that $O(t)$ can be expressed as a function of derivatives of $I(t)$. Instead, $I(t)$ influences $O(t)$ by being the forcing term the differential equation has to satisfy. This makes sense because when we send an input into the amplifier, this input wave ``pushes'' the amplifier, and the amplifier is forced to react in some way to match the input.

Taking the Fourier transform and using $\mathcal{F}\left[\frac{\d O}{\d t}\right] = i\omega \tilde{O}(\omega)$, we have
\[
  \tilde{I}(\omega) = \sum_{j = 0}^m a_j (i\omega)^j \tilde{O}(\omega).
\]
So we get
\[
  \tilde{O}(\omega) = \frac{\tilde{I}(\omega)}{a_0 + i\omega a_1 + \cdots + (i\omega)^m a_m}.
\]
Hence, the transfer function is just
\[
  \tilde{R}(\omega) = \frac{1}{a_0 + i\omega a_1 + \cdots + (i\omega)^m a_m}.
\]
The denominator is an $n$th order polynomial. By the fundamental theorem of algebra, it has $m$ roots, say $c_j \in \C$ for $j = 1, \cdots, J$, where $c_j$ has multiplicity $k_j$. Then we can write our transfer function as
\[
  \tilde{R}(\omega) = \frac{1}{a_m} \prod_{j = 1}^J \frac{1}{(i\omega - c_j)^{k_j}}.
\]
By repeated use of partial fractions, we can write this as
\[
  \tilde{R}(\omega) = \sum_{j = 1}^J \sum_{r = 1}^{k_j} \frac{\Gamma_{rj}}{(i\omega - c_j)^r}
\]
for some constants $\Gamma_{rj} \in \C$.

By linearity of the (inverse) Fourier transform, we can find $O(t)$ if we know the inverse Fourier transform of all functions of the form
\[
  \frac{1}{(i\omega - \alpha)^p}.
\]
To compute this, there are many possible approaches. We can try to evaluate the integral directly, learn some fancy tricks from complex analysis, or cheat, and let the lecturer tell you the answer. Consider the function
\[
  h_0(t) =
  \begin{cases}
    e^{\alpha t} & t > 0\\
    0 & \text{otherwise}
  \end{cases}
\]
The Fourier transform is then
\[
  \tilde{h}_o(\omega) = \int_{-\infty}^\infty e^{-i\omega t} h_0(t) \;\d t = \int_0^\infty e^{(\alpha - i \omega)t} \;\d t = \frac{1}{i\omega - \alpha}
\]
provided $\Re(\alpha) < 0$ (so that $e^{(\alpha - i\omega) t} \to 0$ as $t \to \infty$). Similarly, we can let
\[
  h_1(t) =
  \begin{cases}
    te^{\alpha t} & t > 0\\
    0 & \text{otherwise}
  \end{cases}
\]
Then we get
\[
  \tilde{h}_1(\omega) = \mathcal{F}[t h_0(t)] = i\frac{\d}{\d \omega} \mathcal{F}[h_0(t)] = \frac{1}{(i \omega - \alpha)^2}.
\]
Proceeding inductively, we can define
\[
  h_p(t) =
  \begin{cases}
    \frac{t^p}{p!} e^{\alpha t} & t > 0\\
    0 & \text{otherwise}
  \end{cases},
\]
and this has Fourier transform
\[
  \tilde{h}_p (\omega) = \frac{1}{(i \omega - \alpha)^{p + 1}},
\]
again provided $\Re(\alpha) < 0$. So the response function is a linear combination of these functions $h_p(t)$ (if any of the roots $c_j$ have non-negative real part, then it turns out the system is unstable, and is better analysed by the Laplace transform). We see that the response function does indeed vanish for all $t < 0$. In fact, each term (except $h_0$) increases from zero at $t = 0$ to rise to some maximum before eventually decaying exponentially.

Fourier transforms can also be used to solve ordinary differential equations on the whole real line $\R$, provided the functions are sufficiently well-behaved. For example, suppose $y: \R \to \R$ solves
\[
  y'' - A^2 y = -f(x)
\]
with $y$ and $y' \to 0$ as $|x| \to \infty$. Taking the Fourier transform, we get
\[
  \tilde{y}(k) = \frac{\tilde{f}(k)}{k^2 + A^2}.
\]
Since this is a product, the inverse Fourier transform will be a convolution of $f(x)$ and an object whose Fourier transform is $\frac{1}{k^2 + A^2}$. Again, we'll cheat. Consider
\[
  h(x) = \frac{1}{2\mu} e^{-\mu|x|}.
\]
with $\mu > 0$. Then
\begin{align*}
  \tilde{f}(k) &= \frac{1}{2\mu} \int_{-\infty}^\infty e^{-ikx} e^{-\mu |x|}\;\d x\\
  &= \frac{1}{\mu} \Re \int_0^\infty e^{-(\mu + ik)x}\;\d x \\
  &= \frac{1}{\mu} \Re \frac{1}{ik + \mu} \\
  &= \frac{1}{\mu^2 + k^2}.
\end{align*}
Therefore we get
\[
  y(x) = \frac{1}{2A} \int_{-\infty}^\infty e^{-A|x - u|} f(u)\;\d u.
\]
So far, we have done Fourier analysis over some abelian groups. For example, we've done it over $S^1$, which is an abelian group under multiplication, and $\R$, which is an abelian group under addition. We will next look at Fourier analysis over another abelian group, $\Z_m$, known as the discrete Fourier transform.

\subsection{The discrete Fourier transform}
Recall that the Fourier transform is defined as
\[
  \tilde{f}(k) = \int_\R e^{-ikx} f(x)\;\d x.
\]
To find the Fourier transform, we have to know how to perform the integral. If we cannot do the integral, then we cannot do the Fourier transform. However, it is usually difficult to perform integrals for even slightly more complicated functions.

A more serious problem is that we usually don't even have a closed form of the function $f$ for us to perform the integral. In real life, $f$ might be some radio signal, and we are just given some data of what value $f$ takes at different points.. There is no hope that we can do the integral exactly.

Hence, we first give ourselves a simplifying assumption. We suppose that $f$ is mostly concentrated in some finite region $[-R, S]$. More precisely, $|f(x)|\ll 1$ for $x \not\in [-R, S]$ for some $R, S > 0$. Then we can approximate our integral as
\[
  \tilde{f}(k) = \int_{-R}^S e^{-ikx} f(x)\;\d x.
\]
Afterwards, we perform the integral numerically. Suppose we ``sample'' $f(x)$ at
\[
  x = x_j = -R + j\frac{R + S}{N}
\]
for $N$ a large integer and $j = 0, 1, \cdots, N - 1$. Then
\[
  \tilde{f}(k) \approx \frac{R + S}{N} \sum_{j = 0}^{N - 1} f(x_j) e^{-ik x_j}.
\]
This is just the Riemann sum.

Similarly, our computer can only store the result $\tilde{f}(k)$ for some finite list of $k$. Let's choose these to be at
\[
  k = k_m = \frac{2\pi m}{R + S}.
\]
Then after some cancellation,
\begin{align*}
  \tilde{f}(k_m) &\approx \frac{R + S}{N} e^{\frac{2\pi i m R}{R + S}} \sum_{j = 0}^{N - 1} f(x_j) e^{\frac{2\pi i}{N} jm}\\
  &= (R + S) e^{\frac{2\pi i m R}{R + S}} \left[\frac{1}{N} \sum_{j = 0}^{N - 1} f(x_j) \omega^{-jm}\right],
\end{align*}
where
\[
  \omega = e^{\frac{2\pi i}{N}}
\]
is an $N$th root of unity. We call the thing in the brackets
\[
  F(m) = \frac{1}{N} \sum_{j = 0}^{N - 1} f(x_j) \omega^{-jm}.
\]
Of course, we've thrown away lots of information about our original function $f(x)$, since we made approximations all over the place. For example, we have already lost all knowledge of structures varying more rapidly than our sampling interval $\frac{R + S}{N}$. Also, $F(m + N) = F(m)$, since $\omega^N = 1$. So we have ``forced'' some periodicity into our function $F$, while $\tilde{f}(k)$ itself was not necessarily periodic.

For the usual Fourier transform, we were able to re-construct our original function from the $\tilde{f}(k)$, but here we clearly cannot. However, if we know the $F(m)$ for all $m = 0, 1, \cdots, N - 1$, then we \emph{can} reconstruct the exact values of $f(x_j)$ for all $j$ by just solving linear equations. To make this more precise, we want to put what we're doing into the linear algebra framework we've been using. Let $G = \{1, \omega, \omega^2, \cdots, \omega^{N - 1}\}$. For our purposes below, we can just treat this as a discrete set, and $\omega^i$ are just meaningless symbols that happen to visually look like the powers of our $N$th roots of unity.

Consider a function $g: G \to \C$ defined by
\[
  g(\omega^j) = f(x_j).
\]
This is actually nothing but just a new notation for $f(x_j)$. Then using this new notation, we have
\[
  F(m) = \frac{1}{N} \sum_{j = 0}^{N - 1} \omega^{-jm} g(\omega^j).
\]
The space of \emph{all} functions $g: G \to \C$ is a finite-dimensional vector space, isomorphic to $\C^N$. This has an inner product
\[
  (f, g) = \frac{1}{N} \sum_{j = 0}^{N - 1} f^*(\omega^j) g(\omega^j).
\]
This inner product obeys
\begin{align*}
  (g, f) &= (f, g)^*\\
  (f, c_1 g_1 + c_2 g_2) &= c_1 (f, g_1) + c_2 (f, g_2)\\
  (f, f) &\geq 0\text{ with equality iff }f(\omega_j) = 0
\end{align*}
Now let $e_n: G \to \C$ be the function
\[
  e_m(\omega^j) = \omega^{jm}.
\]
Then the set of functions $\{e_m\}$ for $m = 0, \cdots, N - 1$ is an orthonormal basis with respect to our inner product. To show this, we can compute
\begin{align*}
  (e_m, e_m) &= \frac{1}{N} \sum_{j = 0}^{N - 1} e_m^*(\omega^j) e_m(\omega^j)\\
  &= \frac{1}{N} \sum_{j = 0}^{N - 1} \omega^{-jm} \omega^{jm}\\
  &= \frac{1}{N} \sum_{j = 0}^{N - 1} 1\\
  &= 1.
\end{align*}
For $n \not =m$, we have
\begin{align*}
  (e_n, e_m) &= \frac{1}{N} \sum_{j = 0}^{N - 1} e_n^* (\omega^j) e^m(\omega^j)\\
  &= \frac{1}{N} \sum_{j = 0}^{N - 1} \omega^{j(m - n)}\\
  &= \frac{1}{N} \frac{1 - \omega^{(m - n)N}}{1 - \omega^{m - n}}.
\end{align*}
Since $m - n$ is an integer and $\omega$ is an $N$th root of unity, we know that $\omega^{(m - n)N} = 1$. So the numerator is $0$. However, since $n \not= m$, $m - n\not=0$. So the denominator is non-zero. So we get $0$. Hence
\[
  (e_n, e_m) = 0 .
\]
We can now rewrite our $F(m)$ as
\[
  F(m) = \frac{1}{N}\sum_{j = 0}^{N - 1}\omega^{-jm} f(x_j) = \frac{1}{N}\sum_{j = 0}^{N - 1} e_m^* (\omega^j) g(\omega^j) = (e_m, g).
\]
Hence we can expand our $g$ as
\[
  g = \sum_{m = 0}^{N - 1} (e_m, g) e_m = \sum_{m = 0}^{N - 1} F(m) e_m.
\]
Writing $f$ instead of $g$, we recover the formula
\[
  f(x_j) = g(\omega^j) = \sum_{m = 0}^{N - 1} F(m) e_m(\omega^j).
\]
If we forget about our $f$s and just look at the $g$, what we have effectively done is take the Fourier transform of functions taking values on $G = \{1, \omega, \cdots, \omega^{N - 1}\}\cong \Z_N$.

This is exactly analogous to what we did for the Fourier transform, except that everything is now discrete, and we don't have to worry about convergence since these are finite sums.

\subsection{The fast Fourier transform*}
What we've said so far is we've defined
\[
  F(m) = \frac{1}{N} \sum_{j = 0}^{N - 1} \omega^{-mj}g(\omega^j).
\]
To compute this directly, even given the values of $\omega^{-mj}$ for all $j, m$, this takes $N - 1$ additions and $N + 1$ multiplications. This is $2N$ operations for a \emph{single} value of $m$. To know $F(m)$ for all $m$, this takes $2N^2$ operations.

This is a problem. Historically, during the cold war, especially after the Cuban missile crisis, people were in fear that the world will one day go into a huge nuclear war, and something had to be done to prevent this. Thus the countries decided to come up with a treaty to ensure people don't perform nuclear testings anymore.

However, the obvious problem is that we can't easily check if other countries are doing nuclear tests. Nuclear tests are easy to spot if the test is done above ground, since there will be a huge explosion. However, if it is done underground, it is much harder to test.

They then came up with the idea to put some seismometers all around the world, and record vibrations in the ground. To distinguish normal crust movements from nuclear tests, they wanted to take the Fourier transform and analyze the frequencies of the vibrations. However, they had a \emph{large} amount of data, and the value of $N$ is on the order of magnitude of a few million. So $2N^2$ will be a huge number that the computers at that time were not able to handle. So they need to find a trick to perform Fourier transforms quickly. This is known as the \emph{fast Fourier transform}. Note that this is nothing new mathematically. This is entirely a computational trick.

Now suppose $N = 2M$. We can write
\begin{align*}
  F(m) &= \frac{1}{2M} \sum_{j = 0}^{2M - 1} \omega^{-jm} g(\omega^j) \\
  &= \frac{1}{2}\left[\frac{1}{M}\sum_{k = 0}^{M - 1} \omega^{-2km} g(\omega^{2k}) + \omega^{-(2k + 1)m} g(\omega^{2k + 1})\right].
\end{align*}
We now let $\eta$ be an $M$th root of unity, and define
\[
  G(\eta^k) = g(\omega^{2k}),\quad H(\eta^k) = g(\omega^{2k + 1}).
\]
We then have
\begin{align*}
  F(m) &= \frac{1}{2} \left[\frac{1}{M} \sum_{k = 0}^{M - 1} \eta^{-km} G(\eta^k) + \frac{\omega^{-m}}{M} \sum_{k = 0}^{M - 1} \eta^{-km} H(\eta^k)\right]\\
  &= \frac{1}{2}[\tilde{G}(m) + \omega^{-m} \tilde{H}(m)].
\end{align*}
Suppose we are given the values of $\tilde{G}(m)$ and $\tilde{H}(m)$ and $\omega^{-m}$ for all $m = \{0, \cdots, N - 1\}$. Then we can compute $F(m)$ using $3$ operations per value of $m$. So this takes $3 \times N = 6M$ operations for all $M$.

We can compute $\omega^{-m}$ for all $m$ using at most $2N$ operations, and suppose it takes $P_M$ operations to compute $\tilde{G}(m)$ for all $m$. Then the number of operations needed to compute $F(m)$ is
\[
  P_{2M} = 2 P_M + 6M + 2M.
\]
Now let $N = 2^n$. Then by iterating this, we can find
\[
  P_N \leq 4N \log_2 N \ll N^2.
\]
So by breaking the Fourier transform apart, we are able to greatly reduce the number of operations needed to compute the Fourier transform. This is used nowadays everywhere for everything.
\section{More partial differential equations}
\subsection{Well-posedness}
Recall that to find a unique solution of Laplace's equation $\nabla^2 \phi = 0$ on a bounded domain $\Omega \subseteq \R^n$, we imposed the Dirichlet boundary condition $\phi|_{\partial \Omega} = f$ or the Neumann boundary condition $\mathbf{n}\cdot \nabla \phi|_{\partial \Omega} = g$.

For the heat equation $\frac{\partial \phi}{\partial t} = \kappa \nabla^2 \phi$ on $\Omega \times [0, \infty)$, we asked for $\partial |_{\partial \Omega \times [0, \infty)} = f$ and also $\phi|_{\Omega \times \{0\}} = g$.

For the wave equation $\frac{\partial^2 \phi}{\partial t^2} = c^2 \nabla^2 \phi$ on $\Omega \times [0, \infty)$, we imposed $\phi|_{\partial \Omega \times [0, \infty)} = f$, $\phi|_{\Omega \times \{0\}} = g$ and also $\partial_t \phi|_{\Omega \times \{0\}} = h$.

All the boundary and initial conditions restrict the value of $\phi$ on some co-dimension 1 surface, i.e.\ a surface whose dimension is one less than the domain. This is known as \emph{Cauchy data} for our partial differential equation.

\begin{defi}[Well-posed problem]
  A partial differential equation problem is said to be well-posed if its Cauchy data means
  \begin{enumerate}
    \item A solution exists;
    \item The solution is unique;
    \item A ``small change'' in the Cauchy data leads to a ``small change'' in the solution.
  \end{enumerate}
\end{defi}
To understand the last condition, we need to make it clear what we mean by ``small change''. To do this properly, we need to impose some topology on our space of functions, which is some technicalities we will not go into. Instead, we can look at a simple example.

Suppose we have the heat equation $\partial_t \phi = \kappa \nabla^2 \phi$. We know that whatever starting condition we have, the solution quickly smooths out with time. Any spikiness of the initial conditions get exponentially suppressed. Hence this is a well-posed problem --- changing the initial condition slightly will result in a similar solution.

However, if we take the heat equation but run it backwards in time, we get a non-well-posed problem. If we provide a tiny, small change in the ``ending condition'', as we go back in time, this perturbation grows \emph{exponentially}, and the result could vary wildly.

Another example is as follows: consider the Laplace's equation $\nabla^2 \phi$ on the upper half plane $(x, y) \in \R \times \R^{\geq 0}$ subject to the boundary conditions
\[
  \phi(x, 0) = 0,\quad \partial_y \phi(x, 0) = g(x).
\]
If we take $g(x) = 0$, then $\phi(x, y) = 0$ is the unique solution, obviously.

However, if we take $g(x) = \frac{\sin Ax}{A}$, then we get the unique solution
\[
  \phi(x, y) = \frac{\sin (Ax) \sinh (Ay)}{A^2}.
\]
So far so good. However, now consider the limit as $A \to \infty$. Then
\[
  g(x) = \frac{\sin (Ax)}{A} \to 0.
\]
for all $x \in \R$. However, at the special point $\phi\left(\frac{\pi}{2A}, y\right)$, we get
\[
  \phi\left(\frac{\pi}{2A}, y\right) = \frac{\sinh (Ay)}{A^2} \to A^{-2} e^{Ay},
\]
which is unbounded. So as we take the limit as our boundary conditions $g(x) \to 0$, we get an unbounded solution.

How can we make sure this doesn't happen? We're first going to look at first order equations.

\subsection{Method of characteristics}
\subsubsection*{Curves in the plane}
Suppose we have a curve on the plane $\mathbf{x}: \R \to \R^2$ given by $s\mapsto (x(s), y(s))$.

\begin{defi}[Tangent vector]
  The tangent vector to a smooth curve $C$ given by $\mathbf{x}: \R \to \R^2$ with $\mathbf{x}(s) = (x(s), y(s))$ is
  \[
    \left(\frac{\d x}{\d s}, \frac{\d y}{\d s}\right).
  \]
\end{defi}

If we have some quantity $\phi: \R^2 \to \R$, then its value along the curve $C$ is just $\phi(x(s), y(s))$.

A vector field $\mathbf{V}(x, y): \R^2 \to \R^2$ defines a family of curves. To imagine this, suppose we are living in a river. The vector field tells us the how the water flows at each point. We can then obtain a curve by starting a point and flow along with the water. More precisely,
\begin{defi}[Integral curve]
  Let $\mathbf{V}(x, y): \R^2 \to \R^2$ be a vector field. The \emph{integral curves} associated to $\mathbf{V}$ are curves whose tangent $\left(\frac{\d x}{\d s}, \frac{\d y}{\d s}\right)$ is just $\mathbf{V}(x, y)$.
\end{defi}
For sufficiently regular vector fields, we can fill the space with different curves. We will parametrize which curve we are on by the parameter $t$. More precisely, we have a curve $B = (x(t), y(t))$ that is \emph{transverse} (i.e.\ nowhere parallel) to our family of curves, and we can label the members of our family by the value of $t$ at which they intersect $B$.
\begin{center}
  \begin{tikzpicture}
    \draw rectangle (3, 3);

    \draw plot [smooth] coordinates {(0, 1) (0.7, 1.3) (0.9, 1.2) (1.6, 1.5) (2.3, 1) (3, 0.9)};
    \draw plot [smooth] coordinates {(0, 1.3) (0.7, 1.6) (0.9, 1.6) (1.6, 1.8) (2.3, 1.2) (3, 1.2)};
    \draw plot [smooth] coordinates {(0, 1.7) (0.9, 1.9) (1.6, 2.2) (2.3, 1.7) (3, 1.8)};
    \draw plot [smooth] coordinates {(0, 0.8) (0.7, 1.1) (0.9, 0.9) (1.6, 1.3) (2.3, 0.8) (3, 0.6)};

    \node [right] at (3, 1.8) {$C_4$};
    \node [right] at (3, 1.2) {$C_3$};
    \node [right] at (3, 0.9) {$C_2$};
    \node [right] at (3, 0.6) {$C_1$};

    \draw [mred] plot [smooth] coordinates {(1, 0) (1.3, 1.4) (1.2, 1.9) (1.4, 3)};

    \node [mred, right] at (1.3, 2.4) {$B(t)$};
  \end{tikzpicture}
\end{center}
We can thus label our family of curves by $(x(s, t), y(s, t))$. If the Jacobian
\[
  J = \frac{\partial x}{\partial s} \frac{\partial y}{\partial t} - \frac{\partial x}{\partial t} \frac{\partial y}{\partial s} \not= 0,
\]
then we can invert this to find $(s, t)$ as a function of $(x, y)$, i.e.\ at any point, we know which curve we are on, and how far along the curve we are.

This means we now have a new coordinate system $(s, t)$ for our points in $\R^2$. It turns out by picking the right vector field $\mathbf{V}$, we can make differential equations much easier to solve in this new coordinate system.

\subsubsection*{The method of characteristics}
Suppose $\phi: \R^2 \to \R$ obeys
\[
  a(x, y) \frac{\partial \phi}{\partial x} + b(x, y) \frac{\partial \phi}{\partial y} = 0.
\]
We can define our vector field as
\[
  \mathbf{V}(x, y) =
  \begin{pmatrix}
    a(x, y)\\
    b(x, y)
  \end{pmatrix}.
\]
Then we can write the differential equation as
\[
  \mathbf{V}\cdot \nabla \phi = 0.
\]
Along any particular integral curve of $\mathbf{V}$, we have
\[
  \frac{\partial \phi}{\partial s} = \frac{\d x(s)}{\d s} \frac{\partial \phi}{\partial x} + \frac{\d y(s)}{\d s} \frac{\partial \phi}{\partial y} = \mathbf{V}\cdot \nabla \phi,
\]
where the integral curves of $\mathbf{V}$ are determined by
\[
  \left.\frac{\partial x}{\partial s}\right|_t = a(x, y),\quad \left.\frac{\partial y}{\partial s}\right|_t = b(x, y).
\]
This is known as the characteristic equation.

Hence our partial differential equation just becomes the equation
\[
  \left.\frac{\partial \phi}{\partial s}\right|_t = 0.
\]
To get ourselves a well-posed problem, we have to specify our boundary data along a transverse curve $B$. We pick our transverse curve as $s = 0$, and we suppose we are given the initial data
\[
  \phi(0, t) = h(t).
\]
Since $\phi$ does not vary with $s$, our solution automatically is
\[
  \phi(s, t) = h(t).
\]
Then $\phi(x, y)$ is given by inverting the characteristic equations to find $t(x, y)$.

We do a few examples to make this clear.

\begin{eg}[Trivial]
  Suppose $\phi$ obeys
  \[
    \left.\frac{\partial \phi}{\partial x}\right|_y = 0.
  \]
  We are given the boundary condition
  \[
    \phi(0, y) = f(y).
  \]
  The solution is obvious, since the differential equation tells us the function is just a function of $y$, and then the solution is obviously $\phi(x, y) = h(y)$. However, we can try to use the method of characteristics to practice our skills. Our vector field is given by
  \[
    \mathbf{V} =
    \begin{pmatrix}
      1\\0
    \end{pmatrix}
    =
    \begin{pmatrix}
      \frac{\d x}{\d s}\\
      \frac{\d y}{\d s}
    \end{pmatrix}.
  \]
  Hence, we get
  \[
    \frac{\d x}{\d s} = 1,\quad \frac{\d y}{\d s} = 0.
  \]
  So we have
  \[
    x = s + c,\quad y = d.
  \]
  Our initial curve is given by $y = t, x = 0$. Since we want this to be the curve $s = 0$, we find our integral curves to be
  \[
    x = s,\quad y = t.
  \]
  Now for each fixed $t$, we have to solve
  \[
    \left.\frac{\partial \phi}{\partial s}\right|_t = 0,\quad \phi(s, t) = h(t) = f(t).
  \]
  So we know that
  \[
    \phi(x, y) = f(y).
  \]
\end{eg}

\begin{eg}[First-order PDE, exponential coefficient]
  Consider the equation
  \[
    e^x \frac{\partial \phi}{\partial x} + \frac{\partial \phi}{\partial y} = 0.
  \]
  with the boundary condition
  \[
    \phi(x, 0) = \cosh x.
  \]
  We find that the vector field is
  \[
    \mathbf{V} =
    \begin{pmatrix}
      e^x\\
      1
    \end{pmatrix}
  \]
  This gives
  \[
    \frac{\d x}{\d s} = e^x,\quad \frac{\d y}{\d s} = 1.
  \]
  Thus we have
  \[
    e^{-x} = -s + c,\quad y = s + d.
  \]
  We pick our curve as $x = t, y = 0$. So our relation becomes
  \[
    e^{-x} = -s + e^{-t},\quad y = s.
  \]
  We thus have
  \[
    \left.\frac{\partial \phi}{\partial s}\right|_t = 0,\quad \phi(s, t) = \phi(0, t) = \cosh(t) = \cosh[-\ln (y + e^{-x})]
  \]
  So done.
\end{eg}

\begin{eg}[First-order PDE, inhomogeneous]
  Let $\phi: \R^2 \to \R$ solve the inhomogeneous partial differential equation
  \[
    \partial_x \phi + 2 \partial_y \phi = y e^x
  \]
  with $\phi(x, x) = \sin x$.

  We can still use the method of characteristics. We have
  \[
    \mathbf{u} =
    \begin{pmatrix}
      1\\2
    \end{pmatrix}.
  \]
  So the characteristic curves obey
  \[
    \frac{\d x}{\d s} = 1, \quad \frac{\d y}{\d s} = 2.
  \]
  This gives
  \[
    x = s + t,\quad y = 2s + t
  \]
  so that $x = y = t$ at $s = 0$. We can invert this to obtain the relations
  \[
    s = y - x, \quad t = 2x - y.
  \]
  The partial differential equation now becomes
  \[
    \left.\frac{\d \phi}{\d s}\right|_t = \mathbf{u}\cdot \nabla \phi = y e^x = (2s + t)e^{s + t}
  \]
  Note that this is just an ordinary differential equation in $s$ because $t$ is held constant. We have the boundary conditions
  \[
    \phi(s = 0, t) = \sin t.
  \]
  So we get
  \[
    \phi(x(s, t), y(s, t)) = (2 - t)e^t(1 - e^s) + \sin t + 2s e^{s + t}.
  \]
  Putting it in terms of $x$ and $y$, we have
  \[
    \phi(x, y) = (2 - 2x + y) e^{2x - y} + \sin (2x - y) + (y - 2) e^x.
  \]
\end{eg}
We see that our Cauchy data $\phi(x, x) = \sin x$ should be specified on a curve $B$ that intersects each characteristic curve exactly once.

If we tried to use a characteristic curve $B$ that intersects the characteristics multiple times, if we want a solution \emph{at all}, we cannot specify data freely along $B$. its values at the points of intersection have to compatible with the partial differential equation.

For example, if we have a homogeneous equation, we saw the solution will be constant along the same characteristic. Hence if our Cauchy data requires the solution to take different values on the same characteristic, we will have no solution.

Moreover, even if it is compatible, if $B$ does not intersect all characteristics, we will not get a unique solution. So the solution is not fixed along such characteristics.

On the other hand, if $B$ intersects each characteristic curve transversely, the problem is well-posed. So there exists a unique solution, at least in the neighbourhood of $B$. Notice that data is never transferred between characteristic curves.

\subsection{Characteristics for 2nd order partial differential equations}
Whether the method of characteristics works for a 2nd order partial differential equation, or even higher order ones, depends on the ``type'' of the differential equation.

To classify these differential equations, we need to define

\begin{defi}[Symbol and principal part]
  Let $\mathcal{L}$ be the general $2$nd order differential operator on $\R^n$. We can write it as
  \[
    \mathcal{L} = \sum_{i, j = 1}^n a^{ij}(x) \frac{\partial^2}{\partial x^i \partial x^j} + \sum_{i = 1}^n b^i(x) \frac{\partial}{\partial x^i} + c(X),
  \]
  where $a^{ij}(x), b^i(x), c(x) \in \R$ and $a^{ij} = a^{ji}$ (wlog).

  We define the \emph{symbol} $\sigma(\mathbf{k}, x)$ of $\mathcal{L}$ to be
  \[
    \sigma(\mathbf{k}, x) = \sum_{i, j = 1}^n a^{ij} (x) k_i k_j + \sum_{i = 1}^n b^i(x) k_i + c(x).
  \]
  So we just replace the derivatives by the variable $k$.

  The \emph{principal part} of the symbol is the leading term
  \[
    \sigma^p(\mathbf{k}, x) = \sum_{i, j = 1}^n a^{ij} (x) k_i k_j.
  \]
\end{defi}

\begin{eg}[Symbol of Laplacian and heat operator]
  If $\mathcal{L} = \nabla^2$, then
  \[
    \sigma(\mathbf{k}, x) = \sigma^p(\mathbf{k}, x) = \sum_{i = 1}^n (k_i)^2.
  \]
  If $\mathcal{L}$ is the heat operator (where we think of the last coordinate $x^n$ as the time, and others as space), then the operator is given by
  \[
    \mathcal{L} = \frac{\partial}{\partial x^n} - \sum_{i = 1}^{n - 1}\frac{\partial^2}{\partial x^{i2}}.
  \]
  The symbol is then
  \[
    \sigma(\mathbf{k}, x) = k_n - \sum_{i = 1}^{n - 1} (k_i)^2,
  \]
  and the principal part is
  \[
    \sigma^p (\mathbf{k}, x) = -\sum_{i = 1}^{n - 1}(k_i)^2.
  \]
\end{eg}
Note that the symbol is closely related to the Fourier transform of the differential operator, since both turn differentiation into multiplication. Indeed, they are equal if the coefficients are constant. However, we define this symbol for arbitrary differential operators with non-constant coefficients.

In general, for each $x$, we can write
\[
  \sigma^p (\mathbf{k}, x) = \mathbf{k}^T A(x)\mathbf{k},
\]
where $A(x)$ has elements $a^{ij}(x)$. Recall that a real symmetric matrix (such as $A$) has all real eigenvalues. So we define the following:

\begin{defi}[Elliptic, hyperbolic, ultra-hyperbolic and parabolic differential operators]
  Let $\mathcal{L}$ be a differential operator. We say $\mathcal{L}$ is
  \begin{itemize}
    \item \emph{elliptic at $x$} if all eigenvalues of $A(x)$ have the same sign. Equivalently, if $\sigma^p(\ph, x)$ is a definite quadratic form;
    \item \emph{hyperbolic at $x$} if all but one eigenvalues of $A(x)$ have the same sign;
    \item \emph{ultra-hyperbolic at $x$} if $A(x)$ has more than one eigenvalues of each sign;
    \item \emph{parabolic at $x$} if $A(x)$ has a zero eigenvalue, i.e.\ $\sigma^p(\ph, x)$ is degenerate.
  \end{itemize}
  We say $\mathcal{L}$ is elliptic if $\mathcal{L}$ is elliptic at all $x$, and similarly for the other terms.
\end{defi}
These names are supposed to remind us of conic sections, and indeed if we think of $\mathbf{k}^T A\mathbf{k}$ as an equation in $\mathbf{k}$, then we get a conic section.

\begin{eg}[PDE classification in two dimensions]
  Let
  \[
    \mathcal{L} = a(x, y) \frac{\partial^2}{\partial x^2} + 2b(x, y) \frac{\partial^2}{\partial x \partial y} + c(x, y) \frac{\partial^2}{\partial y^2} + d(x, y) \frac{\partial}{\partial x} + e(x, y) \frac{\partial}{\partial y} + f(x, y).
  \]
  Then the principal part of the symbol is
  \[
    \sigma^p(\mathbf{k}, x) =
    \begin{pmatrix}
      k_x & k_y
    \end{pmatrix}
    \begin{pmatrix}
      a(x, y) & b(x, y)\\
      b(x, y) & c(x, y)
    \end{pmatrix}
    \begin{pmatrix}
      k_x \\ k_y
    \end{pmatrix}
  \]
  Then $\mathcal{L}$ is elliptic at $x$ if $b^2 - ac < 0$; hyperbolic if $b^2 - ac > 0$; and parabolic if $b^2 - ac = 0$.

  Note that since we only have two dimensions and hence only two eigenvalues, we cannot possibly have an ultra-hyperbolic equation.
\end{eg}

\subsubsection*{Characteristic surfaces}
\begin{defi}[Characteristic surface]
  Given a differential operator $\mathcal{L}$, let
  \[
    f(x^1, x^2, \cdots, x^n) = 0
  \]
  define a surface $C \subseteq \R^n$. We say $C$ is \emph{characteristic} if
  \[
    \sum_{i, j = 1}^n a^{ij}(x) \frac{\partial f}{\partial x^i} \frac{\partial f}{\partial x^j} = (\nabla f)^T A (\nabla f) = \sigma^p(\nabla f, x) = 0.
  \]
  In the case where we only have two dimensions, a characteristic surface is just a curve.
\end{defi}
We see that the characteristic equation restricts what values $\nabla f$ can take. Recall that $\nabla f$ is the normal to the surface $C$. So in general, at any point, we can find what the normal of the surface should be, and stitch these together to form the full characteristic surfaces.

For an elliptic operator, all the eigenvalues of $A$ have the same sign. So there are no non-trivial real solutions to this equation $(\nabla f)^T A (\nabla f) = 0$. Consequently, elliptic operators have no real characteristics. So the method of characteristics would not be of any use when studying, say, Laplace's equation, at least if we want to stay in the realm of real numbers.

If $\mathcal{L}$ is parabolic, we for simplicity assume that $A$ has exactly one zero eigenvector, say $\mathbf{n}$, and the other eigenvalues have the same sign. This is the case when, say, there are just two dimensions. So we have $A \mathbf{n} = \mathbf{n}^T A = \mathbf{0}$. We normalize $\mathbf{n}$ such that $\mathbf{n}\cdot \mathbf{n} = 1$.

For any $\nabla f$, we can always decompose it as
\[
  \nabla f = \mathbf{n} (\mathbf{n} \cdot \nabla f) + [\nabla f - \mathbf{n}\cdot (\mathbf{n}\cdot \nabla f)].
\]
This is a relation that is trivially true, since we just add and subtract the same thing. Note, however, that the first term points along $\mathbf{n}$, while the latter term is orthogonal to $\mathbf{n}$. To save some writing, we adopt the notation
\[
  \nabla_\perp f = \nabla f - \mathbf{n}(\mathbf{n} \cdot \nabla \mathbf{f}).
\]
So we have
\[
  \nabla f = \mathbf{n}(\mathbf{n}\cdot \nabla f) + \nabla f_\perp.
\]
Then we can compute
\begin{align*}
  (\nabla f)^T A(\nabla f) &= [\mathbf{n}(\mathbf{n}\cdot \nabla f) + \nabla_\perp f]^T A[\mathbf{n}(\mathbf{n}\cdot \nabla f) + \nabla_\perp f]\\
  &= (\nabla_\perp f)^T A(\nabla_\perp f).
\end{align*}
Then by assumption, $(\nabla_\perp f)^T A(\nabla_\perp f)$ is definite. So just as in the elliptic case, there are no non-trivial solutions. Hence, if $f$ defines a characteristic surface, then $\nabla_\perp f = 0$. In other words, $f$ is parallel to $\mathbf{n}$. So at any point, the normal to a characteristic surface must be $\mathbf{n}$, and there is only one possible characteristic.

If $\mathcal{L}$ is hyperbolic, we assume all but one eigenvalues are positive, and let $-\lambda$ be the unique negative eigenvalue. We let $\mathbf{n}$ be the corresponding unit eigenvector, where we normalize it such that $\mathbf{n}\cdot \mathbf{n} = 1$. We say $f$ is characteristic if
\begin{align*}
  0 &= (\nabla f)^T A(\nabla f) \\
  &= [\mathbf{n}(\mathbf{n}\cdot \nabla f) + \nabla_\perp f]^T A[\mathbf{n}(\mathbf{n}\cdot \nabla f) + \nabla_\perp f]\\
  &= -\lambda (\mathbf{n}\cdot \nabla f)^2 + (\nabla_\perp f)^T A(\nabla_\perp f).
\end{align*}
Consequently, for this to be a characteristic, we need
\[
  \mathbf{n}\cdot \nabla f = \pm \sqrt{\frac{(\nabla_\perp f)^T A(\nabla_\perp f)}{\lambda}}.
\]
So there are two choices for $\mathbf{n}\cdot \nabla f$, given any $\nabla_\perp f$. So hyperbolic equations have two characteristic surfaces through any point. % why?

This is not too helpful in general. However, in the case where we have two dimensions, we can find the characteristic curves explicitly. Suppose our curve is given by $f(x, y) = 0$. We can write $y = y(x)$. Then since $f$ is constant along each characteristic, by the chain rule, we know
\[
  0 = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} \frac{\d y}{\d x}.
\]
Hence, we can compute
\[
  \frac{\d y}{\d x} = -\frac{-b \pm \sqrt{b^2 - ac}}{a}.
\]
We now see explicitly how the type of the differential equation influences the number of characteristics --- if $b^2 - ac > 0$, then we obtain two distinct differential equations and obtain two solutions; if $b^2 - ac = 0$, then we only have one equation; if $b^2 - ac < 0$, then there are no real characteristics.

\begin{eg}[Variable type PDE]
  Consider
  \[
    \partial_y^2 \phi - xy \partial_x^2 \phi = 0
  \]
  on $\R^2$. Then $a = -xy, b = 0, c = 1$. So $b^2 - ac = xy$. So the type is elliptic if $xy < 0$, hyperbolic if $xy > 0$, and parabolic if $xy = 0$.

  In the regions where it is hyperbolic, we find
  \[
    \frac{-b \pm \sqrt{b^2 - ac}}{a} = \pm \frac{1}{\sqrt{xy}}.
  \]
  Hence the two characteristics are given by
  \[
    \frac{\d y}{\d x} = \pm \frac{1}{\sqrt{xy}}.
  \]
  This has a solution
  \[
    \frac{1}{3}y^{3/2}\pm x^{1/2} = c.
  \]
  We now let
  \begin{align*}
    u &= \frac{1}{3}y^{3/2} + x^{1/2},\\
    v &= \frac{1}{3}y^{3/2} - x^{1/2}.
  \end{align*}
  Then the equation becomes
  \[
    \frac{\partial^2 \phi}{\partial u\partial v} + \text{lower order terms} = 0.
  \]
\end{eg}

\begin{eg}[Wave equation, d'Alembert]
  Consider the wave equation
  \[
    \frac{\partial^2 \phi}{\partial t^2} - c^2 \frac{\partial^2 \phi}{\partial x^2} = 0
  \]
  on $\R^{1, 1}$. Then the equation is hyperbolic everywhere, and the characteristic curves are $x \pm ct = $ const. Let's look for a solution to the wave equation that obeys
  \[
    \phi(x, 0) = f(x),\quad \partial_t \phi(x, 0) = g(x).
  \]
  Now put $u = x - ct, v = x + ct$. Then the wave equation becomes
  \[
    \frac{\partial^2}{\partial u \partial v} = 0.
  \]
  So the general solution to this is
  \[
    \phi(x, t) = G(u) + H(v) = G(x - ct) + H(x + ct).
  \]
  The initial conditions now fix these functions
  \[
    f(x) = G(x) + H(x),\quad g(x) = -c G'(x) + c H'(x).
  \]
  Solving these, we find
  \[
    \phi(x, t) = \frac{1}{2}[f(x - ct) + f(x + ct)] + \frac{1}{2c}\int_{x - ct}^{x + ct} g(y)\;\d y.
  \]
  This is d'Alembert's solution to the $1 + 1$ dimensional wave equation.
\end{eg}
Note that the value of $\phi$ at any point $(x, t)$ is \emph{completely} determined by $f, g$ in the interval $[x - ct, x + ct]$. This is known as the \emph{(past) domain of dependence} of the solution at $(x, t)$, written $D^-(x, t)$. Similarly, at any time $t$, the initial data at $(x_0, 0)$ only affects the solution within the region $x_0 - ct \leq x \leq x_0 + ct$. This is the \emph{range of influence} of data at $x_0$, written $D^+(x_0)$.
\begin{center}
  \begin{tikzpicture}
    \node at (-1.5, 2) [circ] {};
    \node at (-1.5, 2) [above] {$p$};
    \fill [mgreen, opacity=0.5] (-3.5, 0) -- (-1.5, 2) -- (0.5, 0) -- cycle;
    \draw (-3.5, 0) -- (-1.5, 2) -- (0.5, 0);

    \node at (-1.5, 0.667) {$D^-(p)$};

    \draw [mred] (-4, 0) -- (4, 0) node [right] {$B$};
    \draw (1.5, 0) -- (-0.5, 2);
    \draw (2, 0) -- (4, 2);

    \fill [morange, opacity=0.5] (1.5, 0) -- (-0.5, 2) -- (4, 2) -- (2, 0) -- cycle;
    \draw [mblue, thick] (1.5, 0) -- (2, 0) node [below, pos=0.5] {$S$};
    \node at (1.75, 1) {$D^+(S)$};
  \end{tikzpicture}
\end{center}
We see that disturbances in the wave equation propagate with speed $c$.

\subsection{Green's functions for PDEs on \texorpdfstring{$\R^n$}{Rn}}
\subsubsection*{Green's functions for the heat equation}
Suppose $\phi: \R^n \times [0, \infty) \to \R$ solves the heat equation
\[
  \partial_t \phi = D \nabla^2 \phi,
\]
where $D$ is the diffusion constant, subject to
\[
  \phi|_{\R^n \times \{0\}} = f.
\]
To solve this, we take the Fourier transform of the heat equation in the spatial variables. For simplicity of notation, we take $n = 1$. Writing $\mathcal{F}[\phi(x, t)] = \tilde{\phi}(k, t)$, we get
\[
  \partial_t \tilde{\phi}(k ,t) = -D k^2 \tilde{\phi}(k, t)
\]
with the boundary conditions
\[
  \tilde{\phi}(k, 0) = \tilde{f}(k).
\]
Note that this is just an ordinary differential equation in $t$, and $k$ is a fixed parameter for each $k$. So we have
\[
  \tilde{\phi}(k, t) = \tilde{f}(k) e^{-Dk^2 t}.
\]
We now take the inverse Fourier transform and get
\[
  \phi(x, t) = \frac{1}{2\pi} \int_\R e^{ikx}\left[\tilde{f}(k) e^{-D k^2 t} \right]\;\d k
\]
This is the inverse Fourier transform of a product. This thus gives the convolution
\[
  \phi(x, t) = f * \mathcal{F}^{-1}[e^{-Dk^2 t}].
\]
So we are done if we can find the inverse Fourier transform of the Gaussian. This is an easy exercise on example sheet 3, where we find
\[
  \mathcal{F}[e^{-a^2 x^2}] = \frac{\sqrt{\pi}}{a} e^{-k^2/4a^2}.
\]
So setting
\[
  a^2 = \frac{1}{4Dt},
\]
So we get
\[
  \mathcal{F}^{-1}[e^{-Dk^2 t}] = \frac{1}{\sqrt{4\pi Dt}} \exp\left(-\frac{x^2}{4Dt}\right)
\]
We shall call this $S_1(x, t)$, where the subscript $1$ tells us we are in $1 + 1$ dimensions. This is known as the \emph{fundamental solution} of the heat equation. We then get
\[
  \phi(x, t) = \int_{-\infty}^\infty f(y) S_1(x - y, t) \;\d y
\]
\begin{eg}[Heat equation, delta initial data]
  Suppose our initial data is
  \[
    f(x) = \phi_0\delta(x).
  \]
  So we start with a really cold room, with a huge spike in temperature at the middle of the room. Then we get
  \[
    \phi(x, t) = \frac{\phi_0}{\sqrt{4\pi Dt}} \exp\left(-\frac{x^2}{4Dt}\right).
  \]
  What this shows, as we've seen before, is that if we start with a delta function, then as time evolves, we get a Gaussian that gets shorter and broader.

  Now note that if we start with a delta function, at $t = 0$, everywhere outside the origin is non-zero. However, after any infinitely small time $t$, $\phi$ becomes non-zero everywhere, instantly. Unlike the wave equation, information travels instantly to all of space, i.e.\ heat propagates arbitrarily fast according to this equation (of course in reality, it doesn't). This fundamentally, is because the heat equation is parabolic, and only has one characteristic.
\end{eg}

Now suppose instead $\phi$ satisfies the inhomogeneous, forced, heat equation
\[
  \partial_t \phi - D \nabla^2 \phi = F(x, t),
\]
but with homogeneous initial conditions $\phi|_{t = 0} = 0$. Physically, this represents having an external heat source somewhere, starting at zero.

Note that if we can solve this, then we have completely solved the heat equation. If we have an inhomogeneous equation \emph{and} an inhomogeneous initial condition, then we can solve this forced problem with homogeneous boundary conditions to get $\phi_F$; and solve the unforced equation with homogeneous equation to get $\phi_H$. Then the sum $\phi = \phi_F + \phi_H$ solves the full equation.

As before, we take the Fourier transform of the forced equation with respect to the spacial variables. As before, we will just do it in the cases with one spacial dimension. We find
\[
  \partial_t \tilde{\phi}(k, t) + Dk^2 \tilde{\phi}(k, t) = \tilde{F}(k, t),
\]
with the initial condition
\[
  \tilde{\phi}(k, 0) = 0.
\]
As before, we have reduced this to a first-order ordinary differential equation in $t$. Using an integrating factor, we can rewrite this as
\[
  \pd{t} [e^{Dk^2 t} \tilde{\phi}(k, t)] = e^{Dk^2 t} \tilde{F}(k, t).
\]
The solution is them
\[
  \tilde{\phi}(k, t) = e^{-Dk^2 t} \int_0^t e^{Dk^2 u} \tilde{F}(k, u)\;\d u.
\]
We define the Green's function $G(x, t; y, \tau))$ to be the solution to
\[
  [\partial_t - D \nabla_x^2] G(x, t; y, \tau) = \delta(x - y) \delta(t - \tau).
\]
So the Fourier transform with respect to $x$ gives
\[
  \tilde{G}(k, t, y, \tau) = e^{-Dk^2 t} \int_0^t e^{D k^2 u}e^{iky}\delta(t - \tau) \;\d u,
\]
where $e^{iky}$ is just the Fourier transform of $\delta(x - y)$. This is equal to
\[
  \tilde{G}(k, t; y, \tau) =
  \begin{cases}
    0 & t < \tau\\
    e^{-iky} e^{-Dk^2 (t - \tau)} & t > \tau
  \end{cases}
  = \Theta(t - \tau) e^{-iky} e^{-Dk^2 (t - \tau)}.
\]
Reverting the Fourier transform, we get
\[
  G(x, t; y, \tau) = \frac{\Theta(t - \tau)}{2\pi} \int_\R e^{ik(x - y)} e^{-Dk^2(t - \tau)}\;\d x
\]
This integral is just the inverse Fourier transform of the Gaussian with a phase shift. So we end up with
\[
  G(x, t; y, \tau) = \frac{\Theta(t - \tau)}{\sqrt{4\pi D(t- \tau)}} \exp\left(-\frac{(x - y)^2}{4D (t - \tau)}\right) = \Theta(t - \tau) S_1 (x - y; t - \tau).
\]
The solution we seek is then
\[
  \phi(x, t) = \int_0^t \int_\R F(y, \tau) G(x, t; y, \tau)\;\d y\;\d \tau.
\]
It is interesting that the solution to the forced equation involves the same function $S_1(x, t)$ as the homogeneous equation with inhomogeneous boundary conditions.

In general, $S_n(\mathbf{x}, t)$ solves
\[
  \frac{\partial S_n}{\partial t} - D \nabla^2 S_n = 0
\]
with boundary conditions $S_n(\mathbf{x}, 0) = \delta^{(n)}(\mathbf{x} - \mathbf{y})$, and we can find
\[
  S_n(\mathbf{x}, t) = \frac{1}{(4\pi D t)^{n/2}} \exp\left(-\frac{|\mathbf{x} - \mathbf{y}|^2}{4 Dt}\right).
\]
Then in general, given an initial condition $\phi|_{t = 0}= f(\mathbf{x})$, the solution is
\[
  \phi(\mathbf{x}, t) = \int f(\mathbf{y}) S(\mathbf{x} - \mathbf{y}, t)\;\d^n y.
\]
Similarly, $G_n(\mathbf{x}, t; \mathbf{y}, t)$ solves
\[
  \frac{\partial G_n}{\partial t} - D \nabla^2 G_n = \delta(t - \tau) \delta^{(n)} (\mathbf{x} - \mathbf{y}),
\]
with the boundary condition $G_n(\mathbf{x}, 0; \mathbf{y}, \tau) = 0$. The solution is
\[
  G(\mathbf{x}, t; \mathbf{y}, \tau) = \Theta(t - \tau) S_n (\mathbf{x} - \mathbf{y}, t - \tau).
\]
Given our Green's function, the general solution to the forced heat equation
\[
  \frac{\partial \phi}{\partial t} - D \nabla^2 \phi = F(\mathbf{x}, t),\quad \phi(\mathbf{x}, 0) = 0
\]
is just
\begin{align*}
  \phi(\mathbf{x}, t) &= \int_0^\infty \int_{\R^n} F(\mathbf{y}, \tau) G(\mathbf{x}, t; \mathbf{y}, \tau) \;\d^n y \;\d \tau \\
  &= \int_0^t \int_{\R^n} F(\mathbf{y}, \tau) G(\mathbf{x}, t; \mathbf{y}, \tau) \;\d^n y \;\d \tau.
\end{align*}
Duhamel noticed that we can write this as
\[
  \phi(\mathbf{x}, t) = \int_0^t \phi_F(\mathbf{x}, t; \tau)\;\d \tau,
\]
where
\[
  \phi_F = \int_\R F(\mathbf{y}, t) S_n(\mathbf{x} - \mathbf{y}, t - \tau)\;\d^n y
\]
solves the homogeneous heat equation with $\phi_F |_{t = \tau} = F(\mathbf{x}, \tau)$.

Hence in general, we can think of the forcing term as providing a whole sequence of ``initial conditions'' for all $t > 0$. We then integrate over the times at which these conditions were imposed to find the full solution to the forced problem. This interpretation is called \emph{Duhamel's principle}.
\subsubsection*{Green's functions for the wave equation}
Suppose $\phi: \R^n \times [0, \infty) \to \C$ solves the inhomogeneous wave equation
\[
  \frac{\partial^2 \phi}{\partial t^2} - c^2 \nabla^2 \phi = F(\mathbf{x}, t)
\]
with
\[
  \phi(\mathbf{x}, 0) = \pd{t} \phi(\mathbf{x}, 0) = 0.
\]
We look for a Green's function $G_n(\mathbf{x}, t; \mathbf{y}, \tau)$ that solves
\[
  \frac{\partial G_n}{\partial t} - c^2 \nabla^2 G_n = \delta(t - \tau) \delta^{(n)}(\mathbf{x} - \mathbf{y}) \tag{$*$}
\]
with the same initial conditions
\[
  G_n(\mathbf{x}, 0, \mathbf{y}, \tau) = \pd{t} G_n(\mathbf{x}, 0, \mathbf{y}, \tau) = 0.
\]
Just as before, we take the Fourier transform of this equation with respect the spacial variables $\mathbf{x}$. We get
\[
  \pd{t} \tilde{G}_n + c^2 |\mathbf{k}|^2 \tilde{G}_n = \delta(t - \tau) e^{-i\mathbf{k} \cdot \mathbf{y}}.
\]
where $\tilde{G}_n = \tilde{G}_n(\mathbf{k}, t, \mathbf{y}, \tau)$.

This is just an ordinary differential equation from the point of view of $t$, and is of the same type of initial value problem that we studied earlier, and the solution is
\[
  \tilde{G}_n (\mathbf{k}, t, \mathbf{y}, \tau) = \Theta(t - \tau) e^{-i\mathbf{k} \cdot \mathbf{y}} \frac{\sin |\mathbf{k}| c (t - \tau)}{|\mathbf{k}| c}.
\]
To recover the Green's function itself, we have to compute the inverse Fourier transform, and find
\[
  G_n(\mathbf{x}, t; \mathbf{y}, \tau) = \frac{1}{(2\pi)^n} \int_{\R^n} e^{i\mathbf{k}\cdot \mathbf{x}} \Theta(t - \tau) e^{-i\mathbf{k}\cdot \mathbf{y}}\frac{\sin |\mathbf{k}| c(t - \tau)}{|\mathbf{k}| c} \;\d^n k.
\]
Unlike the case of the heat equation, the form of the answer we get here does depend on the number of spatial dimensions $n$. For definiteness, we look at the case where $n = 3$, since our world (probably) has three dimensions. Then our Green's function is
\[
  G(\mathbf{x}, t; \mathbf{y}, \tau) = \frac{\Theta(t - \tau)}{(2\pi)^3 c} \int_{\R^3} e^{i\mathbf{k}\cdot (\mathbf{x} - \mathbf{y})} \frac{\sin |\mathbf{k}|c(t - \tau)}{|\mathbf{k}|} \;\d^3 k.
\]
We use spherical polar coordinates with the $z$-axis in $k$-space aligned along the direction of $\mathbf{x} - \mathbf{y}$. Hence $\mathbf{k}\cdot (\mathbf{x} - \mathbf{y}) = kr \cos \theta$, where $r = |\mathbf{x} - \mathbf{y}|$ and $k = |\mathbf{k}|$.

Note that nothing in our integral depends on $\varphi$, so we can pull out a factor of $2\pi$, and get
\begin{align*}
  G(\mathbf{x}, t; \mathbf{y}, \tau) &= \frac{\Theta(t - \tau)}{(2 \pi)^2 c} \int_0^\infty \int_0^\pi e^{ik r\cos \theta} \frac{\sin kc(t - \tau)}{k} k^2 \sin \theta \;\d \theta \;\d k\\
  \intertext{The next integral to do is the $\theta$ integral, which is straightforward since it is an exact differential. Setting $\alpha = c(t - \tau)$, we get}
  &= \frac{\Theta(t - \tau)}{(2\pi)^2 c} \int_0^\infty \left[\frac{e^{ikr} - e^{-ikr}}{ikr}\right] \frac{\sin kc(t - \tau)}{k} k^2 \;\d k\\
  &= \frac{\Theta(t - \tau)}{(2\pi)^2 icr} \left[\int_0^\infty e^{ikr} \sin k\alpha\;\d k - \int_0^\infty e^{-ikr} \sin k\alpha\;\d k\right]\\
  &= \frac{\Theta(t - \tau)}{2\pi i c r} \left[\frac{1}{2\pi} \int_{-\infty}^\infty e^{ikr} \sin k\alpha \;\d k\right]\\
  &= \frac{\Theta(t - \tau)}{ 2\pi i c r} \mathcal{F}^{-1}[\sin k \alpha],
\end{align*}
Now recall $\mathcal{F}[\delta(x - \alpha)] = e^{-ik\alpha}$. So
\[
  \mathcal{F}^{-1}[\sin k\alpha] = \mathcal{F}^{-1} \left[\frac{e^{ik\alpha} - e^{-ik\alpha}}{2 i}\right] = \frac{1}{2i} [\delta(x + \alpha) - \delta(x - \alpha)].
\]
Hence our Green's function is
\[
  G(\mathbf{x}, t; \mathbf{y}, \tau) = -\frac{\Theta(t - \tau)}{4\pi c |\mathbf{x} - \mathbf{y}|}\Big[\delta\big(|\mathbf{x} - \mathbf{y}| + c(t - \tau)\big) - \delta\big(|\mathbf{x} - \mathbf{y}| - c(t - \tau)\big)\Big].
\]
Now we look at our delta functions. The step function is non-zero only if $t > \tau$. Hence $|\mathbf{x} - \mathbf{y}| + c(t - \tau)$ is always positive. So $\delta(|\mathbf{x} - \mathbf{y}| + c(t - \tau))$ does not contribute. On the other hand, $\delta(|\mathbf{x} - \mathbf{y}| - c(t - \tau))$ is non-zero only if $t > \tau$. So $\Theta(t - \tau)$ is always positive in this region. So we can write our Green's function as
\[
  G(\mathbf{x}, t; \mathbf{y}, \tau) = \frac{1}{4\pi c} \frac{1}{|\mathbf{x} - \mathbf{y}|} \delta(|\mathbf{x} - \mathbf{y}| - c(t - \tau)).
\]
As always, given our Green's function, the general solution to the forced equation
\[
  \frac{\partial^2 \phi}{\partial t^2} - c^2 \nabla^2 \phi = F(\mathbf{x}, t)
\]
is
\[
  \phi(\mathbf{x}, t) = \int_0^\infty \int_{\R^3} \frac{F(\mathbf{y}, \tau)}{4\pi c|\mathbf{x} - \mathbf{y}|} \delta(|\mathbf{x} - \mathbf{y}| - c(t - \tau)) \;\d^3 y\;\d \tau.
\]
We can use the delta function to do one of the integrals. It is up to us which integral we do, but we pick the time integral to do. Then we get
\[
  \phi(\mathbf{x}, t) = \frac{1}{4\pi c^2} \int_{\R^3} \frac{F(\mathbf{y}, t_{\mathrm{ret}})}{|\mathbf{x} - \mathbf{y}|} \;\d^3 y,
\]
where
\[
  t_{\mathrm{ret}} = t - \frac{|\mathbf{x} - \mathbf{y}|}{c}.
\]
This shows that the effect of the forcing term at some point $\mathbf{y} \in \R^3$ affects the solution $\phi$ at some other point $\mathbf{x}$ not instantaneously, but only after time $|\mathbf{x} - \mathbf{y}|/c$ has elapsed. This is just as we saw for characteristics. This, again, tells us that information travels at speed $c$.

Also, we see the effect of the forcing term gets weaker and weaker as we move further away from the source. This dispersion depends on the number of dimensions of the space. As we spread out in a three-dimensional space, the ``energy'' from the forcing term has to be distributed over a larger sphere, and the effect diminishes. On the contrary, in one-dimensional space, there is no spreading out to do, and we don't have this reduction. In fact, in one dimensions, we get
\[
  \phi(x, t) = \int_0^t \int_\R F(y, \tau) \frac{\Theta(c(t - \tau) - |x - y|)}{2c} \;\d y \;\d \tau.
\]
We see there is now no suppression factor at the bottom, as expected.

\subsection{Poisson's equation}
Let $\phi: \R^3 \to \R$ satisfy the Poisson's equation
\[
  \nabla^2 \phi = -F,
\]
where $F(\mathbf{x})$ is a forcing term.

The fundamental solution to this equation is defined to be $G_3(\mathbf{x}, \mathbf{y})$, where
\[
  \nabla^2 G_3(\mathbf{x}, \mathbf{y}) = \delta^{(3)}(\mathbf{x} - \mathbf{y}).
\]
By rotational symmetry, $G_3(\mathbf{x}, \mathbf{y}) = G_3(|\mathbf{x} - \mathbf{y}|)$. Integrating over a ball
\[
  B_r = \{|\mathbf{x} - \mathbf{y}| \leq r, \mathbf{x} \in \R^3\},
\]
we have
\begin{align*}
  1 &= \int_{B_r} \nabla^2 G_3\;\d V \\
  &= \int_{\partial B_r} \mathbf{n}\cdot \nabla G_3 \;\d S\\
  &= \int_{S^2} \frac{\d G_3}{\d r} r^2 \sin \theta \;\d \theta \;\d \phi\\
  &= 4\pi r^2 \frac{\d G_3}{\;\d r}.
\end{align*}
So we know
\[
  \frac{\d G_3}{\d r} = \frac{1}{4\pi r^2},
\]
and hence
\[
  G_3(\mathbf{x}, \mathbf{y}) = -\frac{1}{4\pi |\mathbf{x} - \mathbf{y}|} + c.
\]
We often set $c = 0$ such that
\[
  \lim_{|\mathbf{x}| \to \infty} G_3 = 0.
\]
\subsubsection*{Green's identities}
To make use of this fundamental solution in solving Poisson's equation, we first obtain some useful identities.

Suppose $\phi, \psi: \R^3 \to \R$ are both smooth everywhere in some region $\Omega \subseteq \R^3$ with boundary $\partial \Omega$. Then
\[
  \int_{\partial \Omega} \phi\mathbf{n} \cdot \nabla \psi\;\d S = \int_\Omega \nabla\cdot (\phi \nabla \psi) \;\d V = \int_\Omega \phi \nabla^2 \psi + (\nabla \phi)\cdot (\nabla \psi)\;\d V.
\]
So we get
\begin{prop}[Green's first identity]
\[
  \int_{\partial \Omega} \phi\mathbf{n} \cdot \nabla \psi\;\d S = \int_\Omega \phi \nabla^2 \psi + (\nabla \phi)\cdot (\nabla \psi)\;\d V.
\]
\end{prop}
Of course, this is just an easy consequence of the divergence theorem, but when Green first came up with this, divergence theorem hasn't existed yet.

Similarly, we obtain
\[
  \int_{\partial \Omega} \psi\mathbf{n} \cdot \nabla \phi\;\d S = \int_\Omega \psi \nabla^2 \phi + (\nabla \phi)\cdot (\nabla \psi)\;\d V.
\]
Subtracting these two equations gives
\begin{prop}[Green's second identity]
  \[
    \int_\Omega \phi \nabla^2 \psi - \psi \nabla^2 \phi \;\d V = \int_{\partial \Omega} \phi \mathbf{n}\cdot \nabla \psi - \psi \mathbf{n}\cdot \nabla \phi\;\d S.
  \]
\end{prop}
Why is this useful? On the left, we have things like $\nabla^2 \psi$ and $\nabla^2 \phi$. These are things we are given by Poisson's or Laplace's equation. On the right, we have things on the boundary, and these are often the boundary conditions we are given. So this can be rather helpful when we have to solve these equations.

\subsubsection*{Using the Green's function}
We wish to apply this result to the case $\psi = G_3(|\mathbf{x} - \mathbf{y}|)$. However, recall that when deriving Green's identity, we assumed $\phi$ and $\psi$ are smooth everywhere in our domain. However, our Green's function is singular at $\mathbf{x} = \mathbf{y}$, but we want to integrate over this region as well. So we need to do this carefully. Because of the singularity, we want to take
\[
  \Omega = B_r - B_\varepsilon = \{\mathbf{x}\in \R^3: \varepsilon \leq |\mathbf{x} - \mathbf{y}| \leq R\}.
\]
In other words, we remove a small region of radius $\varepsilon$ centered on $\mathbf{y}$ from the domain.

In this choice of $\Omega$, it is completely safe to use Green's identity, since our Green's function is certainly regular everywhere in this $\Omega$.
First note that since $\nabla^2 G_3 = 0$ everywhere except at $\mathbf{x} = \mathbf{y}$, we get
\[
  \int_\Omega \phi \nabla^2 G_3 - G_3 \nabla^2 \phi \;\d V = - \int_\Omega G_3 \nabla^2 \phi \;\d V
\]
Then Green's second identity gives
\begin{align*}
  - \int_\Omega G_3 \nabla^2 \phi \;\d V &= \int_{S_r^2} \phi(\mathbf{n}\cdot \nabla G_3) - G_3(\mathbf{n}\cdot \phi)\;\d S \\
  &\quad+ \int_{S_\varepsilon^2} \phi(\mathbf{n}\cdot \nabla G_3) - G_3(\mathbf{n}\cdot \phi)\;\d S
\end{align*}
Note that on the inner boundary, we have $\mathbf{n} = - \hat{\mathbf{r}}$. Also, at $S_\varepsilon^2$, we have
\[
  G_3|_{S_\varepsilon^2} = -\frac{1}{4\pi \varepsilon},\quad \left.\frac{\d G_3}{\d r}\right|_{S_\varepsilon^2} = \frac{1}{4\pi \varepsilon^2}.
\]
So the inner boundary terms are
\begin{align*}
  &\int_{S_\varepsilon^2} \Big(\phi(\mathbf{n}\cdot \nabla G_3) - G_3(\mathbf{n}\cdot \phi)\Big) \varepsilon^2 \sin \theta\;\d \theta\;\d \phi \\
  &= -\frac{\varepsilon^2}{4 \pi \varepsilon^2} \int_{S_\varepsilon^2} \phi \sin \theta \;\d \theta \;\d \phi + \frac{\varepsilon^2}{4 \pi \varepsilon} \int_{S_\varepsilon^2} (\mathbf{n}\cdot \nabla \phi) \sin \theta \;\d \theta \;\d \phi\\
  \intertext{Now the final integral is bounded by the assumption that $\phi$ is everywhere smooth. So as we take the limit $\varepsilon \to 0$, the final term vanishes. In the first term, the $\varepsilon$'s cancel. So we are left with}
  &= -\frac{1}{4\pi} \int_{S_\varepsilon^2} \phi \;\d \Omega\\
  &= -\bar{\phi} \\
  &\to -\phi (\mathbf{y})
\end{align*}
where $\bar{\phi}$ is the average value of $\phi$ on the sphere.

Now suppose $\nabla^2 \phi = -F$. Then this gives
\begin{prop}[Green's third identity]
  \[
    \phi (\mathbf{y}) = \int_{\partial \Omega} \phi(\mathbf{n}\cdot \nabla G_3) - G_3(\mathbf{n}\cdot \nabla \phi) \;\d S - \int_\Omega G_3 (\mathbf{x}, \mathbf{y}) F(\mathbf{x})\;\d^3 x.
  \]
\end{prop}
This expresses $\phi$ at any point $\mathbf{y}$ in $\Omega$ in terms of the fundamental solution $G_3$, the forcing term $F$ and boundary data. In particular, if the boundary values of $\phi$ and $\mathbf{n} \cdot \nabla \phi$ vanish as we take $r \to \infty$, then we have
\[
  \phi(\mathbf{y}) = - \int_{\R^3} G_3(\mathbf{x}, \mathbf{y}) F(\mathbf{x})\;\d^3 x.
\]
So the fundamental solution \emph{is} the Green's function for Poisson's equation on $\R^3$.

However, there is a puzzle. Suppose $F = 0$. So $\nabla^2 \phi = 0$. Then Green's identity says
\[
  \phi(\mathbf{y}) = \int_{S_r^2} \left(\phi \frac{\d G_3}{\d r} - G_3 \frac{\d \phi}{\d r}\right)\;\d S.
\]
But we know there is a unique solution to Laplace's equation on every bounded domain once we specify the boundary value $\phi|_{\partial \Omega}$, or a unique-up-to-constant solution if we specify the boundary value of $\mathbf{n}\cdot \nabla \phi|_{\partial \Omega}$.

However, to get $\phi(\mathbf{y})$ using Green's identity, we need to know $\phi$ and \emph{and} $\mathbf{n} \cdot \nabla \phi$ on the boundary. This is too much.

Green's third identity \emph{is} a valid relation obeyed by solutions to Poisson's equation, but it is not constructive. We cannot specify $\phi$ \emph{and} $\mathbf{n}\cdot \nabla \phi$ freely. What we would like is a formula of $\phi$ given, say, just the value of $\phi$ on the boundary.

\subsubsection*{Dirichlet Green's function}
To overcome this (in the Dirichlet case), we seek to modify $G_3$ via
\[
  G_3 \to G = G_3 + H(\mathbf{x}, \mathbf{y}),
\]
where $\nabla^2 H = 0$ everywhere in $\Omega$, $H$ is regular throughout $\Omega$, and $G|_{\partial \Omega} = 0$. In other words, we find some $H$ that does not affect the relations on $G$ when acted on by $\nabla^2$, but now our $G$ will have boundary value $0$. We will find this $H$ later, but given such an $H$, we replace $G_3$ with $G - H$ in Green's third identity, and see that all the $H$ terms fall out, i.e.\ $G$ also satisfies Green's third identity. So
\begin{align*}
  \phi(\mathbf{y}) &= \int_{\partial\Omega} \left[\phi \mathbf{n}\cdot \nabla G - G \mathbf{n}\cdot \nabla \phi\right]\;\d S - \int FG\;\d V\\
  &= \int_{\partial\Omega} \phi \mathbf{n}\cdot \nabla G\;\d S - \int FG\;\d V.
\end{align*}
So as long as we find $H$, we can express the value of $\phi(\mathbf{y})$ in terms of the values of $\phi$ on the boundary. Similarly, if we are given a Neumann condition, i.e.\ the value of $\mathbf{n} \cdot \nabla \phi$ on the boundary, we have to find an $H$ that kills off $\mathbf{n}\cdot \nabla G$ on the boundary, and get a similar result.

In general, finding a harmonic $\nabla^2 H = 0$ with
\[
  H|_{\partial \Omega} = \left.\frac{1}{4\pi |\mathbf{x} - \mathbf{x}_0|}\right|_{\partial \Omega}
\]
is a difficult problem. However, the \emph{method of images} allows us to solve this in some special cases with lots of symmetry.

\begin{eg}[Method of images, half-space Poisson]
  Suppose
  \[
    \Omega = \{(x, y, z) \in \R^3: z \geq 0\}.
  \]
  We wish to find a solution to $\nabla^2 = -F$ in $\Omega$ with $\phi \to 0$ rapidly as $|\mathbf{x}| \to \infty$ with boundary condition $\phi(x, y, 0) = g(x, y)$.

  The fundamental solution
  \[
    G_3(\mathbf{x}, \mathbf{x}_0) = -\frac{1}{4\pi} \frac{1}{|\mathbf{x} - \mathbf{x}_0|}
  \]
  obeys all the conditions we need except
  \[
    G_3|_{z = 0} = -\frac{1}{4\pi} \frac{1}{[(x - x_0)^2 + (y - y_0)^2 + z_0^2]^{1/2}} \not= 0.
  \]
  However, let $\mathbf{x}_0^R$ be the point $(x_0, y_0, -z_0)$ . This is the reflection of $x_0$ in the boundary plane $z = 0$. Since the point $\mathbf{x}_0^R$ is outside our domain, $G_3(\mathbf{x}, \mathbf{x}_0^R)$ obeys
  \[
    \nabla^2 G_3(\mathbf{x}, \mathbf{x}_0^R) = 0
  \]
  for all $\mathbf{x} \in \Omega$, and also
  \[
    G_3(\mathbf{x}, \mathbf{x}_0^R)|_{z = 0} = G_3(\mathbf{x}, \mathbf{x}_0)|_{z = 0}.
  \]
  Hence we take
  \[
    G(\mathbf{x}, \mathbf{x}_0) = G_3(\mathbf{x}, \mathbf{x}_0) - G_3(\mathbf{x}, \mathbf{x}_0^R).
  \]
  The outward pointing normal to $\Omega$ at $z = 0$ is $\mathbf{n} = -\hat{\mathbf{z}}$. Hence we have
  \begin{align*}
    \mathbf{n} \cdot \nabla G|_{z = 0} &= \frac{1}{4\pi} \left[\frac{-(z - z_0)}{|\mathbf{x} - \mathbf{x}_0|^3} - \frac{-(z + z_0)}{|\mathbf{x} - \mathbf{x}_0^R|^3}\right]_{z = 0} \\
    &= \frac{1}{2\pi} \frac{z_0}{[(x - x_0)^2 + (y - y_0)^2 + z_0^2]^{3/2}}.
  \end{align*}
  Therefore our solution is
  \begin{align*}
    \phi(\mathbf{x}_0) &= \frac{1}{4\pi} \int_\Omega \left[\frac{1}{|\mathbf{x} - \mathbf{x}_0|} - \frac{1}{|\mathbf{x} - \mathbf{x}_0^R|}\right] F(\mathbf{x})\;\d^3 x \\
    &\quad + \frac{z_0}{2\pi} \int_{\R^2} \frac{g(x, y)}{[(x - x_0)^2 + (y - y_0)^2 + z_0^2]^{3/2}} \;\d x \;\d y.
  \end{align*}
  What have we actually done here? The Green's function $G_3(\mathbf{x}, \mathbf{x}_0)$ in some sense represents a ``charge'' at $\mathbf{x}_0$. We can imagine that the term $G_3(\mathbf{x}, \mathbf{x}_0^R)$ represents the contribution to our solution from a point charge of opposite sign located at $\mathbf{x}_0^R$. Then by symmetry, $G$ is zero at $z = 0$.
  \begin{center}
    \begin{tikzpicture}
      \draw (-2, 0) -- (2, 0) node [right] {$z = 0$};
      \node [circ] at (0, 1.5) {};
      \node at (0, 1.5) [above] {$\mathbf{x}_0$};
      \node [circ] at (0, -1.5) {};
      \node at (0, -1.5) [below] {$\mathbf{x}_0^R$};
    \end{tikzpicture}
  \end{center}
  Our solution for $\phi(\mathbf{x}_0)$ is valid if we extend it to $\mathbf{x}_0 \in \R^3$ where $\phi(\mathbf{x}_0)$ is solved by choosing two mirror charge distributions. But this is irrelevant for our present purposes. We are only concerned with finding a solution in the region $z \geq 0$. This is just an artificial device we have in order to solve the problem.
\end{eg}

\begin{eg}[Chimney smoke]
  Suppose a chimney produces smoke such that the density $\phi(\mathbf{x}, t)$ of smoke obeys
  \[
    \partial_t \phi - D \nabla^2 \phi = F(\mathbf{x}, t).
  \]
  The left side is just the heat equation, modelling the diffusion of smoke, while the right forcing term describes the production of smoke by the chimney.

  If this were a problem for $\mathbf{x} \in \R^3$, then the solution is
  \[
    \phi(\mathbf{x}, t) = \int_0^t \int_{\R^3} F(\mathbf{y}, \tau)S_3(\mathbf{x} - \mathbf{y}, t - \tau)\;\d^3 \mathbf{y}\;\d \tau,
  \]
  where
  \[
    S_3(\mathbf{x} - \mathbf{y}, t - \tau) = \frac{1}{[4\pi D (t - \tau)]^{3/2}} \exp\left(-\frac{|\mathbf{x} - \mathbf{y}|^2}{4D (t - \tau)}\right).
  \]
  This is true only if the smoke can diffuse in all of $\R^3$. However, this is not true for our current circumstances, since smoke does not diffuse into the ground.

  To account for this, we should find a Green's function that obeys
  \[
    \mathbf{n}\cdot \nabla G|_{z = 0} = 0.
  \]
  This says that there is no smoke diffusing in to the ground.

  This is achieved by picking
  \[
    G(\mathbf{x}, t; \mathbf{y}, \tau) = \Theta(t - \tau) [S_3(\mathbf{x} - \mathbf{y}, t - \tau) + S_3(\mathbf{x} - \mathbf{y}^R, t - \tau)].
  \]
  We can directly check that this obeys
  \[
    \partial_t D^2 \nabla^2 G = \delta (t - \tau) \delta^3(\mathbf{x} - \mathbf{y})
  \]
  when $\mathbf{x} \in \Omega$, and also
  \[
    \mathbf{n}\cdot \nabla G|z_0 = 0.
  \]
  Hence the smoke density is given by
  \[
    \phi(\mathbf{x}, t) = \int_0^t \int_\Omega F(\mathbf{y}, \tau) [S_3(\mathbf{x} - \mathbf{y}, t - \tau) + S_3(\mathbf{x} - \mathbf{y}^R, t - \tau)]\;\d^3 y.
  \]
  We can think of the second term as the contribution from a ``mirror chimney''. Without a mirror chimney, we will have smoke flowing into the ground. With a mirror chimney, we will have equal amounts of mirror smoke flowing up from the ground, so there is no net flow. Of course, there are no mirror chimneys in reality. These are just artifacts we use to find the solution we want.
\end{eg}

\begin{eg}[Method of images, wave reflection]
  Suppose we want to solve the wave equation in the region $(x, t)$ such that $x > 0$ with boundary conditions
  \[
    \phi(x, 0) = b(x),\quad \partial_t(x, 0) = 0,\quad \partial_x \phi(0, t) = 0.
  \]
  On $\R^{1, 1}$ d'Alembert's solution gives
  \[
    \phi(x, t) = \frac{1}{2}[b(x - ct) + b(x + ct)]
  \]
  This is not what we want, since eventually we will have a wave moving past the $x = 0$ line.
  \begin{center}
    \begin{tikzpicture}
      \draw [mred, semithick] (-3, 0) -- (3, 0);
      \draw [dashed] (0, -0.5) -- (0, 2) node [right] {$x = 0$};
      \draw [domain=-1:1,samples=50, mblue] plot ({\x - 1.5}, {1.5 * exp(-7 * \x * \x)});

      \draw [->] (-1.1, 0.8) -- +(0.5, 0);
    \end{tikzpicture}
  \end{center}
  To compensate for this, we introduce a mirror wave moving in the opposite direction, such that when as they pass through each other at $x = 0$, there is no net flow across the boundary.
  \begin{center}
    \begin{tikzpicture}
      \draw [mred, semithick] (-3, 0) -- (3, 0);
      \draw [dashed] (0, -0.5) -- (0, 2) node [right] {$x = 0$};

      \draw [domain=-1:1,samples=50, mblue] plot ({\x - 1.5}, {1.5 * exp(-7 * \x * \x)});
      \draw [->] (-1.1, 0.8) -- +(0.5, 0);

      \begin{scope}[xscale=-1]
        \draw [dashed, domain=-1:1,samples=50, mblue] plot ({\x - 1.5}, {1.5 * exp(-7 * \x * \x)});
        \draw [dashed, ->] (-1.1, 0.8) -- +(0.5, 0);
      \end{scope}
    \end{tikzpicture}
  \end{center}
  More precisely, we include a \emph{mirror initial condition} $\phi(x, 0) = b(x) + b(-x)$, where we set $b(x) = 0$ when $x < 0$. In the region $x > 0$ we are interested in, only the $b(x)$ term will contribute. In the $x < 0$ region, only $x > 0$ will contribute. Then the general solution is
  \[
    \phi(x, t) = \frac{1}{2}[b(x + ct) + b(x + c) + b( -x - ct) + b(-x + ct)].
  \]
\end{eg}
\end{document}
